{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322f5ac2-41bd-4d1c-b5c0-0dd50439e99e",
   "metadata": {},
   "source": [
    "# Classificação de Espécies de Psitacídeos do Cerrado: Uma Análise Comparativa com Redes Convolucionais, Transfer Learning e Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uIDTVZhVFfBR",
   "metadata": {
    "id": "uIDTVZhVFfBR"
   },
   "source": [
    "# 1. Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mCjg2b-Iq8S",
   "metadata": {
    "id": "9mCjg2b-Iq8S"
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <table style=\"margin: auto; border-spacing: 60px;\">\n",
    "    <tr>\n",
    "      <td align=\"center\" style=\"padding: 20px;\">\n",
    "        <a href=\"https://github.com/PedroSampaioDias\">\n",
    "          <img style=\"border-radius: 50%;\" src=\"https://avatars.githubusercontent.com/u/90795603?v=4\" width=\"150px;\"/>\n",
    "          <h5 class=\"text-center\">Pedro Sampaio - 211043745</h5>\n",
    "        </a>\n",
    "      </td>\n",
    "      <td align=\"center\" style=\"padding: 20px;\">\n",
    "        <a href=\"https://github.com/raulbreno\">\n",
    "          <img style=\"border-radius: 50%;\" src=\"https://avatars.githubusercontent.com/u/72105072?v=4\" width=\"150px;\"/>\n",
    "          <h5 class=\"text-center\">Raul Breno - 200026810</h5>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9rL3BcQdI1Bj",
   "metadata": {
    "id": "9rL3BcQdI1Bj"
   },
   "source": [
    "# 2. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5tkaM_j4I6YM",
   "metadata": {
    "id": "5tkaM_j4I6YM"
   },
   "source": [
    "# 3. Keywords\n",
    "\n",
    "- **Deep Learning**\n",
    "- **Classificação de Imagens** \n",
    "- **Transfer Learning** \n",
    "- **Fine-Tuning** \n",
    "- **Data Augmentation** \n",
    "- **Psittacidae**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T_MHvmt6FmrP",
   "metadata": {
    "id": "T_MHvmt6FmrP"
   },
   "source": [
    "# 4. Introdução\n",
    "\n",
    "O Cerrado brasileiro, reconhecido por sua vasta biodiversidade, abriga uma notável diversidade de aves, entre as quais se destaca a família Psittacidae. Este grupo, que inclui espécies populares como araras, papagaios e periquitos, é notório por suas características singulares: são animais de notável inteligência e cérebro desenvolvido, grande longevidade e a capacidade de imitar uma variedade de sons. Apesar de suas plumagens vibrantes, a diferenciação visual entre espécies pode ser um desafio significativo, especialmente para observadores não especializados. Esta dificuldade classifica o problema como um desafio de Classificação Visual de Granularidade Fina (Fine-Grained Visual Classification), onde as variações interclasses são sutis.\n",
    "\n",
    "Diante deste cenário, o presente trabalho tem como objetivo central o desenvolvimento e a avaliação de um classificador automático baseado em aprendizado profundo, capaz de identificar a espécie de uma ave a partir de uma imagem. Para tal, será utilizado um conjunto de dados composto por aproximadamente 3.000 imagens de 14 espécies distintas de psitacídeos do Cerrado. O dataset, proveniente da plataforma iNaturalist, é composto por imagens RGB com resoluções variadas, retratando as aves em múltiplas poses e ambientes. Uma análise preliminar do conjunto de dados revela um desbalanceamento no número de imagens por classe , uma característica que será abordada através de técnicas de aumento de dados (data augmentation) para garantir a robustez e a capacidade de generalização dos modelos.\n",
    "\n",
    "As 14 espécies consideradas neste estudo são:\n",
    "\n",
    "- **Amazona aestiva (Papagaio-verdadeiro)**\n",
    "- **Amazona amazonica (Curica)**\n",
    "- **Anodorhynchus hyacinthinus (Arara-azul)**\n",
    "- **Ara ararauna (Arara-canindé)**\n",
    "- **Ara chloropterus (Arara-vermelha)**\n",
    "- **Ara macao (Araracanga)**\n",
    "- **Brotogeris chiriri (Periquito-de-encontro-amarelo)**\n",
    "- **Diopsittaca nobilis (Maracanã-pequena)**\n",
    "- **Eupsittula aurea (Periquito-rei)**\n",
    "- **Forpus xanthopterygius (Tuim)**\n",
    "- **Orthopsittaca manilatus (Maracanã-do-buriti)**\n",
    "- **Primolius maracana (Maracanã)**\n",
    "- **Psittacara leucophthalmus (Periquitão)**\n",
    "- **Touit melanonotus (Apuim-de-costas-pretas)**\n",
    "\n",
    "A abordagem metodológica deste estudo foi estruturada em três estratégias experimentais distintas, visando uma análise comparativa de desempenho. A primeira abordagem consistirá no desenvolvimento de uma Rede Neural Convolucional (CNN) a partir do zero. A segunda estratégia empregará a técnica de Aprendizado por Transferência (Transfer Learning), na qual um modelo pré-treinado será utilizado como um extrator de características fixo. Por fim, a terceira abordagem utilizará a técnica de Fine-Tuning (ajuste fino), que estende o aprendizado por transferência ao reajustar sutilmente os pesos do modelo pré-treinado. O desempenho de cada uma dessas três abordagens será rigorosamente avaliado e comparado utilizando um conjunto abrangente de métricas, incluindo Acurácia, Precisão, Recall, F1-Score e a Matriz de Confusão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DY281S4DJKU_",
   "metadata": {
    "id": "DY281S4DJKU_"
   },
   "source": [
    "# 5. Trabalhos Relacionados\n",
    "\n",
    "A classificação de espécies de aves é um campo de estudo ativo em visão computacional, frequentemente categorizado como um problema de Classificação Visual de Granularidade Fina (Fine-Grained Visual Classification - FGVC), onde as distinções entre classes são sutis e exigem a identificação de características locais específicas. Diversos trabalhos têm explorado a eficácia de diferentes arquiteturas de Redes Neurais Convolucionais (CNN) e outras abordagens para este desafio.\n",
    "\n",
    "Em um estudo comparativo abrangente, Tang (2025) realiza uma análise da eficácia de diferentes famílias de arquiteturas de CNNs aplicadas à tarefa. Empregando um vasto conjunto de dados com mais de 80.000 imagens, abrangendo 525 espécies distintas, a pesquisa avaliou o desempenho de três proeminentes arquiteturas: ResNet, MobileNet e VGG. Os resultados indicaram que as famílias ResNet e MobileNet alcançaram desempenho superior. Notavelmente, a performance da ResNet demonstrou uma correlação positiva com o aumento do número de camadas. Em contrapartida, a família VGG exibiu menor capacidade de discriminação, apresentando dificuldades na distinção entre espécies com características visuais semelhantes. Por fim, a família MobileNet destacou-se por oferecer um balanço notável entre acurácia e eficiência computacional, apresentando-se como uma solução viável para aplicações com restrições de hardware.\n",
    "\n",
    "Além da comparação direta de arquiteturas padrão, a literatura e a indústria apresentam outras abordagens relevantes:\n",
    "\n",
    "- Classificação com Redes de Atenção: Para lidar com a sutileza das características em problemas de granularidade fina, pesquisas têm explorado o uso de redes neurais com mecanismos de atenção. Modelos como o proposto por Fu et al. (2017) são projetados para aprender a focar automaticamente nas regiões mais discriminatórias da imagem (ex: o formato do bico, o padrão de uma asa), imitando a forma como um especialista humano analisa uma ave para identificá-la e melhorando a capacidade de distinção do modelo.\n",
    "\n",
    "- Aplicações Práticas (Merlin Bird ID): Um dos exemplos mais bem-sucedidos da aplicação prática de CNNs para a identificação de aves é o aplicativo Merlin Bird ID, desenvolvido pelo Laboratório de Ornitologia da Cornell. O sistema utiliza modelos de visão computacional treinados com centenas de milhares de imagens para analisar fotos e sugerir a espécie mais provável em tempo real, demonstrando a viabilidade e o impacto desta tecnologia para a ciência cidadã e a educação ambiental.\n",
    "\n",
    "- Identificação por Vocalização (BirdCLEF): Além da identificação visual, a comunidade de ciência de dados tem explorado a classificação de aves a partir de suas vocalizações. Competições anuais, como a \"BirdCLEF\" na plataforma Kaggle, desafiam os participantes a desenvolver modelos que possam identificar espécies de aves em gravações de áudio. As soluções frequentemente utilizam CNNs aplicadas a espectrogramas (representações visuais do som), combinando técnicas de visão computacional e processamento de áudio para o monitoramento da biodiversidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mNe7hXgRH_Nd",
   "metadata": {
    "id": "mNe7hXgRH_Nd"
   },
   "source": [
    "# 6. Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZOMuMZrGaVzn",
   "metadata": {
    "id": "ZOMuMZrGaVzn"
   },
   "source": [
    "A abordagem metodológica deste estudo foi deliberadamente estruturada em três estratégias experimentais distintas e progressivamente complexas para a tarefa de classificação de imagens. O objetivo principal é conduzir uma análise comparativa rigorosa do desempenho, avaliando o compromisso entre o esforço de desenvolvimento, a necessidade de dados e a acurácia final de cada paradigma. As três estratégias representam um espectro que vai desde a criação de um modelo totalmente específico para o domínio até a alavancagem máxima de conhecimento pré-existente.\n",
    "\n",
    "- Desenvolvimento de uma Rede Neural Convolucional (CNN) a partir do zero (from scratch): A primeira abordagem consiste no projeto e implementação de uma arquitetura de CNN customizada. Esta metodologia oferece um controle granular sobre todos os componentes do modelo, incluindo a profundidade da rede, o número e o tamanho dos filtros convolucionais, as funções de ativação e a topologia das camadas de classificação. A principal vantagem reside na possibilidade de criar uma arquitetura otimizada para as características intrínsecas e a complexidade específica do nosso conjunto de dados. Contudo, este método impõe desafios significativos: exige um volume de dados substancialmente maior para que a rede possa aprender representações de características hierárquicas significativas a partir de uma inicialização de pesos aleatória. Além disso, é computacionalmente intensivo e apresenta um risco elevado de superajuste (overfitting), especialmente com datasets de tamanho limitado. Este modelo servirá como um baseline fundamental, estabelecendo um ponto de referência de desempenho contra o qual as técnicas mais avançadas serão comparadas.\n",
    "\n",
    "- Aprendizado por Transferência via Extração de Características (Transfer Learning): A segunda estratégia emprega a técnica de aprendizado por transferência em sua forma mais direta. Nesta abordagem, um modelo pré-treinado em um dataset de larga escala e de domínio geral, como o ImageNet (que contém milhões de imagens em milhares de categorias), é utilizado como um extrator de características fixo. As camadas convolucionais do modelo pré-treinado (o backbone) são \"congeladas\", o que significa que seus pesos não são atualizados durante o treinamento. Apenas a camada de classificação final do modelo original é removida e substituída por uma nova, com um número de saídas correspondente ao número de classes do nosso problema. Subsequentemente, apenas os pesos desta nova camada de classificação são treinados. Esta técnica capitaliza sobre o fato de que as camadas iniciais de uma CNN aprendem características genéricas (e.g., bordas, texturas, cores), que são transferíveis para uma vasta gama de tarefas de visão computacional, reduzindo drasticamente o tempo de treinamento e a necessidade de dados.\n",
    "\n",
    "- Fine-Tuning (Ajuste Fino): A terceira e última abordagem estende o conceito de aprendizado por transferência. O fine-tuning inicia-se de forma semelhante, treinando apenas a nova camada de classificação com o backbone congelado. Contudo, em uma segunda fase, o modelo inteiro (ou uma parte dele, tipicamente as camadas mais profundas) é \"descongelado\". O treinamento então prossegue em todo o conjunto de parâmetros, mas com uma taxa de aprendizado (learning rate) muito baixa. O objetivo é ajustar sutilmente os pesos pré-treinados, permitindo que as características genéricas aprendidas no ImageNet se tornem mais especializadas e adaptadas às nuances específicas do nosso conjunto de dados de aves. Esta metodologia busca um equilíbrio ótimo, aproveitando o conhecimento pré-existente enquanto permite uma adaptação mais profunda ao novo domínio, sendo particularmente eficaz quando o dataset da nova tarefa é de tamanho razoável e possui similaridades com o dataset original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44349d29",
   "metadata": {
    "id": "44349d29"
   },
   "source": [
    "## 6.1 Configuração do Ambiente e Importação de Bibliotecas\n",
    "\n",
    "A fase inicial do desenvolvimento computacional compreende a importação das bibliotecas que constituem o ferramental para a execução do projeto. O ambiente é configurado com módulos para aquisição e descompressão de dados (gdown, zipfile), manipulação de sistema de arquivos e diretórios (os, shutil, pathlib), processamento e análise de dados tabulares (pandas), visualização de dados (matplotlib, seaborn), e processamento de imagens (PIL, OpenCV). De forma crucial, a biblioteca albumentations é importada para a implementação de técnicas de aumento de dados (data augmentation), e o numpy é utilizado como base para operações numéricas e manipulação de vetores multidimensionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcf396",
   "metadata": {
    "id": "d1dcf396"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "from albumentations import Compose, HorizontalFlip, VerticalFlip, Rotate, CoarseDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12859ea1",
   "metadata": {
    "id": "12859ea1"
   },
   "source": [
    "## 6.2 Aquisição e Preparação do Conjunto de Dados\n",
    "\n",
    "A fundação de qualquer modelo de aprendizado de máquina reside na qualidade e integridade do conjunto de dados. Nesta etapa, é executado um processo programático e automatizado para a obtenção e preparação do dataset original. O script inicialmente verifica a existência e integridade do conjunto de dados no ambiente local para evitar redundância. Caso o dataset não esteja presente ou esteja incompleto, o processo de aquisição é iniciado: o arquivo compactado é baixado de um URI específico do Google Drive utilizando a biblioteca gdown.\n",
    "\n",
    "Posteriormente, o arquivo é descompactado para um diretório de destino. Uma etapa de sanitização é realizada para remover arquivos e diretórios de metadados específicos de sistemas operacionais (e.g., __MACOSX, .DS_Store), garantindo uma estrutura de dados limpa, consistente e reprodutível, pré-requisito para as etapas subsequentes de pré-processamento e treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50265f17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "50265f17",
    "outputId": "7a54136b-cf72-44ac-f479-aac9a999c69b"
   },
   "outputs": [],
   "source": [
    "file_id = \"1y8tfmhAtVxEHb8hxDlXx6Ek1ALs0SCAm\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "zip_name = \"dataset_original.zip\"\n",
    "\n",
    "base = Path(\"datasets\"); compact = base/\"compactados\"; extract = base/\"dataset_original\"\n",
    "base.mkdir(exist_ok=True); compact.mkdir(parents=True, exist_ok=True); extract.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tiff\",\".tif\",\".webp\"}\n",
    "contar_imgs = lambda p: sum(1 for f in p.rglob(\"*\") if f.is_file() and f.suffix.lower() in exts)\n",
    "\n",
    "def pronto(p):\n",
    "    return p.exists() and sum(1 for d in p.iterdir() if d.is_dir()) == 14 and contar_imgs(p) == 2879\n",
    "\n",
    "if pronto(extract):\n",
    "    print(\"✅ Dataset já existe.\")\n",
    "else:\n",
    "    print(\"📥 Baixando...\"); gdown.download(url, zip_name, quiet=False)\n",
    "    dest = compact/zip_name; dest.unlink(missing_ok=True); shutil.move(zip_name, dest)\n",
    "    print(\"📂 Extraindo...\"); zipfile.ZipFile(dest).extractall(extract)\n",
    "    [shutil.rmtree(d, ignore_errors=True) for d in extract.rglob(\"__MACOSX\")]\n",
    "    [f.unlink(missing_ok=True) for f in extract.rglob(\".DS_Store\")]\n",
    "    print(f\"📸 Total de imagens: {contar_imgs(extract)}\")\n",
    "    print(\"✅ Dataset pronto em:\", base.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v8mk7jom52Zp",
   "metadata": {
    "id": "v8mk7jom52Zp"
   },
   "source": [
    "## 6.3 Exploração de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TdKHyVqHbP8r",
   "metadata": {
    "id": "TdKHyVqHbP8r"
   },
   "source": [
    "Neste ponto, a separação, classificação e data augmentation serão feitas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JVfl1FBp3lTa",
   "metadata": {
    "id": "JVfl1FBp3lTa"
   },
   "source": [
    "### 6.3.1 Organização do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Aj1QAse-4ja3",
   "metadata": {
    "id": "Aj1QAse-4ja3"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/dataset_original\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PvhHKrI34n9F",
   "metadata": {
    "id": "PvhHKrI34n9F"
   },
   "source": [
    "#### 6.3.1.1 Quantidade total de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1KUnenF3kmr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1KUnenF3kmr",
    "outputId": "415b144d-33b6-4c74-86bc-a2ad98a3c54b"
   },
   "outputs": [],
   "source": [
    "total_images = sum(len(files) for _, _, files in os.walk(dataset_path))\n",
    "total_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b-vABkUP5npT",
   "metadata": {
    "id": "b-vABkUP5npT"
   },
   "source": [
    "#### 6.3.1.2 Número de classes (espécies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nSnlLqAW6Ois",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSnlLqAW6Ois",
    "outputId": "d87ffa89-5151-4820-c90f-7e2da0dc6cfc"
   },
   "outputs": [],
   "source": [
    "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0PRih2bu5nsK",
   "metadata": {
    "id": "0PRih2bu5nsK"
   },
   "source": [
    "### 6.3.2 Distribuição das classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ppDf8KvLgzK",
   "metadata": {
    "id": "1ppDf8KvLgzK"
   },
   "source": [
    "#### 6.3.2.1 Frequência de imagens por classe com representação em quantidade e porcentagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EeIHV4IcG0RI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "EeIHV4IcG0RI",
    "outputId": "2d2e371a-a7e3-4847-b2d3-911411d9d7ff"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    (c, sum(1 for _,_,fs in os.walk(os.path.join(dataset_path, c))\n",
    "            for f in fs if os.path.splitext(f)[1].lower() in exts))\n",
    "    for c in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, c))\n",
    "], columns=[\"classe\",\"imagens\"]).sort_values(\"imagens\")\n",
    "\n",
    "df[\"pct\"] = (df[\"imagens\"] / df[\"imagens\"].sum() * 100).round(2)\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "ax = sns.barplot(data=df, x=\"imagens\", y=\"classe\",\n",
    "                 hue=\"classe\", dodge=False, palette=\"viridis\",\n",
    "                 orient=\"h\", legend=False)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.text(p.get_width()+5, p.get_y()+p.get_height()/2,\n",
    "            f\"{int(p.get_width())} ({df['pct'].iloc[i]}%)\",\n",
    "            va=\"center\", ha=\"left\", weight=\"bold\")\n",
    "\n",
    "max_val = df[\"imagens\"].max()\n",
    "ax.set_xlim(0, max_val * 1.25)\n",
    "\n",
    "ax.set(title=\"Distribuição de imagens por classe\",\n",
    "       xlabel=\"Quantidade de imagens e Porcentagem\", ylabel=\"Classe\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mIa18c8nQ9Dv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "mIa18c8nQ9Dv",
    "outputId": "a30e1e71-aee2-4943-d8b9-d4ca78cddb13"
   },
   "outputs": [],
   "source": [
    "extensoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext:  # garante que tem extensão\n",
    "            extensoes.append(ext)\n",
    "\n",
    "# Conta as ocorrências de cada extensão\n",
    "contagem = Counter(extensoes)\n",
    "\n",
    "# Separa chaves e valores\n",
    "extensoes_unicas = list(contagem.keys())\n",
    "quantidades = list(contagem.values())\n",
    "\n",
    "# Cria gráfico de barras horizontal\n",
    "plt.figure(figsize=(8,5))\n",
    "bars = plt.barh(extensoes_unicas, quantidades, color=\"skyblue\")\n",
    "plt.xlabel(\"Quantidade de Imagens\")\n",
    "plt.ylabel(\"Extensão\")\n",
    "plt.title(\"Distribuição das Extensões de Arquivos de Imagem\")\n",
    "\n",
    "# Adiciona os valores na frente das barras\n",
    "for bar, qtd in zip(bars, quantidades):\n",
    "    plt.text(qtd + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "             str(qtd), va=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_Igaq-wQ9BM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "i_Igaq-wQ9BM",
    "outputId": "2610eb26-6c8b-49a0-ff3d-778c787ec3ef"
   },
   "outputs": [],
   "source": [
    "resolucoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    resolucoes.append((w, h))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Estatísticas\n",
    "larguras = [w for w, h in resolucoes]\n",
    "alturas = [h for w, h in resolucoes]\n",
    "\n",
    "res_min = (min(larguras), min(alturas))\n",
    "res_max = (max(larguras), max(alturas))\n",
    "res_media = (int(np.mean(larguras)), int(np.mean(alturas)))\n",
    "\n",
    "# Lista de barras com valores e rótulos\n",
    "barras = [\n",
    "    (\"Mínimo\", res_min),\n",
    "    (\"Média\", res_media),\n",
    "    (\"Máximo\", res_max)\n",
    "]\n",
    "\n",
    "# Ordena por largura*altura (área da imagem)\n",
    "barras.sort(key=lambda x: x[1][0]*x[1][1])\n",
    "\n",
    "# Extrai dados para plot\n",
    "nomes = [nome for nome, res in barras]\n",
    "res_labels = [f\"{res[0]}x{res[1]}\" for nome, res in barras]\n",
    "valores = [res[0]*res[1] for nome, res in barras]  # usado só para tamanho da barra\n",
    "\n",
    "# Cria gráfico horizontal\n",
    "plt.figure(figsize=(8,4))\n",
    "bars = plt.barh(range(len(nomes)), valores, color=[\"skyblue\", \"orange\", \"green\"])\n",
    "plt.yticks(range(len(nomes)), nomes)\n",
    "plt.xlabel(\"Resolução relativa (área)\")\n",
    "plt.title(\"Estatísticas das Resoluções das Imagens\")\n",
    "\n",
    "# Adiciona rótulo [LxA] na frente de cada barra\n",
    "for bar, label in zip(bars, res_labels):\n",
    "    plt.text(bar.get_width() + max(valores)*0.01,\n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             label, va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eXyB5mQ8-z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "58eXyB5mQ8-z",
    "outputId": "dc3346a7-266d-4891-ac18-a4ec2b568940"
   },
   "outputs": [],
   "source": [
    "resolucoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    resolucoes.append(f\"{w}x{h}\")  # guarda como string \"LxA\"\n",
    "            except:\n",
    "                pass  # ignora arquivos corrompidos\n",
    "\n",
    "# Conta quantas imagens têm cada resolução\n",
    "contagem = Counter(resolucoes)\n",
    "\n",
    "# Pega as 5 resoluções mais comuns\n",
    "top5 = contagem.most_common(5)  # retorna lista de tuplas: [(resolucao, quantidade), ...]\n",
    "\n",
    "# Separa em listas para plot\n",
    "resolucoes_top5 = [item[0] for item in top5]\n",
    "quantidades_top5 = [item[1] for item in top5]\n",
    "\n",
    "# Cria gráfico horizontal\n",
    "plt.figure(figsize=(8,5))\n",
    "# inverte listas para colocar a maior barra em cima\n",
    "bars = plt.barh(resolucoes_top5[::-1], quantidades_top5[::-1], color=\"skyblue\")\n",
    "plt.xlabel(\"Número de imagens\")\n",
    "plt.ylabel(\"Resolução [Largura x Altura]\")\n",
    "plt.title(\"Top 5 resoluções mais comuns\")\n",
    "\n",
    "# Adiciona número de imagens na frente de cada barra\n",
    "for bar, qtd in zip(bars, quantidades_top5[::-1]):\n",
    "    plt.text(qtd + max(quantidades_top5)*0.01,\n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             str(qtd), va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zKAISzUaQ88W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKAISzUaQ88W",
    "outputId": "d7be4523-bf35-4135-8eed-dbc98db78c7b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Número total de resoluções únicas\n",
    "total_resolucoes = len(contagem)\n",
    "print(f\"Número total de resoluções únicas: {total_resolucoes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saZaeYi7Q857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "saZaeYi7Q857",
    "outputId": "d700e6de-b1ff-4d32-8ef8-323bb051e165"
   },
   "outputs": [],
   "source": [
    "proporcoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    proporcao = Fraction(w, h).limit_denominator(100)  # simplifica a fração\n",
    "                    proporcoes.append(proporcao)\n",
    "            except:\n",
    "                pass  # ignora arquivos corrompidos\n",
    "\n",
    "# Calcula estatísticas\n",
    "prop_min = min(proporcoes)\n",
    "prop_max = max(proporcoes)\n",
    "prop_media_decimal = np.mean([float(p) for p in proporcoes])\n",
    "prop_media = Fraction(prop_media_decimal).limit_denominator(100)\n",
    "\n",
    "# Lista para plot\n",
    "barras = [\n",
    "    (\"Mínima\", prop_min),\n",
    "    (\"Média\", prop_media),\n",
    "    (\"Máxima\", prop_max)\n",
    "]\n",
    "\n",
    "# Ordena por proporção decimal\n",
    "barras.sort(key=lambda x: float(x[1]))\n",
    "\n",
    "# Extrai dados para plot\n",
    "nomes = [nome for nome, val in barras]\n",
    "valores = [float(val) for nome, val in barras]\n",
    "labels = [f\"{val.numerator}:{val.denominator}\" for nome, val in barras]  # rótulo L:A\n",
    "\n",
    "# Cria gráfico horizontal\n",
    "plt.figure(figsize=(8,4))\n",
    "bars = plt.barh(nomes, valores, color=[\"skyblue\", \"orange\", \"green\"])\n",
    "plt.xlabel(\"Proporção (Largura / Altura)\")\n",
    "plt.title(\"Proporções das imagens: mínima, média e máxima\")\n",
    "\n",
    "# Adiciona rótulo L:A na frente da barra\n",
    "for bar, label in zip(bars, labels):\n",
    "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "             label, va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGjzny-kQ83G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "RGjzny-kQ83G",
    "outputId": "e1f3038d-57c2-452f-d906-6cabbee3b296"
   },
   "outputs": [],
   "source": [
    "razoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    razao = Fraction(w, h).limit_denominator(100)\n",
    "                    razoes.append(f\"{razao.numerator}:{razao.denominator}\")\n",
    "            except:\n",
    "                pass  # ignora arquivos corrompidos\n",
    "\n",
    "# Conta quantas imagens têm cada razão\n",
    "contagem = Counter(razoes)\n",
    "\n",
    "# Pega as 10 razões mais frequentes\n",
    "top10 = contagem.most_common(10)  # lista de tuplas: [(razao, quantidade), ...]\n",
    "\n",
    "# Separa em listas para plot\n",
    "razoes_top10 = [item[0] for item in top10]\n",
    "quantidades_top10 = [item[1] for item in top10]\n",
    "\n",
    "# Cria gráfico horizontal\n",
    "plt.figure(figsize=(10, max(4, len(razoes_top10)*0.4)))\n",
    "bars = plt.barh(razoes_top10[::-1], quantidades_top10[::-1], color=\"skyblue\")  # inverte para a mais frequente em cima\n",
    "plt.xlabel(\"Número de imagens\")\n",
    "plt.ylabel(\"Razão de aspecto [L:A]\")\n",
    "plt.title(\"Top 10 razões de aspecto mais frequentes\")\n",
    "\n",
    "# Adiciona número de imagens na frente de cada barra\n",
    "for bar, qtd in zip(bars, quantidades_top10[::-1]):\n",
    "    plt.text(qtd + max(quantidades_top10)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "             str(qtd), va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512629eb",
   "metadata": {},
   "source": [
    "## 6.4 Divisão e Aumentação do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176622a1-d2e5-4b53-a7d6-9c921f14eabe",
   "metadata": {},
   "source": [
    "### 6.4.1 Configuração e Dependências\n",
    "\n",
    "O processo inicia-se com a importação das bibliotecas essenciais para cada etapa da tarefa. Para a manipulação de arquivos e diretórios, são utilizados os módulos os e shutil; a geração de números aleatórios é gerenciada por random; e o processamento de imagens fica a cargo do cv2 (OpenCV) e albumentations. Para a etapa de verificação visual, são empregadas as bibliotecas do Matplotlib: matplotlib.pyplot (como plt), utilizada para criar a grade de visualização, e matplotlib.image (como mpimg), responsável por carregar os arquivos de imagem.\n",
    "\n",
    "Em seguida, são definidos os parâmetros fundamentais que guiarão todo o processo de preparação de dados. Isso inclui a definição dos caminhos dos diretórios: o de origem (dataset_original), o de destino para os dados aumentados (dataset_aumentado), e os diretórios finais para os subconjuntos de treinamento (train_images) e validação (test_images). Adicionalmente, são estabelecidos os parâmetros numéricos: a variável target_count, que define o número de imagens por classe após o balanceamento, e a proporção de 80/20 para a divisão entre os dados de treinamento e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e011624-f5fe-4d1a-8f39-50a6627cbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "from albumentations import Compose, Rotate, HorizontalFlip, VerticalFlip, CoarseDropout\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe9850-e974-42b0-8768-798b76956906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parâmetros de Aumento de Dados ---\n",
    "dataset_path = \"./datasets/dataset_original\"\n",
    "dataset_final_path = \"./datasets/dataset_aumentado\"\n",
    "target_count = 300\n",
    "\n",
    "# --- Parâmetros de Divisão do Dataset ---\n",
    "train_dir = \"./datasets/train_images\"\n",
    "val_dir = \"./datasets/val_images\"\n",
    "train_split = 0.8\n",
    "\n",
    "# --- Criação dos Diretórios ---\n",
    "os.makedirs(dataset_final_path, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# --- Exibição das Configurações ---\n",
    "print(\"--- Configurações do Ambiente ---\")\n",
    "print(f\"Dataset de origem: '{os.path.abspath(dataset_path)}'\")\n",
    "print(f\"Dataset aumentado (destino): '{os.path.abspath(dataset_final_path)}'\")\n",
    "print(f\"Diretório de treino: '{os.path.abspath(train_dir)}'\")\n",
    "print(f\"Diretório de validação: '{os.path.abspath(val_dir)}'\")\n",
    "print(f\"Número alvo de imagens por classe: {target_count}\")\n",
    "print(f\"Proporção de divisão (Treino/Validação): {train_split*100:.0f}% / {(1-train_split)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c9851-4f05-4107-8e43-a07669847030",
   "metadata": {},
   "source": [
    "### 6.4.2 Definição da Estratégia de Aumento de Dados\n",
    "\n",
    "Para gerar diversidade sintética nos dados, foi implementada a função get_random_augmentation. Esta função encapsula a lógica de transformação, selecionando aleatoriamente, a cada chamada, uma de três possíveis técnicas de aumento de dados da biblioteca Albumentations:\n",
    "\n",
    "- Rotação (Rotate): Aplica uma rotação na imagem em um ângulo aleatório entre 10 e 340 graus.\n",
    "\n",
    "- Inversão (Flip): Inverte a imagem, com 50% de chance de ser na horizontal e 50% na vertical.\n",
    "\n",
    "- Recorte (CoarseDropout): Remove de 1 a 3 pequenos retângulos pretos da imagem em locais aleatórios para simular oclusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa44cb7-91b1-48f5-ba67-de773973980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_augmentation():\n",
    "    aug_type = random.choice([\"rotate\", \"flip\", \"cutout\"])\n",
    "    \n",
    "    if aug_type == \"rotate\":\n",
    "        return Compose([Rotate(limit=(10, 340), p=1)])\n",
    "    \n",
    "    elif aug_type == \"flip\":\n",
    "        if random.random() > 0.5:\n",
    "            return Compose([HorizontalFlip(p=1)])\n",
    "        else:\n",
    "            return Compose([VerticalFlip(p=1)])\n",
    "            \n",
    "    elif aug_type == \"cutout\":\n",
    "        return Compose([CoarseDropout(\n",
    "            max_holes=3, min_holes=1,\n",
    "            max_height=0.2, min_height=0.1,\n",
    "            max_width=0.2, min_width=0.1,\n",
    "            fill_value=0, p=1\n",
    "        )])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f4e1b5-99cd-4c8e-a43c-a7aeab24d41b",
   "metadata": {},
   "source": [
    "### 6.4.3 Geração do Dataset Aumentado\n",
    "\n",
    "Para corrigir o desbalanceamento entre as classes e aumentar a robustez do modelo, foi executado um processo de aumento de dados (data augmentation). A introdução de variações sintéticas, como rotações e recortes, expõe o modelo a uma gama mais ampla de cenários visuais, melhorando sua capacidade de generalização para imagens do mundo real.\n",
    "\n",
    "O script a seguir implementa essa estratégia. Ele itera sobre cada subdiretório de classe do dataset original, primeiramente copiando todas as imagens existentes para a nova estrutura de pastas. Em seguida, um laço while é acionado para gerar novas imagens sintéticas, aplicando transformações aleatórias através da função get_random_augmentation. Este processo continua até que o número de imagens em cada classe atinja o limiar pré-definido (target_count), resultando em um conjunto de dados final que é não apenas maior, mas também perfeitamente balanceado e mais diversificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2db17-00cd-43a5-afaf-12ab251d21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    \n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    final_class_path = os.path.join(dataset_final_path, class_folder)\n",
    "    os.makedirs(final_class_path, exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "    for img_name in images:\n",
    "        shutil.copy(os.path.join(class_path, img_name), os.path.join(final_class_path, img_name))\n",
    "\n",
    "    current_count = len(images)\n",
    "    print(f\"Classe '{class_folder}': {current_count} imagens originais. Gerando novas imagens...\")\n",
    "\n",
    "    while current_count < target_count:\n",
    "        img_name = random.choice(images)\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        aug = get_random_augmentation()\n",
    "        augmented = aug(image=img)['image']\n",
    "\n",
    "        save_name = f\"aug_{current_count}.jpg\"\n",
    "        save_path = os.path.join(final_class_path, save_name)\n",
    "        \n",
    "        augmented_bgr = cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(save_path, augmented_bgr)\n",
    "\n",
    "        current_count += 1\n",
    "\n",
    "    print(f\"-> Classe '{class_folder}' finalizada com {current_count} imagens.\")\n",
    "\n",
    "print(\"\\n✅ Processo de Data Augmentation concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cb765-7ea0-48d9-b2e2-4c59ba7aaf13",
   "metadata": {},
   "source": [
    "### 6.4.4 Verificação do Dataset Aumentado\n",
    "\n",
    "Após a geração sintética dos dados, é fundamental realizar uma verificação para assegurar a integridade do novo dataset. Esta etapa valida tanto a correção do balanceamento de classes quanto a qualidade visual das imagens que serão utilizadas no treinamento.\n",
    "\n",
    "O script a seguir executa duas funções de validação em sequência. A primeira é uma verificação quantitativa, que itera sobre o diretório dataset_aumentado para contar e exibir o número de imagens por classe. Este procedimento confirma que o objetivo de balanceamento foi alcançado. A segunda é uma inspeção visual, que seleciona e exibe uma amostra aleatória de cada classe em uma grade. Esta análise qualitativa permite validar que as transformações sintéticas são realistas, não distorcem as características essenciais das espécies e contribuem positivamente para a robustez do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c36d04-47b7-403b-aa24-2e0af855b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Verificação Quantitativa ---\")\n",
    "total_images = 0\n",
    "class_folders = sorted([d for d in os.listdir(dataset_final_path) if os.path.isdir(os.path.join(dataset_final_path, d))])\n",
    "\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(dataset_final_path, class_folder)\n",
    "    count = len(os.listdir(class_path))\n",
    "    print(f\"Classe '{class_folder}': {count} imagens\")\n",
    "    total_images += count\n",
    "\n",
    "print(f\"\\nTotal de imagens no dataset aumentado: {total_images}\")\n",
    "print(\"\\n--- Verificação Visual (Amostra Aleatória por Classe) ---\")\n",
    "\n",
    "nrows, ncols = 4, 4\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, class_folder in enumerate(class_folders):\n",
    "    class_path = os.path.join(dataset_final_path, class_folder)\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if images:\n",
    "        random_image_name = random.choice(images)\n",
    "        image_path = os.path.join(class_path, random_image_name)\n",
    "        \n",
    "        img = mpimg.imread(image_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(class_folder, fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "for j in range(len(class_folders), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863407a-fae5-4a48-aecc-f033e6d82626",
   "metadata": {},
   "source": [
    "### 6.4.5 Divisão de Datasets em Traino e Validação\n",
    "\n",
    "Para avaliar a capacidade de generalização do modelo e evitar o superajuste (overfitting), o conjunto de dados aumentado foi dividido em dois subconjuntos distintos e mutuamente exclusivos: treinamento e validação. O subconjunto de treinamento é utilizado para o ajuste dos pesos da rede neural, enquanto o subconjunto de validação, composto por dados não vistos durante o treinamento, serve para fornecer uma estimativa imparcial do desempenho do modelo em dados novos.\n",
    "\n",
    "Foi adotada uma proporção de 80% para treinamento e 20% para validação, um padrão consolidado na literatura de aprendizado de máquina. A divisão foi realizada de maneira estratificada, garantindo que essa proporção de 80/20 fosse mantida dentro de cada uma das 14 classes de aves. Para assegurar a aleatoriedade da amostragem, a lista de imagens de cada classe foi embaralhada antes da divisão. O script a seguir automatiza a criação das estruturas de diretório e a cópia dos arquivos de imagem para seus respectivos subconjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HYL1h2vsVjnB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYL1h2vsVjnB",
    "outputId": "affc3373-880f-467d-b320-ad3b6f252888"
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    split_idx = int(len(images) * train_split)\n",
    "    train_files = images[:split_idx]\n",
    "    val_files = images[split_idx:]\n",
    "\n",
    "    for img in train_files:\n",
    "        shutil.copy(os.path.join(class_path, img),\n",
    "                    os.path.join(train_dir, class_name, img))\n",
    "    for img in val_files:\n",
    "        shutil.copy(os.path.join(class_path, img),\n",
    "                    os.path.join(val_dir, class_name, img))\n",
    "\n",
    "print(\"✅ Divisão concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3K-IyYmWV2O4",
   "metadata": {
    "id": "3K-IyYmWV2O4"
   },
   "source": [
    "## 6.5 Configuração e Treinamento da Rede Personalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787PKNILGegW",
   "metadata": {
    "id": "787PKNILGegW"
   },
   "source": [
    "### 6.5.1 Preparação do Ambiente e Importação de Bibliotecas\n",
    "\n",
    "Antes da construção e treinamento do modelo, é fundamental configurar o ambiente computacional e importar todas as bibliotecas necessárias. Esta etapa inicial garante que o backend de processamento esteja corretamente definido, os recursos de hardware sejam alocados de forma eficiente e todas as ferramentas para modelagem, treinamento e avaliação estejam disponíveis.\n",
    "\n",
    "O script a seguir executa as seguintes ações:\n",
    "\n",
    "- Configuração do Ambiente: Através de variáveis de ambiente (os.environ), o Keras é configurado para utilizar o backend PyTorch. Adicionalmente, são aplicadas configurações para o gerenciamento de memória da GPU, prevenindo a alocação total e permitindo um crescimento dinâmico, além de especificar qual dispositivo GPU deve ser utilizado.\n",
    "\n",
    "- Importação do Framework Keras: São importados os componentes centrais da biblioteca Keras, incluindo o modelo Sequential, as diversas layers (como Dense, Dropout e BatchNormalization) e os regularizers, que são essenciais para construir a arquitetura da rede neural convolucional e implementar técnicas de prevenção de overfitting.\n",
    "\n",
    "- Ferramentas de Preparação de Dados: Inclui a biblioteca numpy para operações numéricas e a função image_dataset_from_directory do TensorFlow/Keras, uma ferramenta de alto nível para carregar e pré-processar imagens diretamente de uma estrutura de diretórios, criando um pipeline de dados eficiente.\n",
    "\n",
    "- Módulos de Avaliação e Visualização: São importadas funções da biblioteca scikit-learn para gerar métricas de avaliação, como a matriz de confusão (confusion_matrix), e as bibliotecas matplotlib e seaborn para a visualização gráfica dos resultados do treinamento e da performance do modelo.\n",
    "\n",
    "- Callbacks para Controle do Treinamento: São importados os callbacks ModelCheckpoint (para salvar o melhor modelo durante o treinamento), EarlyStopping (para interromper o treinamento caso a performance estagne) e ReduceLROnPlateau (para ajustar dinamicamente a taxa de aprendizado), que permitem um controle mais robusto e automatizado do processo de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gv6GBz3mV0wB",
   "metadata": {
    "id": "gv6GBz3mV0wB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import datasets, layers, models\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2hscSzOGyD_",
   "metadata": {
    "id": "G2hscSzOGyD_"
   },
   "source": [
    "### 6.5.2 Carregamento e Pré-processamento dos Dados\n",
    "\n",
    "A etapa de carregamento e pré-processamento de dados é fundamental para a construção de um modelo de aprendizado profundo eficaz. Nesta fase, os dados brutos (imagens em disco) são transformados em um formato estruturado, normalizado e otimizado para o consumo pela rede neural. O processo foi dividido em quatro etapas sequenciais: definição de parâmetros, construção do pipeline de dados, conversão para vetores e verificação final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0953ca-039e-4d8e-93fa-bdaa4fd16dd7",
   "metadata": {},
   "source": [
    "#### 1. Definição de Parâmetros e Estrutura de Dados\n",
    "A primeira etapa consiste na definição de todos os parâmetros que governarão o pipeline de dados. É declarada uma lista explícita de class_names, que assegura uma correspondência consistente e determinística entre os nomes das espécies e os rótulos numéricos (inteiros) que serão utilizados pelo modelo. São também especificados os caminhos para os diretórios de treinamento e validação.\n",
    "\n",
    "Dois hiperparâmetros críticos são definidos:\n",
    "\n",
    "- IMAGE_SIZE: Estabelecido como (128, 128), este parâmetro define as dimensões para as quais todas as imagens de entrada serão redimensionadas. As redes neurais convolucionais exigem um tensor de entrada com tamanho fixo, e esta uniformização é um pré-requisito essencial.\n",
    "\n",
    "- BATCH_SIZE: Definido como 32, corresponde ao número de imagens que serão processadas em um único lote (batch) durante o carregamento e o treinamento. A utilização de lotes é uma técnica padrão que otimiza o uso da memória e a estabilidade do processo de otimização dos gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236da8f-2137-4557-8951-2cc8748fa560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"amazona_aestiva\",\n",
    "    \"amazona_amazonica\",\n",
    "    \"anodorhynchus_hyacinthinus\",\n",
    "    \"ara_ararauna\",\n",
    "    \"ara_chloropterus\",\n",
    "    \"ara_macao\",\n",
    "    \"brotogeris_chiriri\",\n",
    "    \"diopsittaca_nobilis\",\n",
    "    \"eupsittula_aurea\",\n",
    "    \"forpus_xanthopterygius\",\n",
    "    \"orthopsittaca_manilatus\",\n",
    "    \"primolius_maracana\",\n",
    "    \"psittacara_leucophthalmus\",\n",
    "    \"touit_melanonotus\",\n",
    "]\n",
    "\n",
    "BASE_PATH = './datasets'\n",
    "train_dir = os.path.join(BASE_PATH, 'train_images')\n",
    "val_dir = os.path.join(BASE_PATH, 'val_images')\n",
    "\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e8aa6-b077-43d7-8689-408a6193c042",
   "metadata": {},
   "source": [
    "#### 2. Construção do Pipeline de Dados com tf.data\n",
    "\n",
    "Nesta fase, utiliza-se a função de alto nível image_dataset_from_directory da biblioteca Keras para criar um pipeline de dados eficiente (tf.data.Dataset). Esta função automatiza o processo de leitura das imagens a partir dos diretórios, inferindo os rótulos (labels='inferred') a partir da estrutura de subpastas e convertendo-os para um formato inteiro (label_mode='int').\n",
    "\n",
    "Para o conjunto de treinamento (train_ds), a opção shuffle=True é ativada para embaralhar os dados, uma prática crucial para evitar que o modelo aprenda com a ordem de apresentação das amostras e para melhorar a generalização. Para o conjunto de validação (val_ds), o embaralhamento é desativado (shuffle=False) para garantir que a avaliação da performance seja consistente e reprodutível entre as épocas de treinamento.\n",
    "\n",
    "Subsequentemente, é aplicada uma camada de normalização (Rescaling(1./255)) a ambos os datasets. Este é um passo de pré-processamento crítico que reescala os valores de pixel, originalmente no intervalo [0, 255], para o intervalo [0, 1]. A normalização dos dados de entrada acelera a convergência do treinamento e melhora a estabilidade numérica do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba03ed0-4917-469c-9581-583b8b45a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando dados de TREINO...\")\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=class_names,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    interpolation='bilinear',\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nCarregando dados de VALIDAÇÃO...\")\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=class_names,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    interpolation='bilinear',\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ee932-dab0-4568-878b-4a6313ee633c",
   "metadata": {},
   "source": [
    "#### 3. Conversão para Vetores NumPy\n",
    "Embora o formato tf.data.Dataset seja otimizado para alimentar os modelos durante o treinamento, a conversão dos dados para arrays NumPy oferece maior flexibilidade para inspeção manual e para o uso de bibliotecas de análise de dados, como a scikit-learn. A função dataset_to_numpy foi criada para iterar sobre o pipeline de dados, desempacotar os lotes (unbatch) e agregar todas as imagens e rótulos em dois vetores NumPy distintos: um para as imagens e outro para os rótulos correspondentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14305ead-6e4a-4b89-9d67-367743e5ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_batch, label_batch in dataset.unbatch().as_numpy_iterator():\n",
    "        images.append(img_batch)\n",
    "        labels.append(label_batch)\n",
    "\n",
    "    if not images:\n",
    "        print(\"Atenção: Nenhum dado encontrado no dataset.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "print(\"\\nConvertendo dataset de treino para arrays NumPy...\")\n",
    "train_images, train_labels = dataset_to_numpy(train_ds)\n",
    "\n",
    "print(\"Convertendo dataset de validação para arrays NumPy...\")\n",
    "val_images, val_labels = dataset_to_numpy(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2024e-1f11-4d6e-916a-c0bf2bed23b7",
   "metadata": {},
   "source": [
    "#### 4. Verificação da Estrutura e Qualidade dos Dados\n",
    "\n",
    "Como etapa final do pré-processamento, é apresentado um resumo tanto quantitativo quanto qualitativo dos dados preparados. Esta verificação combinada é crucial para validar a integridade estrutural dos vetores e a qualidade visual das amostras antes de iniciar o treinamento.\n",
    "\n",
    "O resumo quantitativo confirma as dimensões (shape) e os tipos de dados (dtype) dos vetores NumPy gerados. A verificação da dimensionalidade — (número_de_amostras, altura, largura, canais_de_cor) — assegura que os dados estão no formato correto esperado pela camada de entrada da rede neural.\n",
    "\n",
    "Complementarmente, uma verificação qualitativa é realizada através da exibição de uma amostra aleatória de cada uma das 14 classes a partir do conjunto de treinamento. Esta inspeção visual serve como uma verificação de sanidade final, confirmando que as imagens foram carregadas e normalizadas corretamente e, mais importante, que a associação entre as imagens e seus respectivos rótulos está correta antes de proceder para a etapa de construção do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc575c5-14d2-4586-828d-ab36f18d37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"RESUMO DOS VETORES GERADOS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"train_images (imagens de treino): {train_images.shape}\")\n",
    "print(f\"train_labels (rótulos de treino): {train_labels.shape}\")\n",
    "print(f\"val_images (imagens de validação): {val_images.shape}\")\n",
    "print(f\"val_labels (rótulos de validação): {val_labels.shape}\")\n",
    "print(f\"Tipo de dado das imagens: {train_images.dtype}\")\n",
    "print(f\"Tipo de dado dos rótulos: {train_labels.dtype}\")\n",
    "\n",
    "if train_labels.size > 0:\n",
    "    exemplo_label_idx = train_labels[0]\n",
    "    nome_classe = class_names[exemplo_label_idx]\n",
    "    print(f\"\\nExemplo: O primeiro rótulo ({exemplo_label_idx}) corresponde à classe: '{nome_classe}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICAÇÃO VISUAL DOS DADOS (Amostra por Classe)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    indices = np.where(train_labels == i)[0]\n",
    "    \n",
    "    if indices.size > 0:\n",
    "        random_index = np.random.choice(indices)\n",
    "        image_to_show = train_images[random_index]\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.imshow(image_to_show)\n",
    "        ax.set_title(class_name)\n",
    "        ax.axis('off')\n",
    "\n",
    "for j in range(len(class_names), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2qSW3YmHPo_",
   "metadata": {
    "id": "p2qSW3YmHPo_"
   },
   "source": [
    "### 6.5.3 Definição de Hiperparâmetros e Configurações do Modelo\n",
    "\n",
    "Antes da construção da arquitetura, é crucial definir todos os hiperparâmetros e configurações que governarão o processo de treinamento. Centralizar estes parâmetros em uma única célula de configuração facilita a experimentação, a reprodutibilidade e a clareza do código.\n",
    "\n",
    "Os parâmetros são agrupados em quatro categorias:\n",
    "\n",
    "- Dados e Modelo: Define as características fundamentais dos dados, como o formato de entrada das imagens (INPUT_SHAPE) e o número total de classes (NUM_CLASSES). Também especifica o nome do arquivo para salvar o modelo de melhor performance (MODEL_FILEPATH).\n",
    "\n",
    "- Arquitetura: Estabelece os valores que moldarão a estrutura da rede, como o tamanho dos filtros convolucionais (KERNEL_SIZE), o fator de subamostragem (POOL_SIZE), o número de neurônios na camada densa (DENSE_UNITS), e as taxas de Dropout para regularização. Crucialmente, define-se o fator de regularização L2 (L2_FACTOR), que penaliza pesos grandes para mitigar o superajuste (overfitting).\n",
    "\n",
    "- Treinamento: Determina os componentes do processo de otimização, incluindo o otimizador (OPTIMIZER), a função de perda (LOSS_FUNCTION), o tamanho dos lotes de dados (BATCH_SIZE) e o número máximo de épocas de treinamento (EPOCHS).\n",
    "\n",
    "- Callbacks: Configura os mecanismos de controle do treinamento, como a métrica a ser monitorada (MONITOR_METRIC) e os parâmetros de paciência (patience) para as funções de parada antecipada e redução da taxa de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91349911-d5ab-461e-aa5f-116a5471e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parâmetros de Dados e Modelo ---\n",
    "INPUT_SHAPE = train_images.shape[1:]\n",
    "NUM_CLASSES = len(class_names)\n",
    "MODEL_FILEPATH = 'modelo_personalizado_v1.keras'\n",
    "\n",
    "# --- Hiperparâmetros da Arquitetura ---\n",
    "KERNEL_SIZE = (3, 3)\n",
    "POOL_SIZE = (2, 2)\n",
    "DENSE_UNITS = 512\n",
    "DROPOUT_CONV = 0.35\n",
    "DROPOUT_DENSE = 0.5\n",
    "L2_FACTOR = 0.001\n",
    "\n",
    "# --- Hiperparâmetros de Treinamento ---\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "\n",
    "# --- Configurações dos Callbacks ---\n",
    "MONITOR_METRIC = 'val_accuracy'\n",
    "ES_PATIENCE = 10\n",
    "RLR_PATIENCE = 3\n",
    "RLR_FACTOR = 0.2\n",
    "RLR_MIN_LR = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cd7a2-16c7-4513-bdb8-7975c908d2fa",
   "metadata": {},
   "source": [
    "### 6.5.4 Preparação dos Rótulos para Classificação\n",
    "O modelo será treinado para uma tarefa de classificação multi-classe utilizando a função de ativação softmax na camada de saída. Esta função produz uma distribuição de probabilidade sobre as N classes. Para que a função de perda categorical_crossentropy possa comparar corretamente a predição do modelo com o rótulo verdadeiro, os rótulos, que estão em formato de inteiros (e.g., 0, 1, 2...), devem ser convertidos para o formato one-hot encoding.\n",
    "\n",
    "Neste formato, cada rótulo é transformado em um vetor binário de tamanho NUM_CLASSES, com todos os elementos sendo 0, exceto pelo índice correspondente à classe, que é 1. A função to_categorical da biblioteca Keras é utilizada para realizar essa conversão de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c146d-ff46-4074-8f69-ec1071f72be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_one_hot = to_categorical(train_labels, NUM_CLASSES)\n",
    "val_labels_one_hot = to_categorical(val_labels, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157556fa-cdeb-4877-b9d0-f274f9c4469d",
   "metadata": {},
   "source": [
    "### 6.5.5 Construção da Arquitetura da Rede Neural Convolucional\n",
    "\n",
    "A arquitetura do modelo foi construída utilizando a API Sequential do Keras, seguindo um design inspirado em arquiteturas VGG, caracterizado pelo empilhamento de blocos convolucionais que progressivamente aumentam a profundidade dos filtros enquanto reduzem a dimensão espacial dos mapas de características.\n",
    "\n",
    "A rede é composta por:\n",
    "\n",
    "- Quatro Blocos Convolucionais: Cada bloco é formado por duas camadas Conv2D com ativação ReLU, seguidas por BatchNormalization para estabilizar e acelerar o treinamento. A profundidade dos filtros aumenta a cada bloco (32, 64, 128, 256), permitindo que a rede aprenda características de complexidade crescente. Ao final de cada bloco, uma camada MaxPool2D realiza a subamostragem (downsampling), reduzindo a dimensionalidade e criando invariância a pequenas translações. Uma camada Dropout é aplicada para regularização.\n",
    "\n",
    "- Regularização L2 (Weight Decay): Para combater o overfitting, a regularização L2 (kernel_regularizer=regularizers.l2(L2_FACTOR)) é aplicada a todas as camadas convolucionais e densas. Esta técnica adiciona um termo de penalidade à função de perda, proporcional ao quadrado do valor dos pesos, incentivando o modelo a aprender pesos menores e, consequentemente, funções mais simples.\n",
    "\n",
    "- Bloco Classificador: Após os blocos convolucionais, uma camada Flatten transforma o mapa de características 2D em um vetor 1D. Este vetor é então processado por uma camada Dense com DENSE_UNITS neurônios e ativação ReLU. BatchNormalization e Dropout são novamente aplicados antes da camada final.\n",
    "\n",
    "- Camada de Saída: A última camada é uma Dense com NUM_CLASSES neurônios e ativação softmax, que produz a distribuição de probabilidade final para a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa272b1f-590e-40ca-b798-4b9d623c56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    # Bloco 1 (32 filtros)\n",
    "    layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, activation='relu', padding='same', input_shape=INPUT_SHAPE, kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Bloco 2 (64 filtros)\n",
    "    layers.Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Bloco 3 (128 filtros)\n",
    "    layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Bloco 4 (256 filtros)\n",
    "    layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Classificador\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(DENSE_UNITS, activation='relu', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(DROPOUT_DENSE),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d4b24-4824-4b94-a049-dc862931a511",
   "metadata": {},
   "source": [
    "### 6.5.6 Compilação do Modelo e Configuração dos Callbacks\n",
    "\n",
    "Com a arquitetura definida, o modelo deve ser compilado. A etapa de compilação configura o processo de treinamento, especificando o otimizador, a função de perda e as métricas de avaliação. Em seguida, são configurados os callbacks, que são mecanismos para monitorar e controlar o treinamento em tempo de execução.\n",
    "\n",
    "Compilação: O modelo é compilado com o otimizador Adam, a função de perda categorical_crossentropy (adequada para a tarefa) e a métrica de accuracy para monitoramento.\n",
    "\n",
    "Callbacks:\n",
    "\n",
    "- ModelCheckpoint: Salva o modelo em disco (MODEL_FILEPATH) apenas quando a métrica monitorada (val_accuracy) melhora, garantindo que a versão de melhor performance seja preservada.\n",
    "\n",
    "- EarlyStopping: Interrompe o treinamento se a val_accuracy não apresentar melhora por um número definido de épocas (ES_PATIENCE), evitando o desperdício de recursos computacionais e o overfitting.\n",
    "\n",
    "- ReduceLROnPlateau: Reduz a taxa de aprendizado por um fator (RLR_FACTOR) se a val_accuracy estagnar, permitindo que o modelo refine sua busca por um mínimo local de forma mais precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83ba99-a5af-457d-a835-407f2e7819e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=LOSS_FUNCTION,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nResumo do Modelo:\")\n",
    "model.summary()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath=MODEL_FILEPATH, monitor=MONITOR_METRIC, save_best_only=True, mode='max', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=MONITOR_METRIC, patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=MONITOR_METRIC, factor=RLR_FACTOR, patience=RLR_PATIENCE, min_lr=RLR_MIN_LR, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d0730-fad6-49cd-8fb6-8947aeb13a54",
   "metadata": {},
   "source": [
    "### 6.5.7 Execução do Processo de Treinamento\n",
    "\n",
    "A etapa de treinamento é iniciada através da chamada ao método model.fit. Este método alimenta o modelo com os dados de treinamento (train_images, train_labels_one_hot) em lotes de tamanho BATCH_SIZE por um número máximo de EPOCHS.\n",
    "\n",
    "O argumento validation_data é crucial, pois fornece o conjunto de dados de validação (val_images, val_labels_one_hot) que será utilizado ao final de cada época para avaliar o desempenho do modelo em dados não vistos. As métricas calculadas neste conjunto de validação são utilizadas pelos callbacks para tomar decisões, como salvar o modelo ou interromper o treinamento. O histórico completo do treinamento, contendo as perdas e métricas de treino e validação por época, é armazenado na variável history para posterior análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab355a-cfb7-42bc-b7c9-afb62568ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIniciando Treinamento...\")\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_images, val_labels_one_hot),\n",
    "    callbacks=[checkpoint_callback, early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0068cea-a214-4391-8b4c-1e5522b00e29",
   "metadata": {},
   "source": [
    "### 6.5.8 Avaliação Final do Modelo de Melhor Performance\n",
    "\n",
    "Após a conclusão do treinamento, a performance final do modelo não é necessariamente a da última época, mas sim a do modelo que atingiu a maior acurácia de validação, salvo pelo callback ModelCheckpoint.\n",
    "\n",
    "Portanto, a etapa final consiste em carregar este modelo de melhor performance a partir do arquivo salvo (MODEL_FILEPATH) e reavaliá-lo no conjunto de validação. O método evaluate retorna a perda (loss) e a acurácia (accuracy) finais, que representam a estimativa mais confiável da capacidade de generalização do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1caca-9e66-4b3d-83c3-458805409790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCarregando o melhor modelo salvo em '{MODEL_FILEPATH}'...\")\n",
    "best_model = models.load_model(MODEL_FILEPATH)\n",
    "\n",
    "print(\"\\nAvaliando o desempenho do MELHOR modelo no conjunto de validação:\")\n",
    "loss, accuracy = best_model.evaluate(val_images, val_labels_one_hot)\n",
    "\n",
    "print(f\"\\n-> Acurácia do MELHOR modelo no conjunto de validação: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3993a-a22c-4543-a6bd-d2e6366ae424",
   "metadata": {},
   "source": [
    "### 6.5.9 Análise de Desempenho e Validação Aprofundada do Modelo\n",
    "A avaliação final de um modelo de classificação vai além da acurácia geral, exigindo uma análise aprofundada de seu comportamento durante o treinamento e de sua performance em métricas mais granulares. Esta seção detalha a validação do modelo de melhor performance, utilizando ferramentas visuais e relatórios de classificação para obter uma compreensão completa de seus pontos fortes e fracos.\n",
    "\n",
    "A análise é conduzida em duas frentes:\n",
    "\n",
    "1. Análise da Dinâmica de Treinamento: As curvas de acurácia e perda (loss) ao longo das épocas, tanto para o conjunto de treinamento quanto para o de validação, são plotadas. Estes gráficos são ferramentas diagnósticas essenciais. A convergência das curvas indica um aprendizado estável, enquanto a divergência entre as curvas de treinamento e validação pode sinalizar overfitting (superajuste), onde o modelo memoriza os dados de treino em detrimento de sua capacidade de generalização.\n",
    "\n",
    "2. Avaliação Detalhada da Performance: Para avaliar o desempenho no conjunto de validação, são geradas previsões e calculadas as seguintes métricas, compiladas em um relatório de classificação:\n",
    "\n",
    "    - Precisão (Precision): Mede a proporção de predições positivas que foram de fato corretas. É um indicador da exatidão do modelo quando ele prevê uma classe específica.\n",
    "\n",
    "    - Revocação (Recall): Mede a proporção de instâncias positivas reais que foram corretamente identificadas pelo modelo. É um indicador da capacidade do modelo de encontrar todas as amostras de uma classe específica.\n",
    "\n",
    "    - F1-Score: É a média harmônica entre precisão e revocação, fornecendo uma única métrica que equilibra ambos os aspectos. É particularmente útil quando há um desequilíbrio entre as classes.\n",
    "\n",
    "    -  Matriz de Confusão: Esta matriz visualiza o desempenho do modelo de forma detalhada, mostrando o número de predições corretas e incorretas para cada classe. A diagonal principal representa as classificações corretas, enquanto os valores fora da diagonal indicam os erros, permitindo identificar quais classes são frequentemente confundidas entre si.\n",
    "\n",
    "O código a seguir implementa esta análise completa, gerando os gráficos de histórico,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78571a-8224-44d7-b143-02d0a3cf09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
    "    ax1.set_title('Acurácia do Modelo', fontsize=16)\n",
    "    ax1.set_xlabel('Época', fontsize=12)\n",
    "    ax1.set_ylabel('Acurácia', fontsize=12)\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(history.history['loss'], label='Loss de Treino')\n",
    "    ax2.plot(history.history['val_loss'], label='Loss de Validação')\n",
    "    ax2.set_title('Loss (Perda) do Modelo', fontsize=16)\n",
    "    ax2.set_xlabel('Época', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nGerando previsões para o conjunto de validação...\")\n",
    "y_pred_probs = best_model.predict(val_images)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = val_labels\n",
    "\n",
    "print(\"\\n# ========================================= #\")\n",
    "print(\"#     RELATÓRIO DE CLASSIFICAÇÃO      #\")\n",
    "print(\"# ========================================= #\\n\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "\n",
    "print(\"\\nGerando a Matriz de Confusão...\")\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confusão', fontsize=20)\n",
    "plt.ylabel('Classe Verdadeira', fontsize=15)\n",
    "plt.xlabel('Classe Prevista', fontsize=15)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QTg2n8lFGRRW",
   "metadata": {
    "id": "QTg2n8lFGRRW"
   },
   "source": [
    "## 6.6 Treinamento com Aprendizado por Transferência (Transfer Learning)\n",
    "\n",
    "Nesta seção, adota-se a metodologia de aprendizado por transferência para treinar um classificador de aves. A arquitetura selecionada é a Vision Transformer (ViT), um modelo que aplica o mecanismo de atenção, originalmente da área de Processamento de Linguagem Natural, ao domínio da visão computacional. O processo consiste em utilizar um modelo ViT pré-treinado na base de dados ImageNet, congelar os pesos de sua base extratora de características (backbone), e treinar exclusivamente uma nova camada de classificação (\"cabeça\") adaptada para as 14 espécies de psitacídeos do nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pSLnMcdNTfIh",
   "metadata": {
    "id": "pSLnMcdNTfIh"
   },
   "source": [
    "### 6.6.1 Importação de Bibliotecas e Frameworks\n",
    "\n",
    "A etapa inaugural do processo experimental consiste na importação das bibliotecas e frameworks que constituem a fundação do ambiente de desenvolvimento. A escolha recai sobre o fastai, um framework de alto nível, construído sobre PyTorch, que encapsula as melhores práticas da área de aprendizado profundo. Sua utilização visa abstrair complexidades operacionais, permitindo um foco maior na experimentação e na aplicação de técnicas avançadas, como políticas de treinamento otimizadas e pipelines de dados eficientes. Implicitamente, o fastai integra a biblioteca timm (PyTorch Image Models), um repositório extensivo que oferece acesso a um vasto leque de arquiteturas de visão computacional estado-da-arte, incluindo a família de modelos Vision Transformer. Complementarmente, as bibliotecas padrão os e pathlib são empregadas para a manipulação de caminhos e diretórios, garantindo a portabilidade e a robustez do código em diferentes sistemas operacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nlmFEkSfTbaQ",
   "metadata": {
    "id": "nlmFEkSfTbaQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b47f66-1597-4d7a-a605-846d41e45e45",
   "metadata": {},
   "source": [
    "### 6.6.2 Configuração de Parâmetros e Hiperparâmetros\n",
    "\n",
    "Nesta fase, realiza-se a definição centralizada de todos os parâmetros e hiperparâmetros que regem o experimento, uma prática essencial para garantir a consistência e a reprodutibilidade dos resultados. São estabelecidos os caminhos para os diretórios contendo os dados de treinamento e validação, bem como o diretório de destino para o armazenamento dos artefatos do modelo treinado. Os hiperparâmetros, que definem a configuração do treinamento, são cuidadosamente especificados:\n",
    "\n",
    "- IMG_SIZE: A resolução das imagens de entrada é fixada em 224x224 pixels. Esta dimensão não é arbitrária; ela corresponde à resolução com a qual a arquitetura ViT selecionada foi pré-treinada na base de dados ImageNet, sendo, portanto, um requisito para a correta utilização dos pesos transferidos.\n",
    "\n",
    "- BATCH_SIZE: O tamanho do lote de dados é definido como 64. Este hiperparâmetro influencia diretamente a estimativa do gradiente durante a otimização, a utilização da memória da GPU e a velocidade de treinamento.\n",
    "\n",
    "- LEARNING_RATE e EPOCHS: A taxa de aprendizado máxima (2e-3) e o número máximo de épocas (100) são definidos como os principais controladores do processo de otimização, ditando a magnitude das atualizações dos pesos e a duração total do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63579e32-e599-4153-92c2-e3eb9c54ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('./datasets')\n",
    "SAVE_PATH = Path('./transfer_learning_models')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dir_name = 'train_images'\n",
    "val_dir_name = 'val_images'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-3\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054c9a9-9edf-4647-8d29-8991882dd9a6",
   "metadata": {},
   "source": [
    "### 6.6.3 Construção do Pipeline de Dados com ImageDataLoaders\n",
    "\n",
    "A construção de um pipeline de dados eficiente e robusto é realizada por meio da classe ImageDataLoaders do fastai. Este objeto de alto nível automatiza a criação de um pipeline completo que abrange desde a leitura dos arquivos de imagem até a aplicação de transformações complexas. O processo é configurado da seguinte forma: as classes são inferidas a partir da estrutura de subdiretórios; uma transformação ao nível do item (item_tfms=Resize(IMG_SIZE)) é aplicada para garantir que todas as imagens sejam redimensionadas para a resolução de entrada uniforme exigida pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292849e-1835-494e-8782-9d6f8bd1eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(\n",
    "    BASE_PATH,\n",
    "    train=train_dir_name,\n",
    "    valid=val_dir_name,\n",
    "    item_tfms=Resize(IMG_SIZE),\n",
    "    bs=BATCH_SIZE\n",
    ")\n",
    "\n",
    "num_classes = len(dls.vocab)\n",
    "print(f\"Classes detectadas ({num_classes}): {dls.vocab}\")\n",
    "dls.show_batch(max_n=9, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff1d77-1a34-495d-bcec-43196b0ea231",
   "metadata": {},
   "source": [
    "### 6.6.4 Instanciação do Modelo e Implementação da Estratégia de Congelamento\n",
    "\n",
    "UNesta etapa, o modelo Vision Transformer é instanciado através da função vision_learner, que baixa a arquitetura vit_small_patch32_224 com seus pesos pré-treinados e anexa uma nova \"cabeça\" de classificação, inicializada aleatoriamente, com um número de saídas correspondente ao número de classes do nosso problema.\n",
    "\n",
    "Subsequentemente, a estratégia central do transfer learning é implementada através do congelamento explícito dos pesos do backbone. O modelo no fastai é estruturado em duas partes: o corpo extrator de características (learn.model[0]) e a cabeça de classificação (learn.model[1]). O código itera sobre todos os parâmetros do backbone e define seu atributo requires_grad como False. Esta operação instrui o framework de otimização (PyTorch) a não calcular gradientes para estes parâmetros durante a fase de retropropagação, efetivamente os \"congelando\". Apenas os parâmetros da nova cabeça de classificação permanecem treináveis (requires_grad = True), focando o aprendizado exclusivamente na tarefa de mapear as características de alto nível, extraídas pelo backbone, para as classes de aves específicas do nosso dataset. Por fim, o otimizador é recriado para garantir que ele esteja ciente do novo conjunto de parâmetros treináveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71143399-b6ec-4278-a1a4-a9c2978ff648",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(\n",
    "    dls,\n",
    "    'vit_small_patch32_224',\n",
    "    metrics=[accuracy, error_rate]\n",
    ")\n",
    "\n",
    "print(\"\\n🧊 Aplicando congelamento explícito — todas as camadas do backbone serão congeladas...\")\n",
    "\n",
    "for p in learn.model[0].parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in learn.model[1].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "learn.create_opt()\n",
    "\n",
    "print(\"\\n🔍 Verificando camadas congeladas e treináveis:\")\n",
    "trainable, frozen = 0, 0\n",
    "for name, param in learn.model.named_parameters():\n",
    "    status = \"🔓 treinável\" if param.requires_grad else \"🧊 congelado\"\n",
    "    if param.requires_grad: trainable += 1\n",
    "    else: frozen += 1\n",
    "print(f\"Resumo: 🧊 {frozen} camadas congeladas | 🔓 {trainable} camadas treináveis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760d1e7-fa90-407a-bdbe-8b23c15a1c33",
   "metadata": {},
   "source": [
    "### 6.6.5 Treinamento da Camada de Classificação\n",
    "\n",
    "Com o corpo do modelo congelado, o processo de treinamento é iniciado, focando exclusivamente na otimização dos pesos da nova cabeça de classificação. São configurados os callbacks EarlyStoppingCallback e SaveModelCallback como mecanismos de controle empírico. O primeiro previne o overfitting ao interromper o treinamento caso a performance de validação estagne, enquanto o segundo garante a persistência do modelo que alcançou o melhor desempenho. O treinamento é conduzido pelo método fit_one_cycle, que implementa a política de treinamento \"1-cycle\". Esta técnica avançada modula a taxa de aprendizado de forma cíclica ao longo das épocas, começando baixa, aumentando até um máximo e depois decaindo. Essa abordagem tem se mostrado eficaz para acelerar a convergência e navegar de forma mais eficiente pelo espaço de perda, frequentemente resultando em modelos com melhor capacidade de generalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ee308-4b16-4276-b392-7223499fe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStoppingCallback(\n",
    "        monitor='accuracy',\n",
    "        patience=10,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    "    SaveModelCallback(\n",
    "        monitor='accuracy',\n",
    "        fname='best_model_vit'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n🚀 Iniciando treinamento (apenas a cabeça será ajustada)...\")\n",
    "learn.fit_one_cycle(\n",
    "    EPOCHS,\n",
    "    lr_max=LEARNING_RATE,\n",
    "    cbs=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6287ca7-aec5-4861-aaa1-310e58c88462",
   "metadata": {},
   "source": [
    "### 6.6.6 Análise Diagnóstica, Validação Final e Serialização do Modelo\n",
    "\n",
    "A avaliação da performance de um modelo de classificação transcende a simples mensuração da acurácia. Para obter uma compreensão completa do comportamento do modelo, é imperativo realizar uma análise mais granular de seu desempenho. Nesta seção, o modelo de melhor performance, restaurado pelo callback, é submetido a uma validação aprofundada.\n",
    "\n",
    "Para tal, é gerado um relatório de classificação textual, que sumariza métricas essenciais como Precisão, Revocação e F1-Score para cada uma das 14 classes. Em conjunto, estas métricas oferecem uma visão granular do equilíbrio do modelo entre os diferentes tipos de erro de classificação (falsos positivos e falsos negativos). Adicionalmente, uma Matriz de Confusão é gerada para visualizar de forma direta os padrões de erro, permitindo identificar quais classes são sistematicamente confundidas entre si. A análise é complementada pela visualização das amostras com maior erro de predição (top losses), uma ferramenta diagnóstica para inspecionar os casos mais desafiadores para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67d5e0-dc47-4a47-a438-6ea3b1d1421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Avaliando os resultados do modelo treinado...\")\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Extrai os rótulos verdadeiros e as predições\n",
    "y_true = interp.targs.numpy()\n",
    "y_pred = interp.preds.argmax(dim=1).numpy()\n",
    "class_names = dls.vocab\n",
    "\n",
    "print(\"\\n# =================================================== #\")\n",
    "print(\"#     RELATÓRIO DE CLASSIFICAÇÃO DETALHADO      #\")\n",
    "print(\"# =================================================== #\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "print(\"\\nGerando a Matriz de Confusão...\")\n",
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
    "\n",
    "print(\"\\nExibindo amostras com maior erro de predição (Top Losses)...\")\n",
    "interp.plot_top_losses(9, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e180c-c389-4eba-b63c-01d60c75e97a",
   "metadata": {},
   "source": [
    "### 6.6.7 Validação Final e Exportação do Artefato do Modelo\n",
    "\n",
    "Como etapa conclusiva do processo experimental, é realizada a validação final para registrar a acurácia global do modelo de melhor performance e, em seguida, o artefato do modelo é serializado e exportado. O método learn.validate() é invocado para recalcular e exibir as métricas de perda (loss) e acurácia no conjunto de dados de validação, fornecendo um registro quantitativo final e definitivo do desempenho do modelo.\n",
    "\n",
    "Posteriormente, o objeto Learner treinado é exportado para um arquivo .pkl através do método learn.export(). Este processo de serialização é crítico, pois encapsula não apenas a arquitetura da rede neural e seus pesos otimizados, mas todo o pipeline de transformações de dados (e.g., redimensionamento, normalização, aumento de dados) necessário para o pré-processamento de novas imagens. O resultado é um artefato de modelo autocontido, que pode ser facilmente carregado em ambientes de produção para realizar inferências em novos dados, garantindo que o pré-processamento aplicado na inferência seja idêntico ao utilizado durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793fdd8-b40b-4bfb-9e5f-13feb487eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculando as métricas finais do melhor modelo no conjunto de validação...\")\n",
    "final_metrics = learn.validate()\n",
    "\n",
    "final_accuracy = final_metrics[1]\n",
    "print(f\"\\n{'='*35}\")\n",
    "print(f\"  Acurácia Final de Validação: {final_accuracy*100:.4f}%\")\n",
    "print(f\"{'='*35}\\n\")\n",
    "\n",
    "final_model_path = SAVE_PATH / \"modelo_vit_transfer_learning.pkl\"\n",
    "learn.export(final_model_path)\n",
    "\n",
    "print(f\"✅ Treinamento e validação concluídos.\")\n",
    "print(f\"💾 O melhor modelo (pesos) foi salvo em: {learn.path/learn.model_dir}/best_model_vit.pth\")\n",
    "print(f\"💾 O modelo final (artefato completo) foi exportado para: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D_K9T2ewSzjQ",
   "metadata": {
    "id": "D_K9T2ewSzjQ"
   },
   "source": [
    "## 6.7 Treinamento com a Estratégia de Fine-Tuning\n",
    "\n",
    "A técnica de fine-tuning (ajuste fino) é uma metodologia de aprendizado por transferência mais avançada, que visa adaptar não apenas a camada de classificação, mas também as representações de características aprendidas pelo corpo do modelo (backbone). Diferentemente da abordagem de congelamento total, o fine-tuning opera em duas fases: primeiro, treina-se apenas a nova \"cabeça\" de classificação com o backbone congelado; em seguida, o modelo inteiro é \"descongelado\" e treinado de ponta a ponta, tipicamente com taxas de aprendizado diferenciais (discriminative learning rates). Esta abordagem permite que as características genéricas aprendidas no dataset original (e.g., ImageNet) sejam sutilmente ajustadas para se tornarem mais específicas ao domínio do novo problema. Para este experimento, a arquitetura selecionada é a ResNet50, uma rede neural convolucional de referência, conhecida por sua eficácia e pelo uso de conexões residuais para mitigar o problema do gradiente evanescente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lg0HRQSnS76o",
   "metadata": {
    "id": "lg0HRQSnS76o"
   },
   "source": [
    "### 6.7.1 Importação de Bibliotecas e Frameworks\n",
    "\n",
    "A etapa inaugural consiste na importação das bibliotecas que formam a base do experimento. O framework fastai é novamente empregado como uma interface de alto nível sobre PyTorch, pois suas abstrações são particularmente poderosas para implementar estratégias de treinamento complexas como o fine-tuning de forma concisa e eficaz. As bibliotecas os e pathlib são utilizadas para a manipulação de caminhos de arquivos de forma independente do sistema operacional, garantindo a robustez do código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVRkAm5KTwLo",
   "metadata": {
    "id": "IVRkAm5KTwLo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6762ce-c3c3-4181-abe3-e28a43a71507",
   "metadata": {},
   "source": [
    "### 6.7.2 Configuração de Parâmetros e Hiperparâmetros do Experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c6894-ab25-41a6-bd56-20f46cd3b36e",
   "metadata": {},
   "source": [
    "Nesta fase, realiza-se a definição centralizada de todos os parâmetros e hiperparâmetros que regem o experimento. São estabelecidos os caminhos para os diretórios de dados. Os hiperparâmetros críticos para a estratégia de fine-tuning são especificados:\n",
    "\n",
    "- IMG_SIZE e BATCH_SIZE: A resolução de entrada (224x224) é mantida para ser compatível com as dimensões de pré-treinamento da ResNet50, e o tamanho do lote é definido para otimizar o uso de recursos computacionais.\n",
    "\n",
    "- HEAD_LEARNING_RATE: Esta é a taxa de aprendizado que será utilizada para treinar a cabeça de classificação durante a primeira fase do fine-tuning e como taxa de aprendizado base para as camadas finais na segunda fase.\n",
    "\n",
    "- EPOCHS: Define o número máximo de épocas para a segunda fase do fine-tuning, na qual o modelo inteiro é treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a8e43-5071-4920-b0da-f083f13e5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('./datasets')\n",
    "SAVE_PATH = Path('./finetuning_models')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dir_name = 'train_images'\n",
    "val_dir_name = 'val_images'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "HEAD_LEARNING_RATE = 2e-3\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d9ebb-37ff-4f7c-a032-7ca7e4bd9851",
   "metadata": {},
   "source": [
    "### 6.7.3 Construção do Pipeline de Dados\n",
    "\n",
    "O carregamento e pré-processamento dos dados são gerenciados pela classe ImageDataLoaders do fastai. Este objeto constrói um pipeline de dados que automatiza a inferência de classes a partir da estrutura de diretórios e aplica uma transformação de redimensionamento (item_tfms=Resize(IMG_SIZE)) para uniformizar as dimensões das imagens. É importante notar que, nesta configuração, não foram explicitamente adicionadas transformações de aumento de dados, focando na capacidade do fine-tuning de adaptar os pesos do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8f67b-7c03-47ee-ad46-26302d700a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(\n",
    "    BASE_PATH,\n",
    "    train=train_dir_name,\n",
    "    valid=val_dir_name,\n",
    "    item_tfms=Resize(IMG_SIZE),\n",
    "    bs=BATCH_SIZE\n",
    ")\n",
    "\n",
    "num_classes = len(dls.vocab)\n",
    "print(f\"Classes detectadas ({num_classes}): {dls.vocab}\")\n",
    "dls.show_batch(max_n=9, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ed8ba-430a-4a89-a164-efe4d775677a",
   "metadata": {},
   "source": [
    "### 6.7.4 Instanciação do Modelo e Configuração dos Callbacks\n",
    "\n",
    "Um objeto Learner é instanciado através da função vision_learner, que baixa a arquitetura resnet50 com pesos pré-treinados no ImageNet, anexa uma nova cabeça de classificação adaptada ao número de classes do nosso problema e configura as métricas de accuracy e error_rate para monitoramento. A taxa de aprendizado padrão do otimizador é definida como HEAD_LEARNING_RATE.\n",
    "\n",
    "Os callbacks EarlyStoppingCallback e SaveModelCallback são configurados para controlar o treinamento de forma robusta. O primeiro interrompe o processo caso a performance de validação estagne, prevenindo o overfitting, enquanto o segundo garante a persistência do modelo que alcançou o melhor desempenho ao longo de todas as épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30238a-c40c-496d-8bfa-aa50abaa1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(\n",
    "    dls,\n",
    "    resnet50,\n",
    "    metrics=[accuracy, error_rate],\n",
    "    lr=HEAD_LEARNING_RATE\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStoppingCallback(\n",
    "        monitor='accuracy',\n",
    "        patience=10,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    "    SaveModelCallback(\n",
    "        monitor='accuracy',\n",
    "        fname='best_model_resnet50_finetuned'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e2e61-b8b8-4018-86f4-ea20c6ee4bab",
   "metadata": {},
   "source": [
    "### 6.7.5 Execução da Estratégia de Fine-Tuning\n",
    "A execução do fine-tuning é encapsulada pelo método learn.fine_tune(), que automatiza a estratégia de duas fases:\n",
    "\n",
    "- Fase 1 (Congelada): O corpo do modelo (backbone) é mantido congelado, e apenas a nova cabeça de classificação é treinada por um número fixo de épocas (por padrão, uma época). Esta etapa permite que a camada de saída se ajuste rapidamente às novas classes sem perturbar os pesos pré-treinados.\n",
    "\n",
    "- Fase 2 (Descongelada): O modelo inteiro é descongelado, e todas as camadas são treinadas por EPOCHS épocas. Crucialmente, o fine_tune aplica taxas de aprendizado discriminatórias. A taxa de aprendizado fornecida (base_lr=BASE_LR_BODY) é aplicada às camadas mais iniciais do backbone, e o fastai interpola gradualmente as taxas de aprendizado para as camadas subsequentes, até atingir a taxa de aprendizado original (HEAD_LEARNING_RATE) na cabeça de classificação. Esta técnica é fundamental, pois permite que as características mais genéricas (nas primeiras camadas) sejam ajustadas de forma sutil (com uma taxa de aprendizado baixa), enquanto as características mais específicas (nas camadas finais) são ajustadas de forma mais agressiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3bf85-c4d7-42d0-9995-a06c7e1ebcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando treinamento com ResNet50 e a estratégia de Fine-Tuning...\")\n",
    "\n",
    "BASE_LR_BODY = 1e-4\n",
    "\n",
    "learn.fine_tune(\n",
    "    EPOCHS,\n",
    "    base_lr=BASE_LR_BODY,\n",
    "    cbs=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac09d3-fa01-4325-a3a3-877e9fb12c8d",
   "metadata": {},
   "source": [
    "### 6.7.6 Análise de Desempenho e Validação de Métricas\n",
    "\n",
    "Para uma avaliação de desempenho robusta do modelo final, uma análise aprofundada é conduzida. Esta etapa vai além da acurácia global para fornecer uma compreensão granular do comportamento do modelo. Para tal, é gerado um relatório de classificação textual, que sumariza um conjunto de métricas essenciais, incluindo Acurácia, Precisão, Recall e F1-Score para cada uma das 14 classes. Em complemento, uma Matriz de Confusão é visualizada para permitir a inspeção direta dos padrões de erro de classificação entre as espécies. A análise é finalizada com a exibição das amostras que geraram o maior erro de predição (top losses), uma ferramenta diagnóstica para inspecionar os casos mais desafiadores para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cc0a7-8f2d-4547-ba48-84ca8128956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Avaliando os resultados do modelo treinado...\")\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Extrai os rótulos verdadeiros e as predições\n",
    "y_true = interp.targs.numpy()\n",
    "y_pred = interp.preds.argmax(dim=1).numpy()\n",
    "class_names_report = dls.vocab\n",
    "\n",
    "print(\"\\n# =================================================== #\")\n",
    "print(\"#     RELATÓRIO DE CLASSIFICAÇÃO DETALHADO      #\")\n",
    "print(\"# =================================================== #\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_report))\n",
    "\n",
    "print(\"\\nGerando a Matriz de Confusão...\")\n",
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
    "\n",
    "print(\"\\nExibindo amostras com maior erro de predição (Top Losses)...\")\n",
    "interp.plot_top_losses(9, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728eacd-85c3-4b2e-97ee-f01775cf98c4",
   "metadata": {},
   "source": [
    "### 6.7.7 Análise de Desempenho e Validação de Métricas\n",
    "\n",
    "Como etapa conclusiva do processo experimental, é realizada a validação final para registrar a acurácia global do modelo de melhor performance. O método learn.validate() é invocado para recalcular e exibir as métricas de perda (loss) e acurácia no conjunto de dados de validação, fornecendo um registro quantitativo final e definitivo do desempenho do modelo.\n",
    "\n",
    "Posteriormente, o objeto Learner treinado é exportado para um arquivo .pkl. Este processo de serialização encapsula a arquitetura, os pesos otimizados e todo o pipeline de transformações de dados, criando um artefato de modelo autocontido e pronto para ser implantado em ambientes de inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce95002-8291-42cd-8a0f-eaae9c4a9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculando as métricas finais do melhor modelo no conjunto de validação...\")\n",
    "final_metrics = learn.validate()\n",
    "\n",
    "final_accuracy = final_metrics[1]\n",
    "print(f\"\\n{'='*35}\")\n",
    "print(f\"  Acurácia Final de Validação: {final_accuracy*100:.4f}%\")\n",
    "print(f\"{'='*35}\\n\")\n",
    "\n",
    "final_model_path = SAVE_PATH / \"modelo_resnet50_finetuned.pkl\"\n",
    "learn.export(final_model_path)\n",
    "\n",
    "print(f\"✅ Treinamento e validação concluídos.\")\n",
    "print(f\"💾 O melhor modelo (pesos) foi salvo em: {learn.path/learn.model_dir}/best_model_resnet50_finetuned.pth\")\n",
    "print(f\"💾 O modelo final (artefato completo) foi exportado para: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CG7VJ0wXML_v",
   "metadata": {
    "id": "CG7VJ0wXML_v"
   },
   "source": [
    "# 7. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZZkqZgFCT0LC",
   "metadata": {
    "id": "ZZkqZgFCT0LC"
   },
   "source": [
    "## 7.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12pD5TTqTz3t",
   "metadata": {
    "id": "12pD5TTqTz3t"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVB44ouWTkK3",
   "metadata": {
    "id": "rVB44ouWTkK3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Para evitar um erro comum no Windows\n",
    "# temp = pathlib.PosixPath\n",
    "# pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# ===================================================================\n",
    "# 1. CONFIGURE OS CAMINHOS AQUI\n",
    "# ===================================================================\n",
    "\n",
    "# Altere esta linha para o caminho exato onde seu modelo .pkl foi salvo.\n",
    "# Exemplo: \"/home/lab11/papagaio/modelo_resnet50_finetuned_fastai.pkl\"\n",
    "caminho_do_modelo = \"/content/modelos/modelo_resnet50_finetuned_fastai.pkl\"  # <--- MUDE AQUI\n",
    "\n",
    "# Altere esta linha para o caminho da imagem que você quer classificar.\n",
    "# Exemplo: \"imagens/teste/pardal_01.jpg\"\n",
    "caminho_da_imagem = \"/home/lab11/papagaio/papagaio-main/image copy 3.png\"  # <--- MUDE AQUI\n",
    "\n",
    "# ===================================================================\n",
    "# 2. FUNÇÃO PARA CARREGAR O MODELO E PREVER A IMAGEM\n",
    "# ===================================================================\n",
    "\n",
    "def prever_imagem(caminho_modelo, caminho_imagem):\n",
    "    \"\"\"\n",
    "    Carrega um modelo treinado da fastai e faz a previsão em uma única imagem.\n",
    "    \"\"\"\n",
    "    # ---- Carregamento do Modelo ----\n",
    "    try:\n",
    "        print(f\"Carregando modelo de: {caminho_modelo}\")\n",
    "        # load_learner carrega tudo o que é necessário para a inferência\n",
    "        learn = load_learner(caminho_modelo)\n",
    "        print(\"Modelo carregado com sucesso!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: Arquivo do modelo não encontrado em '{caminho_modelo}'. Verifique o caminho.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao carregar o modelo: {e}\")\n",
    "        return\n",
    "\n",
    "    # ---- Previsão na Imagem ----\n",
    "    try:\n",
    "        print(f\"\\nFazendo previsão na imagem: {caminho_imagem}\")\n",
    "\n",
    "        # O método .predict faz todo o processamento necessário na imagem\n",
    "        classe_predita, _, probabilidades = learn.predict(caminho_imagem)\n",
    "\n",
    "        # Exibe a imagem com o resultado\n",
    "        img = Image.open(caminho_imagem)\n",
    "        plt.imshow(img)\n",
    "        # Pega a probabilidade da classe prevista\n",
    "        prob_max = probabilidades.max()\n",
    "        plt.title(f\"Previsão: {classe_predita}\\nConfiança: {prob_max*100:.2f}%\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # ---- Mostra o resultado detalhado no terminal ----\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"      RESULTADO DA CLASSIFICAÇÃO\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"A imagem foi classificada como: '{classe_predita}'\")\n",
    "\n",
    "        # Mostra o top 5 de probabilidades para ter mais detalhes\n",
    "        print(\"\\nTop 5 probabilidades:\")\n",
    "        # Combina os nomes das classes (vocab) com suas probabilidades\n",
    "        prob_por_classe = sorted(zip(learn.dls.vocab, probabilidades), key=lambda x: x[1], reverse=True)\n",
    "        for i, (classe, prob) in enumerate(prob_por_classe[:5]):\n",
    "            print(f\"{i+1}. {classe}: {prob*100:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: Arquivo de imagem não encontrado em '{caminho_imagem}'. Verifique o caminho.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a previsão: {e}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3. EXECUÇÃO PRINCIPAL\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    prever_imagem(caminho_do_modelo, caminho_da_imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0LpNqrQZTWCg",
   "metadata": {
    "id": "0LpNqrQZTWCg"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 1) CAMINHOS\n",
    "# ===================================================================\n",
    "caminho_do_modelo = \"/content/modelos/modelo_resnet50_finetuned_fastai.pkl\"\n",
    "caminho_teste     = \"/content/datasets/test_images\"\n",
    "\n",
    "# (Opcional) classes que você informou; usaremos APENAS para validar nomes/pastas\n",
    "class_names = [\n",
    "    \"amazona_aestiva\",\n",
    "    \"amazona_amazonica\",\n",
    "    \"anodorhynchus_hyacinthinus\",\n",
    "    \"ara_ararauna\",\n",
    "    \"ara_chloropterus\",\n",
    "    \"ara_macao\",\n",
    "    \"brotogeris_chiriri\",\n",
    "    \"diopsittaca_nobilis\",\n",
    "    \"eupsittula_aurea\",\n",
    "    \"forpus_xanthopterygius\",\n",
    "    \"orthopsittaca_manilatus\",\n",
    "    \"primolius_maracana\",\n",
    "    \"psittacara_leucophthalmus\",\n",
    "    \"touit_melanonotus\",\n",
    "]\n",
    "\n",
    "# ===================================================================\n",
    "# 2) FUNÇÃO\n",
    "# ===================================================================\n",
    "def avaliar_modelo_no_teste(caminho_modelo, caminho_teste, class_names=None):\n",
    "    # 1) Carrega o modelo\n",
    "    print(f\"Carregando modelo de: {caminho_modelo}\")\n",
    "    learn = load_learner(caminho_modelo)\n",
    "    print(\"✅ Modelo carregado com sucesso!\\n\")\n",
    "\n",
    "    if not hasattr(learn.dls, \"vocab\") or learn.dls.vocab is None:\n",
    "        raise RuntimeError(\"Este Learner não tem 'vocab' — o modelo provavelmente não é de classificação.\")\n",
    "\n",
    "    vocab = list(map(str, learn.dls.vocab))\n",
    "    print(\"🔠 Vocab do modelo:\", vocab)\n",
    "\n",
    "    # 2) Valida classes (opcional)\n",
    "    if class_names is not None:\n",
    "        print(\"🔤 Labels fornecidas:\", class_names)\n",
    "        if set(class_names) != set(vocab):\n",
    "            faltando = [c for c in vocab if c not in class_names]\n",
    "            extras   = [c for c in class_names if c not in vocab]\n",
    "            raise RuntimeError(\n",
    "                \"As classes do modelo e as fornecidas diferem.\\n\"\n",
    "                f\"Faltando no seu 'class_names': {faltando}\\n\"\n",
    "                f\"Extras no seu 'class_names': {extras}\"\n",
    "            )\n",
    "\n",
    "    # 3) Coleta arquivos do teste e rótulos a partir da subpasta\n",
    "    base = Path(caminho_teste)\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Pasta de teste não encontrada: {base}\")\n",
    "\n",
    "    files = get_image_files(base)\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(\"Nenhuma imagem encontrada no conjunto de teste.\")\n",
    "\n",
    "    # Filtra só arquivos cujas pastas são classes válidas\n",
    "    valid_classes = set(vocab)\n",
    "    files = [f for f in files if f.parent.name in valid_classes]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(\"As imagens não estão em subpastas com nomes de classes do modelo.\")\n",
    "\n",
    "    # Contagem por classe (diagnóstico)\n",
    "    contagem = collections.Counter(f.parent.name for f in files)\n",
    "    print(\"📦 Imagens por classe no teste:\", dict(contagem))\n",
    "    print(f\"📦 Total de imagens no teste: {len(files)}\")\n",
    "\n",
    "    # 4) Monta rótulos verdadeiros (targs) como índices do vocab\n",
    "    cls2idx = {c:i for i,c in enumerate(vocab)}\n",
    "    targs_idx = tensor([cls2idx[f.parent.name] for f in files]).long()\n",
    "\n",
    "    # 5) Cria um test_dl **sem** rótulos (apenas itens). Isso evita o erro de tuplas.\n",
    "    test_dl = learn.dls.test_dl(files)  # sem with_labels\n",
    "\n",
    "    # 6) Predições e acurácia manual\n",
    "    print(\"Calculando predições...\")\n",
    "    preds = learn.get_preds(dl=test_dl)[0]  # só preds; targs vazio pq não há labels no dl\n",
    "    pred_idx = preds.argmax(dim=1)\n",
    "\n",
    "    acc = (pred_idx == targs_idx).float().mean()\n",
    "    print(f\"\\n📊 Acurácia no conjunto de teste: {acc.item()*100:.2f}%\")\n",
    "\n",
    "    # (Opcional) mostra matriz de confusão rapidamente\n",
    "    try:\n",
    "        from fastai.metrics import ConfusionMatrix\n",
    "        cm = ConfusionMatrix()\n",
    "        cm.add(pred_idx, targs_idx)\n",
    "        print(\"\\nMatriz de confusão (primeiras linhas):\")\n",
    "        print(cm[:min(10, len(vocab)), :min(10, len(vocab))])  # evita imprimir gigante\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "# ===================================================================\n",
    "# 3) EXECUÇÃO\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    avaliar_modelo_no_teste(caminho_do_modelo, caminho_teste, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7KxBDsRjytN",
   "metadata": {
    "id": "A7KxBDsRjytN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sXznSQCDeSzV",
   "metadata": {
    "id": "sXznSQCDeSzV"
   },
   "source": [
    "A acurácia de validação obtida para os três modelos foi:\n",
    "\n",
    "* Rede criada: 0.8864\n",
    "* Transfer Learning: 0.998243\n",
    "* Fine-Tuning: 0.996779\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zp_0AS8TMJEX",
   "metadata": {
    "id": "Zp_0AS8TMJEX"
   },
   "source": [
    "# 8. Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vydmq1hVFvg_",
   "metadata": {
    "id": "vydmq1hVFvg_"
   },
   "source": [
    "Conclui-se que, durante os três treinamentos, o com pior resultado é a rede criada. Isso se dá por causa do número limitado de imagens utilizadas. O treinamento das redes utilizando o ResNet50 como base obteve um melhor resultado, porque a rede já havia sido treinada com mais imagens, o que gera mais precisão na detecção dos elementos da imagem. O Fine-Tuning obteve o melhor resultado, pois, além do treinamento do classificador para obter os resultados esperados, a rede como um todo foi ajustada para a detecção das classes desejadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lu43YmsGAn5",
   "metadata": {
    "id": "4lu43YmsGAn5"
   },
   "source": [
    "# 9. Referências\n",
    "\n",
    "CORNELL LAB OF ORNITHOLOGY. Merlin Bird ID. Ithaca, NY, 2025. Disponível em: https://merlin.allaboutbirds.org/. Acesso em: 14 out. 2025.\n",
    "\n",
    "FU, J.; ZHENG, H.; MEI, T. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In: PROCEEDINGS OF THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, 2017. Anais... [S.l.]: IEEE, 2017. p. 4438-4446.\n",
    "\n",
    "KAGGLE. BirdCLEF 2024 - Birdcall Identification. San Francisco, CA, 2024. Disponível em: https://www.kaggle.com/competitions/birdclef-2024. Acesso em: 14 out. 2025.\n",
    "\n",
    "TANG, Jiayi. Comparative analysis of CNN architectures for bird species classification. ITM Web of Conferences, [S. l.], v. 78, p. 02019, 2025. DOI: 10.1051/itmconf/20257802019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
