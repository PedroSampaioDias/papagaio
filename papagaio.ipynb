{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322f5ac2-41bd-4d1c-b5c0-0dd50439e99e",
   "metadata": {},
   "source": [
    "# Classifica√ß√£o de Esp√©cies de Psitac√≠deos do Cerrado: Uma An√°lise Comparativa com Redes Convolucionais, Transfer Learning e Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uIDTVZhVFfBR",
   "metadata": {
    "id": "uIDTVZhVFfBR"
   },
   "source": [
    "# 1. Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mCjg2b-Iq8S",
   "metadata": {
    "id": "9mCjg2b-Iq8S"
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <table style=\"margin: auto; border-spacing: 60px;\">\n",
    "    <tr>\n",
    "      <td align=\"center\" style=\"padding: 20px;\">\n",
    "        <a href=\"https://github.com/PedroSampaioDias\">\n",
    "          <img style=\"border-radius: 50%;\" src=\"https://avatars.githubusercontent.com/u/90795603?v=4\" width=\"150px;\"/>\n",
    "          <h5 class=\"text-center\">Pedro Sampaio - 211043745</h5>\n",
    "        </a>\n",
    "      </td>\n",
    "      <td align=\"center\" style=\"padding: 20px;\">\n",
    "        <a href=\"https://github.com/raulbreno\">\n",
    "          <img style=\"border-radius: 50%;\" src=\"https://avatars.githubusercontent.com/u/72105072?v=4\" width=\"150px;\"/>\n",
    "          <h5 class=\"text-center\">Raul Breno - 200026810</h5>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9rL3BcQdI1Bj",
   "metadata": {
    "id": "9rL3BcQdI1Bj"
   },
   "source": [
    "# 2. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5tkaM_j4I6YM",
   "metadata": {
    "id": "5tkaM_j4I6YM"
   },
   "source": [
    "# 3. Keywords\n",
    "\n",
    "- **Deep Learning**\n",
    "- **Classifica√ß√£o de Imagens** \n",
    "- **Transfer Learning** \n",
    "- **Fine-Tuning** \n",
    "- **Data Augmentation** \n",
    "- **Psittacidae**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T_MHvmt6FmrP",
   "metadata": {
    "id": "T_MHvmt6FmrP"
   },
   "source": [
    "# 4. Introdu√ß√£o\n",
    "\n",
    "O Cerrado brasileiro, reconhecido por sua vasta biodiversidade, abriga uma not√°vel diversidade de aves, entre as quais se destaca a fam√≠lia Psittacidae. Este grupo, que inclui esp√©cies populares como araras, papagaios e periquitos, √© not√≥rio por suas caracter√≠sticas singulares: s√£o animais de not√°vel intelig√™ncia e c√©rebro desenvolvido, grande longevidade e a capacidade de imitar uma variedade de sons. Apesar de suas plumagens vibrantes, a diferencia√ß√£o visual entre esp√©cies pode ser um desafio significativo, especialmente para observadores n√£o especializados. Esta dificuldade classifica o problema como um desafio de Classifica√ß√£o Visual de Granularidade Fina (Fine-Grained Visual Classification), onde as varia√ß√µes interclasses s√£o sutis.\n",
    "\n",
    "Diante deste cen√°rio, o presente trabalho tem como objetivo central o desenvolvimento e a avalia√ß√£o de um classificador autom√°tico baseado em aprendizado profundo, capaz de identificar a esp√©cie de uma ave a partir de uma imagem. Para tal, ser√° utilizado um conjunto de dados composto por aproximadamente 3.000 imagens de 14 esp√©cies distintas de psitac√≠deos do Cerrado. O dataset, proveniente da plataforma iNaturalist, √© composto por imagens RGB com resolu√ß√µes variadas, retratando as aves em m√∫ltiplas poses e ambientes. Uma an√°lise preliminar do conjunto de dados revela um desbalanceamento no n√∫mero de imagens por classe , uma caracter√≠stica que ser√° abordada atrav√©s de t√©cnicas de aumento de dados (data augmentation) para garantir a robustez e a capacidade de generaliza√ß√£o dos modelos.\n",
    "\n",
    "As 14 esp√©cies consideradas neste estudo s√£o:\n",
    "\n",
    "- **Amazona aestiva (Papagaio-verdadeiro)**\n",
    "- **Amazona amazonica (Curica)**\n",
    "- **Anodorhynchus hyacinthinus (Arara-azul)**\n",
    "- **Ara ararauna (Arara-canind√©)**\n",
    "- **Ara chloropterus (Arara-vermelha)**\n",
    "- **Ara macao (Araracanga)**\n",
    "- **Brotogeris chiriri (Periquito-de-encontro-amarelo)**\n",
    "- **Diopsittaca nobilis (Maracan√£-pequena)**\n",
    "- **Eupsittula aurea (Periquito-rei)**\n",
    "- **Forpus xanthopterygius (Tuim)**\n",
    "- **Orthopsittaca manilatus (Maracan√£-do-buriti)**\n",
    "- **Primolius maracana (Maracan√£)**\n",
    "- **Psittacara leucophthalmus (Periquit√£o)**\n",
    "- **Touit melanonotus (Apuim-de-costas-pretas)**\n",
    "\n",
    "A abordagem metodol√≥gica deste estudo foi estruturada em tr√™s estrat√©gias experimentais distintas, visando uma an√°lise comparativa de desempenho. A primeira abordagem consistir√° no desenvolvimento de uma Rede Neural Convolucional (CNN) a partir do zero. A segunda estrat√©gia empregar√° a t√©cnica de Aprendizado por Transfer√™ncia (Transfer Learning), na qual um modelo pr√©-treinado ser√° utilizado como um extrator de caracter√≠sticas fixo. Por fim, a terceira abordagem utilizar√° a t√©cnica de Fine-Tuning (ajuste fino), que estende o aprendizado por transfer√™ncia ao reajustar sutilmente os pesos do modelo pr√©-treinado. O desempenho de cada uma dessas tr√™s abordagens ser√° rigorosamente avaliado e comparado utilizando um conjunto abrangente de m√©tricas, incluindo Acur√°cia, Precis√£o, Recall, F1-Score e a Matriz de Confus√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DY281S4DJKU_",
   "metadata": {
    "id": "DY281S4DJKU_"
   },
   "source": [
    "# 5. Trabalhos Relacionados\n",
    "\n",
    "A classifica√ß√£o de esp√©cies de aves √© um campo de estudo ativo em vis√£o computacional, frequentemente categorizado como um problema de Classifica√ß√£o Visual de Granularidade Fina (Fine-Grained Visual Classification - FGVC), onde as distin√ß√µes entre classes s√£o sutis e exigem a identifica√ß√£o de caracter√≠sticas locais espec√≠ficas. Diversos trabalhos t√™m explorado a efic√°cia de diferentes arquiteturas de Redes Neurais Convolucionais (CNN) e outras abordagens para este desafio.\n",
    "\n",
    "Em um estudo comparativo abrangente, Tang (2025) realiza uma an√°lise da efic√°cia de diferentes fam√≠lias de arquiteturas de CNNs aplicadas √† tarefa. Empregando um vasto conjunto de dados com mais de 80.000 imagens, abrangendo 525 esp√©cies distintas, a pesquisa avaliou o desempenho de tr√™s proeminentes arquiteturas: ResNet, MobileNet e VGG. Os resultados indicaram que as fam√≠lias ResNet e MobileNet alcan√ßaram desempenho superior. Notavelmente, a performance da ResNet demonstrou uma correla√ß√£o positiva com o aumento do n√∫mero de camadas. Em contrapartida, a fam√≠lia VGG exibiu menor capacidade de discrimina√ß√£o, apresentando dificuldades na distin√ß√£o entre esp√©cies com caracter√≠sticas visuais semelhantes. Por fim, a fam√≠lia MobileNet destacou-se por oferecer um balan√ßo not√°vel entre acur√°cia e efici√™ncia computacional, apresentando-se como uma solu√ß√£o vi√°vel para aplica√ß√µes com restri√ß√µes de hardware.\n",
    "\n",
    "Al√©m da compara√ß√£o direta de arquiteturas padr√£o, a literatura e a ind√∫stria apresentam outras abordagens relevantes:\n",
    "\n",
    "- Classifica√ß√£o com Redes de Aten√ß√£o: Para lidar com a sutileza das caracter√≠sticas em problemas de granularidade fina, pesquisas t√™m explorado o uso de redes neurais com mecanismos de aten√ß√£o. Modelos como o proposto por Fu et al. (2017) s√£o projetados para aprender a focar automaticamente nas regi√µes mais discriminat√≥rias da imagem (ex: o formato do bico, o padr√£o de uma asa), imitando a forma como um especialista humano analisa uma ave para identific√°-la e melhorando a capacidade de distin√ß√£o do modelo.\n",
    "\n",
    "- Aplica√ß√µes Pr√°ticas (Merlin Bird ID): Um dos exemplos mais bem-sucedidos da aplica√ß√£o pr√°tica de CNNs para a identifica√ß√£o de aves √© o aplicativo Merlin Bird ID, desenvolvido pelo Laborat√≥rio de Ornitologia da Cornell. O sistema utiliza modelos de vis√£o computacional treinados com centenas de milhares de imagens para analisar fotos e sugerir a esp√©cie mais prov√°vel em tempo real, demonstrando a viabilidade e o impacto desta tecnologia para a ci√™ncia cidad√£ e a educa√ß√£o ambiental.\n",
    "\n",
    "- Identifica√ß√£o por Vocaliza√ß√£o (BirdCLEF): Al√©m da identifica√ß√£o visual, a comunidade de ci√™ncia de dados tem explorado a classifica√ß√£o de aves a partir de suas vocaliza√ß√µes. Competi√ß√µes anuais, como a \"BirdCLEF\" na plataforma Kaggle, desafiam os participantes a desenvolver modelos que possam identificar esp√©cies de aves em grava√ß√µes de √°udio. As solu√ß√µes frequentemente utilizam CNNs aplicadas a espectrogramas (representa√ß√µes visuais do som), combinando t√©cnicas de vis√£o computacional e processamento de √°udio para o monitoramento da biodiversidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mNe7hXgRH_Nd",
   "metadata": {
    "id": "mNe7hXgRH_Nd"
   },
   "source": [
    "# 6. Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZOMuMZrGaVzn",
   "metadata": {
    "id": "ZOMuMZrGaVzn"
   },
   "source": [
    "A abordagem metodol√≥gica deste estudo foi deliberadamente estruturada em tr√™s estrat√©gias experimentais distintas e progressivamente complexas para a tarefa de classifica√ß√£o de imagens. O objetivo principal √© conduzir uma an√°lise comparativa rigorosa do desempenho, avaliando o compromisso entre o esfor√ßo de desenvolvimento, a necessidade de dados e a acur√°cia final de cada paradigma. As tr√™s estrat√©gias representam um espectro que vai desde a cria√ß√£o de um modelo totalmente espec√≠fico para o dom√≠nio at√© a alavancagem m√°xima de conhecimento pr√©-existente.\n",
    "\n",
    "- Desenvolvimento de uma Rede Neural Convolucional (CNN) a partir do zero (from scratch): A primeira abordagem consiste no projeto e implementa√ß√£o de uma arquitetura de CNN customizada. Esta metodologia oferece um controle granular sobre todos os componentes do modelo, incluindo a profundidade da rede, o n√∫mero e o tamanho dos filtros convolucionais, as fun√ß√µes de ativa√ß√£o e a topologia das camadas de classifica√ß√£o. A principal vantagem reside na possibilidade de criar uma arquitetura otimizada para as caracter√≠sticas intr√≠nsecas e a complexidade espec√≠fica do nosso conjunto de dados. Contudo, este m√©todo imp√µe desafios significativos: exige um volume de dados substancialmente maior para que a rede possa aprender representa√ß√µes de caracter√≠sticas hier√°rquicas significativas a partir de uma inicializa√ß√£o de pesos aleat√≥ria. Al√©m disso, √© computacionalmente intensivo e apresenta um risco elevado de superajuste (overfitting), especialmente com datasets de tamanho limitado. Este modelo servir√° como um baseline fundamental, estabelecendo um ponto de refer√™ncia de desempenho contra o qual as t√©cnicas mais avan√ßadas ser√£o comparadas.\n",
    "\n",
    "- Aprendizado por Transfer√™ncia via Extra√ß√£o de Caracter√≠sticas (Transfer Learning): A segunda estrat√©gia emprega a t√©cnica de aprendizado por transfer√™ncia em sua forma mais direta. Nesta abordagem, um modelo pr√©-treinado em um dataset de larga escala e de dom√≠nio geral, como o ImageNet (que cont√©m milh√µes de imagens em milhares de categorias), √© utilizado como um extrator de caracter√≠sticas fixo. As camadas convolucionais do modelo pr√©-treinado (o backbone) s√£o \"congeladas\", o que significa que seus pesos n√£o s√£o atualizados durante o treinamento. Apenas a camada de classifica√ß√£o final do modelo original √© removida e substitu√≠da por uma nova, com um n√∫mero de sa√≠das correspondente ao n√∫mero de classes do nosso problema. Subsequentemente, apenas os pesos desta nova camada de classifica√ß√£o s√£o treinados. Esta t√©cnica capitaliza sobre o fato de que as camadas iniciais de uma CNN aprendem caracter√≠sticas gen√©ricas (e.g., bordas, texturas, cores), que s√£o transfer√≠veis para uma vasta gama de tarefas de vis√£o computacional, reduzindo drasticamente o tempo de treinamento e a necessidade de dados.\n",
    "\n",
    "- Fine-Tuning (Ajuste Fino): A terceira e √∫ltima abordagem estende o conceito de aprendizado por transfer√™ncia. O fine-tuning inicia-se de forma semelhante, treinando apenas a nova camada de classifica√ß√£o com o backbone congelado. Contudo, em uma segunda fase, o modelo inteiro (ou uma parte dele, tipicamente as camadas mais profundas) √© \"descongelado\". O treinamento ent√£o prossegue em todo o conjunto de par√¢metros, mas com uma taxa de aprendizado (learning rate) muito baixa. O objetivo √© ajustar sutilmente os pesos pr√©-treinados, permitindo que as caracter√≠sticas gen√©ricas aprendidas no ImageNet se tornem mais especializadas e adaptadas √†s nuances espec√≠ficas do nosso conjunto de dados de aves. Esta metodologia busca um equil√≠brio √≥timo, aproveitando o conhecimento pr√©-existente enquanto permite uma adapta√ß√£o mais profunda ao novo dom√≠nio, sendo particularmente eficaz quando o dataset da nova tarefa √© de tamanho razo√°vel e possui similaridades com o dataset original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44349d29",
   "metadata": {
    "id": "44349d29"
   },
   "source": [
    "## 6.1 Configura√ß√£o do Ambiente e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "A fase inicial do desenvolvimento computacional compreende a importa√ß√£o das bibliotecas que constituem o ferramental para a execu√ß√£o do projeto. O ambiente √© configurado com m√≥dulos para aquisi√ß√£o e descompress√£o de dados (gdown, zipfile), manipula√ß√£o de sistema de arquivos e diret√≥rios (os, shutil, pathlib), processamento e an√°lise de dados tabulares (pandas), visualiza√ß√£o de dados (matplotlib, seaborn), e processamento de imagens (PIL, OpenCV). De forma crucial, a biblioteca albumentations √© importada para a implementa√ß√£o de t√©cnicas de aumento de dados (data augmentation), e o numpy √© utilizado como base para opera√ß√µes num√©ricas e manipula√ß√£o de vetores multidimensionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcf396",
   "metadata": {
    "id": "d1dcf396"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "from albumentations import Compose, HorizontalFlip, VerticalFlip, Rotate, CoarseDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12859ea1",
   "metadata": {
    "id": "12859ea1"
   },
   "source": [
    "## 6.2 Aquisi√ß√£o e Prepara√ß√£o do Conjunto de Dados\n",
    "\n",
    "A funda√ß√£o de qualquer modelo de aprendizado de m√°quina reside na qualidade e integridade do conjunto de dados. Nesta etapa, √© executado um processo program√°tico e automatizado para a obten√ß√£o e prepara√ß√£o do dataset original. O script inicialmente verifica a exist√™ncia e integridade do conjunto de dados no ambiente local para evitar redund√¢ncia. Caso o dataset n√£o esteja presente ou esteja incompleto, o processo de aquisi√ß√£o √© iniciado: o arquivo compactado √© baixado de um URI espec√≠fico do Google Drive utilizando a biblioteca gdown.\n",
    "\n",
    "Posteriormente, o arquivo √© descompactado para um diret√≥rio de destino. Uma etapa de sanitiza√ß√£o √© realizada para remover arquivos e diret√≥rios de metadados espec√≠ficos de sistemas operacionais (e.g., __MACOSX, .DS_Store), garantindo uma estrutura de dados limpa, consistente e reprodut√≠vel, pr√©-requisito para as etapas subsequentes de pr√©-processamento e treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50265f17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "50265f17",
    "outputId": "7a54136b-cf72-44ac-f479-aac9a999c69b"
   },
   "outputs": [],
   "source": [
    "file_id = \"1y8tfmhAtVxEHb8hxDlXx6Ek1ALs0SCAm\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "zip_name = \"dataset_original.zip\"\n",
    "\n",
    "base = Path(\"datasets\"); compact = base/\"compactados\"; extract = base/\"dataset_original\"\n",
    "base.mkdir(exist_ok=True); compact.mkdir(parents=True, exist_ok=True); extract.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tiff\",\".tif\",\".webp\"}\n",
    "contar_imgs = lambda p: sum(1 for f in p.rglob(\"*\") if f.is_file() and f.suffix.lower() in exts)\n",
    "\n",
    "def pronto(p):\n",
    "    return p.exists() and sum(1 for d in p.iterdir() if d.is_dir()) == 14 and contar_imgs(p) == 2879\n",
    "\n",
    "if pronto(extract):\n",
    "    print(\"‚úÖ Dataset j√° existe.\")\n",
    "else:\n",
    "    print(\"üì• Baixando...\"); gdown.download(url, zip_name, quiet=False)\n",
    "    dest = compact/zip_name; dest.unlink(missing_ok=True); shutil.move(zip_name, dest)\n",
    "    print(\"üìÇ Extraindo...\"); zipfile.ZipFile(dest).extractall(extract)\n",
    "    [shutil.rmtree(d, ignore_errors=True) for d in extract.rglob(\"__MACOSX\")]\n",
    "    [f.unlink(missing_ok=True) for f in extract.rglob(\".DS_Store\")]\n",
    "    print(f\"üì∏ Total de imagens: {contar_imgs(extract)}\")\n",
    "    print(\"‚úÖ Dataset pronto em:\", base.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v8mk7jom52Zp",
   "metadata": {
    "id": "v8mk7jom52Zp"
   },
   "source": [
    "## 6.3 Explora√ß√£o de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TdKHyVqHbP8r",
   "metadata": {
    "id": "TdKHyVqHbP8r"
   },
   "source": [
    "Neste ponto, a separa√ß√£o, classifica√ß√£o e data augmentation ser√£o feitas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JVfl1FBp3lTa",
   "metadata": {
    "id": "JVfl1FBp3lTa"
   },
   "source": [
    "### 6.3.1 Organiza√ß√£o do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Aj1QAse-4ja3",
   "metadata": {
    "id": "Aj1QAse-4ja3"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/dataset_original\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PvhHKrI34n9F",
   "metadata": {
    "id": "PvhHKrI34n9F"
   },
   "source": [
    "#### 6.3.1.1 Quantidade total de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1KUnenF3kmr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1KUnenF3kmr",
    "outputId": "415b144d-33b6-4c74-86bc-a2ad98a3c54b"
   },
   "outputs": [],
   "source": [
    "total_images = sum(len(files) for _, _, files in os.walk(dataset_path))\n",
    "total_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b-vABkUP5npT",
   "metadata": {
    "id": "b-vABkUP5npT"
   },
   "source": [
    "#### 6.3.1.2 N√∫mero de classes (esp√©cies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nSnlLqAW6Ois",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSnlLqAW6Ois",
    "outputId": "d87ffa89-5151-4820-c90f-7e2da0dc6cfc"
   },
   "outputs": [],
   "source": [
    "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0PRih2bu5nsK",
   "metadata": {
    "id": "0PRih2bu5nsK"
   },
   "source": [
    "### 6.3.2 Distribui√ß√£o das classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ppDf8KvLgzK",
   "metadata": {
    "id": "1ppDf8KvLgzK"
   },
   "source": [
    "#### 6.3.2.1 Frequ√™ncia de imagens por classe com representa√ß√£o em quantidade e porcentagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EeIHV4IcG0RI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "EeIHV4IcG0RI",
    "outputId": "2d2e371a-a7e3-4847-b2d3-911411d9d7ff"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    (c, sum(1 for _,_,fs in os.walk(os.path.join(dataset_path, c))\n",
    "            for f in fs if os.path.splitext(f)[1].lower() in exts))\n",
    "    for c in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, c))\n",
    "], columns=[\"classe\",\"imagens\"]).sort_values(\"imagens\")\n",
    "\n",
    "df[\"pct\"] = (df[\"imagens\"] / df[\"imagens\"].sum() * 100).round(2)\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "ax = sns.barplot(data=df, x=\"imagens\", y=\"classe\",\n",
    "                 hue=\"classe\", dodge=False, palette=\"viridis\",\n",
    "                 orient=\"h\", legend=False)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.text(p.get_width()+5, p.get_y()+p.get_height()/2,\n",
    "            f\"{int(p.get_width())} ({df['pct'].iloc[i]}%)\",\n",
    "            va=\"center\", ha=\"left\", weight=\"bold\")\n",
    "\n",
    "max_val = df[\"imagens\"].max()\n",
    "ax.set_xlim(0, max_val * 1.25)\n",
    "\n",
    "ax.set(title=\"Distribui√ß√£o de imagens por classe\",\n",
    "       xlabel=\"Quantidade de imagens e Porcentagem\", ylabel=\"Classe\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mIa18c8nQ9Dv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "mIa18c8nQ9Dv",
    "outputId": "a30e1e71-aee2-4943-d8b9-d4ca78cddb13"
   },
   "outputs": [],
   "source": [
    "extensoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext:  # garante que tem extens√£o\n",
    "            extensoes.append(ext)\n",
    "\n",
    "# Conta as ocorr√™ncias de cada extens√£o\n",
    "contagem = Counter(extensoes)\n",
    "\n",
    "# Separa chaves e valores\n",
    "extensoes_unicas = list(contagem.keys())\n",
    "quantidades = list(contagem.values())\n",
    "\n",
    "# Cria gr√°fico de barras horizontal\n",
    "plt.figure(figsize=(8,5))\n",
    "bars = plt.barh(extensoes_unicas, quantidades, color=\"skyblue\")\n",
    "plt.xlabel(\"Quantidade de Imagens\")\n",
    "plt.ylabel(\"Extens√£o\")\n",
    "plt.title(\"Distribui√ß√£o das Extens√µes de Arquivos de Imagem\")\n",
    "\n",
    "# Adiciona os valores na frente das barras\n",
    "for bar, qtd in zip(bars, quantidades):\n",
    "    plt.text(qtd + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "             str(qtd), va=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_Igaq-wQ9BM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "i_Igaq-wQ9BM",
    "outputId": "2610eb26-6c8b-49a0-ff3d-778c787ec3ef"
   },
   "outputs": [],
   "source": [
    "resolucoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    resolucoes.append((w, h))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Estat√≠sticas\n",
    "larguras = [w for w, h in resolucoes]\n",
    "alturas = [h for w, h in resolucoes]\n",
    "\n",
    "res_min = (min(larguras), min(alturas))\n",
    "res_max = (max(larguras), max(alturas))\n",
    "res_media = (int(np.mean(larguras)), int(np.mean(alturas)))\n",
    "\n",
    "# Lista de barras com valores e r√≥tulos\n",
    "barras = [\n",
    "    (\"M√≠nimo\", res_min),\n",
    "    (\"M√©dia\", res_media),\n",
    "    (\"M√°ximo\", res_max)\n",
    "]\n",
    "\n",
    "# Ordena por largura*altura (√°rea da imagem)\n",
    "barras.sort(key=lambda x: x[1][0]*x[1][1])\n",
    "\n",
    "# Extrai dados para plot\n",
    "nomes = [nome for nome, res in barras]\n",
    "res_labels = [f\"{res[0]}x{res[1]}\" for nome, res in barras]\n",
    "valores = [res[0]*res[1] for nome, res in barras]  # usado s√≥ para tamanho da barra\n",
    "\n",
    "# Cria gr√°fico horizontal\n",
    "plt.figure(figsize=(8,4))\n",
    "bars = plt.barh(range(len(nomes)), valores, color=[\"skyblue\", \"orange\", \"green\"])\n",
    "plt.yticks(range(len(nomes)), nomes)\n",
    "plt.xlabel(\"Resolu√ß√£o relativa (√°rea)\")\n",
    "plt.title(\"Estat√≠sticas das Resolu√ß√µes das Imagens\")\n",
    "\n",
    "# Adiciona r√≥tulo [LxA] na frente de cada barra\n",
    "for bar, label in zip(bars, res_labels):\n",
    "    plt.text(bar.get_width() + max(valores)*0.01,\n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             label, va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eXyB5mQ8-z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "58eXyB5mQ8-z",
    "outputId": "dc3346a7-266d-4891-ac18-a4ec2b568940"
   },
   "outputs": [],
   "source": [
    "resolucoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    resolucoes.append(f\"{w}x{h}\")  # guarda como string \"LxA\"\n",
    "            except:\n",
    "                pass  # ignora arquivos corrompidos\n",
    "\n",
    "# Conta quantas imagens t√™m cada resolu√ß√£o\n",
    "contagem = Counter(resolucoes)\n",
    "\n",
    "# Pega as 5 resolu√ß√µes mais comuns\n",
    "top5 = contagem.most_common(5)  # retorna lista de tuplas: [(resolucao, quantidade), ...]\n",
    "\n",
    "# Separa em listas para plot\n",
    "resolucoes_top5 = [item[0] for item in top5]\n",
    "quantidades_top5 = [item[1] for item in top5]\n",
    "\n",
    "# Cria gr√°fico horizontal\n",
    "plt.figure(figsize=(8,5))\n",
    "# inverte listas para colocar a maior barra em cima\n",
    "bars = plt.barh(resolucoes_top5[::-1], quantidades_top5[::-1], color=\"skyblue\")\n",
    "plt.xlabel(\"N√∫mero de imagens\")\n",
    "plt.ylabel(\"Resolu√ß√£o [Largura x Altura]\")\n",
    "plt.title(\"Top 5 resolu√ß√µes mais comuns\")\n",
    "\n",
    "# Adiciona n√∫mero de imagens na frente de cada barra\n",
    "for bar, qtd in zip(bars, quantidades_top5[::-1]):\n",
    "    plt.text(qtd + max(quantidades_top5)*0.01,\n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             str(qtd), va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zKAISzUaQ88W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKAISzUaQ88W",
    "outputId": "d7be4523-bf35-4135-8eed-dbc98db78c7b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# N√∫mero total de resolu√ß√µes √∫nicas\n",
    "total_resolucoes = len(contagem)\n",
    "print(f\"N√∫mero total de resolu√ß√µes √∫nicas: {total_resolucoes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saZaeYi7Q857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "saZaeYi7Q857",
    "outputId": "d700e6de-b1ff-4d32-8ef8-323bb051e165"
   },
   "outputs": [],
   "source": [
    "proporcoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    proporcao = Fraction(w, h).limit_denominator(100)  # simplifica a fra√ß√£o\n",
    "                    proporcoes.append(proporcao)\n",
    "            except:\n",
    "                pass  # ignora arquivos corrompidos\n",
    "\n",
    "# Calcula estat√≠sticas\n",
    "prop_min = min(proporcoes)\n",
    "prop_max = max(proporcoes)\n",
    "prop_media_decimal = np.mean([float(p) for p in proporcoes])\n",
    "prop_media = Fraction(prop_media_decimal).limit_denominator(100)\n",
    "\n",
    "# Lista para plot\n",
    "barras = [\n",
    "    (\"M√≠nima\", prop_min),\n",
    "    (\"M√©dia\", prop_media),\n",
    "    (\"M√°xima\", prop_max)\n",
    "]\n",
    "\n",
    "# Ordena por propor√ß√£o decimal\n",
    "barras.sort(key=lambda x: float(x[1]))\n",
    "\n",
    "# Extrai dados para plot\n",
    "nomes = [nome for nome, val in barras]\n",
    "valores = [float(val) for nome, val in barras]\n",
    "labels = [f\"{val.numerator}:{val.denominator}\" for nome, val in barras]  # r√≥tulo L:A\n",
    "\n",
    "# Cria gr√°fico horizontal\n",
    "plt.figure(figsize=(8,4))\n",
    "bars = plt.barh(nomes, valores, color=[\"skyblue\", \"orange\", \"green\"])\n",
    "plt.xlabel(\"Propor√ß√£o (Largura / Altura)\")\n",
    "plt.title(\"Propor√ß√µes das imagens: m√≠nima, m√©dia e m√°xima\")\n",
    "\n",
    "# Adiciona r√≥tulo L:A na frente da barra\n",
    "for bar, label in zip(bars, labels):\n",
    "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "             label, va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGjzny-kQ83G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "RGjzny-kQ83G",
    "outputId": "e1f3038d-57c2-452f-d906-6cabbee3b296"
   },
   "outputs": [],
   "source": [
    "razoes = []\n",
    "\n",
    "# Percorre todas as subpastas e arquivos\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    razao = Fraction(w, h).limit_denominator(100)\n",
    "                    razoes.append(f\"{razao.numerator}:{razao.denominator}\")\n",
    "            except:\n",
    "                pass  # ignora arquivos corrompidos\n",
    "\n",
    "# Conta quantas imagens t√™m cada raz√£o\n",
    "contagem = Counter(razoes)\n",
    "\n",
    "# Pega as 10 raz√µes mais frequentes\n",
    "top10 = contagem.most_common(10)  # lista de tuplas: [(razao, quantidade), ...]\n",
    "\n",
    "# Separa em listas para plot\n",
    "razoes_top10 = [item[0] for item in top10]\n",
    "quantidades_top10 = [item[1] for item in top10]\n",
    "\n",
    "# Cria gr√°fico horizontal\n",
    "plt.figure(figsize=(10, max(4, len(razoes_top10)*0.4)))\n",
    "bars = plt.barh(razoes_top10[::-1], quantidades_top10[::-1], color=\"skyblue\")  # inverte para a mais frequente em cima\n",
    "plt.xlabel(\"N√∫mero de imagens\")\n",
    "plt.ylabel(\"Raz√£o de aspecto [L:A]\")\n",
    "plt.title(\"Top 10 raz√µes de aspecto mais frequentes\")\n",
    "\n",
    "# Adiciona n√∫mero de imagens na frente de cada barra\n",
    "for bar, qtd in zip(bars, quantidades_top10[::-1]):\n",
    "    plt.text(qtd + max(quantidades_top10)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "             str(qtd), va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512629eb",
   "metadata": {},
   "source": [
    "## 6.4 Divis√£o e Aumenta√ß√£o do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176622a1-d2e5-4b53-a7d6-9c921f14eabe",
   "metadata": {},
   "source": [
    "### 6.4.1 Configura√ß√£o e Depend√™ncias\n",
    "\n",
    "O processo inicia-se com a importa√ß√£o das bibliotecas essenciais para cada etapa da tarefa. Para a manipula√ß√£o de arquivos e diret√≥rios, s√£o utilizados os m√≥dulos os e shutil; a gera√ß√£o de n√∫meros aleat√≥rios √© gerenciada por random; e o processamento de imagens fica a cargo do cv2 (OpenCV) e albumentations. Para a etapa de verifica√ß√£o visual, s√£o empregadas as bibliotecas do Matplotlib: matplotlib.pyplot (como plt), utilizada para criar a grade de visualiza√ß√£o, e matplotlib.image (como mpimg), respons√°vel por carregar os arquivos de imagem.\n",
    "\n",
    "Em seguida, s√£o definidos os par√¢metros fundamentais que guiar√£o todo o processo de prepara√ß√£o de dados. Isso inclui a defini√ß√£o dos caminhos dos diret√≥rios: o de origem (dataset_original), o de destino para os dados aumentados (dataset_aumentado), e os diret√≥rios finais para os subconjuntos de treinamento (train_images) e valida√ß√£o (test_images). Adicionalmente, s√£o estabelecidos os par√¢metros num√©ricos: a vari√°vel target_count, que define o n√∫mero de imagens por classe ap√≥s o balanceamento, e a propor√ß√£o de 80/20 para a divis√£o entre os dados de treinamento e valida√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e011624-f5fe-4d1a-8f39-50a6627cbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "from albumentations import Compose, Rotate, HorizontalFlip, VerticalFlip, CoarseDropout\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe9850-e974-42b0-8768-798b76956906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Par√¢metros de Aumento de Dados ---\n",
    "dataset_path = \"./datasets/dataset_original\"\n",
    "dataset_final_path = \"./datasets/dataset_aumentado\"\n",
    "target_count = 300\n",
    "\n",
    "# --- Par√¢metros de Divis√£o do Dataset ---\n",
    "train_dir = \"./datasets/train_images\"\n",
    "val_dir = \"./datasets/val_images\"\n",
    "train_split = 0.8\n",
    "\n",
    "# --- Cria√ß√£o dos Diret√≥rios ---\n",
    "os.makedirs(dataset_final_path, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# --- Exibi√ß√£o das Configura√ß√µes ---\n",
    "print(\"--- Configura√ß√µes do Ambiente ---\")\n",
    "print(f\"Dataset de origem: '{os.path.abspath(dataset_path)}'\")\n",
    "print(f\"Dataset aumentado (destino): '{os.path.abspath(dataset_final_path)}'\")\n",
    "print(f\"Diret√≥rio de treino: '{os.path.abspath(train_dir)}'\")\n",
    "print(f\"Diret√≥rio de valida√ß√£o: '{os.path.abspath(val_dir)}'\")\n",
    "print(f\"N√∫mero alvo de imagens por classe: {target_count}\")\n",
    "print(f\"Propor√ß√£o de divis√£o (Treino/Valida√ß√£o): {train_split*100:.0f}% / {(1-train_split)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c9851-4f05-4107-8e43-a07669847030",
   "metadata": {},
   "source": [
    "### 6.4.2 Defini√ß√£o da Estrat√©gia de Aumento de Dados\n",
    "\n",
    "Para gerar diversidade sint√©tica nos dados, foi implementada a fun√ß√£o get_random_augmentation. Esta fun√ß√£o encapsula a l√≥gica de transforma√ß√£o, selecionando aleatoriamente, a cada chamada, uma de tr√™s poss√≠veis t√©cnicas de aumento de dados da biblioteca Albumentations:\n",
    "\n",
    "- Rota√ß√£o (Rotate): Aplica uma rota√ß√£o na imagem em um √¢ngulo aleat√≥rio entre 10 e 340 graus.\n",
    "\n",
    "- Invers√£o (Flip): Inverte a imagem, com 50% de chance de ser na horizontal e 50% na vertical.\n",
    "\n",
    "- Recorte (CoarseDropout): Remove de 1 a 3 pequenos ret√¢ngulos pretos da imagem em locais aleat√≥rios para simular oclus√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa44cb7-91b1-48f5-ba67-de773973980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_augmentation():\n",
    "    aug_type = random.choice([\"rotate\", \"flip\", \"cutout\"])\n",
    "    \n",
    "    if aug_type == \"rotate\":\n",
    "        return Compose([Rotate(limit=(10, 340), p=1)])\n",
    "    \n",
    "    elif aug_type == \"flip\":\n",
    "        if random.random() > 0.5:\n",
    "            return Compose([HorizontalFlip(p=1)])\n",
    "        else:\n",
    "            return Compose([VerticalFlip(p=1)])\n",
    "            \n",
    "    elif aug_type == \"cutout\":\n",
    "        return Compose([CoarseDropout(\n",
    "            max_holes=3, min_holes=1,\n",
    "            max_height=0.2, min_height=0.1,\n",
    "            max_width=0.2, min_width=0.1,\n",
    "            fill_value=0, p=1\n",
    "        )])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f4e1b5-99cd-4c8e-a43c-a7aeab24d41b",
   "metadata": {},
   "source": [
    "### 6.4.3 Gera√ß√£o do Dataset Aumentado\n",
    "\n",
    "Para corrigir o desbalanceamento entre as classes e aumentar a robustez do modelo, foi executado um processo de aumento de dados (data augmentation). A introdu√ß√£o de varia√ß√µes sint√©ticas, como rota√ß√µes e recortes, exp√µe o modelo a uma gama mais ampla de cen√°rios visuais, melhorando sua capacidade de generaliza√ß√£o para imagens do mundo real.\n",
    "\n",
    "O script a seguir implementa essa estrat√©gia. Ele itera sobre cada subdiret√≥rio de classe do dataset original, primeiramente copiando todas as imagens existentes para a nova estrutura de pastas. Em seguida, um la√ßo while √© acionado para gerar novas imagens sint√©ticas, aplicando transforma√ß√µes aleat√≥rias atrav√©s da fun√ß√£o get_random_augmentation. Este processo continua at√© que o n√∫mero de imagens em cada classe atinja o limiar pr√©-definido (target_count), resultando em um conjunto de dados final que √© n√£o apenas maior, mas tamb√©m perfeitamente balanceado e mais diversificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2db17-00cd-43a5-afaf-12ab251d21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    \n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    final_class_path = os.path.join(dataset_final_path, class_folder)\n",
    "    os.makedirs(final_class_path, exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "    for img_name in images:\n",
    "        shutil.copy(os.path.join(class_path, img_name), os.path.join(final_class_path, img_name))\n",
    "\n",
    "    current_count = len(images)\n",
    "    print(f\"Classe '{class_folder}': {current_count} imagens originais. Gerando novas imagens...\")\n",
    "\n",
    "    while current_count < target_count:\n",
    "        img_name = random.choice(images)\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        aug = get_random_augmentation()\n",
    "        augmented = aug(image=img)['image']\n",
    "\n",
    "        save_name = f\"aug_{current_count}.jpg\"\n",
    "        save_path = os.path.join(final_class_path, save_name)\n",
    "        \n",
    "        augmented_bgr = cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(save_path, augmented_bgr)\n",
    "\n",
    "        current_count += 1\n",
    "\n",
    "    print(f\"-> Classe '{class_folder}' finalizada com {current_count} imagens.\")\n",
    "\n",
    "print(\"\\n‚úÖ Processo de Data Augmentation conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cb765-7ea0-48d9-b2e2-4c59ba7aaf13",
   "metadata": {},
   "source": [
    "### 6.4.4 Verifica√ß√£o do Dataset Aumentado\n",
    "\n",
    "Ap√≥s a gera√ß√£o sint√©tica dos dados, √© fundamental realizar uma verifica√ß√£o para assegurar a integridade do novo dataset. Esta etapa valida tanto a corre√ß√£o do balanceamento de classes quanto a qualidade visual das imagens que ser√£o utilizadas no treinamento.\n",
    "\n",
    "O script a seguir executa duas fun√ß√µes de valida√ß√£o em sequ√™ncia. A primeira √© uma verifica√ß√£o quantitativa, que itera sobre o diret√≥rio dataset_aumentado para contar e exibir o n√∫mero de imagens por classe. Este procedimento confirma que o objetivo de balanceamento foi alcan√ßado. A segunda √© uma inspe√ß√£o visual, que seleciona e exibe uma amostra aleat√≥ria de cada classe em uma grade. Esta an√°lise qualitativa permite validar que as transforma√ß√µes sint√©ticas s√£o realistas, n√£o distorcem as caracter√≠sticas essenciais das esp√©cies e contribuem positivamente para a robustez do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c36d04-47b7-403b-aa24-2e0af855b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Verifica√ß√£o Quantitativa ---\")\n",
    "total_images = 0\n",
    "class_folders = sorted([d for d in os.listdir(dataset_final_path) if os.path.isdir(os.path.join(dataset_final_path, d))])\n",
    "\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(dataset_final_path, class_folder)\n",
    "    count = len(os.listdir(class_path))\n",
    "    print(f\"Classe '{class_folder}': {count} imagens\")\n",
    "    total_images += count\n",
    "\n",
    "print(f\"\\nTotal de imagens no dataset aumentado: {total_images}\")\n",
    "print(\"\\n--- Verifica√ß√£o Visual (Amostra Aleat√≥ria por Classe) ---\")\n",
    "\n",
    "nrows, ncols = 4, 4\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, class_folder in enumerate(class_folders):\n",
    "    class_path = os.path.join(dataset_final_path, class_folder)\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if images:\n",
    "        random_image_name = random.choice(images)\n",
    "        image_path = os.path.join(class_path, random_image_name)\n",
    "        \n",
    "        img = mpimg.imread(image_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(class_folder, fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "for j in range(len(class_folders), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863407a-fae5-4a48-aecc-f033e6d82626",
   "metadata": {},
   "source": [
    "### 6.4.5 Divis√£o de Datasets em Traino e Valida√ß√£o\n",
    "\n",
    "Para avaliar a capacidade de generaliza√ß√£o do modelo e evitar o superajuste (overfitting), o conjunto de dados aumentado foi dividido em dois subconjuntos distintos e mutuamente exclusivos: treinamento e valida√ß√£o. O subconjunto de treinamento √© utilizado para o ajuste dos pesos da rede neural, enquanto o subconjunto de valida√ß√£o, composto por dados n√£o vistos durante o treinamento, serve para fornecer uma estimativa imparcial do desempenho do modelo em dados novos.\n",
    "\n",
    "Foi adotada uma propor√ß√£o de 80% para treinamento e 20% para valida√ß√£o, um padr√£o consolidado na literatura de aprendizado de m√°quina. A divis√£o foi realizada de maneira estratificada, garantindo que essa propor√ß√£o de 80/20 fosse mantida dentro de cada uma das 14 classes de aves. Para assegurar a aleatoriedade da amostragem, a lista de imagens de cada classe foi embaralhada antes da divis√£o. O script a seguir automatiza a cria√ß√£o das estruturas de diret√≥rio e a c√≥pia dos arquivos de imagem para seus respectivos subconjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HYL1h2vsVjnB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYL1h2vsVjnB",
    "outputId": "affc3373-880f-467d-b320-ad3b6f252888"
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    split_idx = int(len(images) * train_split)\n",
    "    train_files = images[:split_idx]\n",
    "    val_files = images[split_idx:]\n",
    "\n",
    "    for img in train_files:\n",
    "        shutil.copy(os.path.join(class_path, img),\n",
    "                    os.path.join(train_dir, class_name, img))\n",
    "    for img in val_files:\n",
    "        shutil.copy(os.path.join(class_path, img),\n",
    "                    os.path.join(val_dir, class_name, img))\n",
    "\n",
    "print(\"‚úÖ Divis√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3K-IyYmWV2O4",
   "metadata": {
    "id": "3K-IyYmWV2O4"
   },
   "source": [
    "## 6.5 Configura√ß√£o e Treinamento da Rede Personalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787PKNILGegW",
   "metadata": {
    "id": "787PKNILGegW"
   },
   "source": [
    "### 6.5.1 Prepara√ß√£o do Ambiente e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Antes da constru√ß√£o e treinamento do modelo, √© fundamental configurar o ambiente computacional e importar todas as bibliotecas necess√°rias. Esta etapa inicial garante que o backend de processamento esteja corretamente definido, os recursos de hardware sejam alocados de forma eficiente e todas as ferramentas para modelagem, treinamento e avalia√ß√£o estejam dispon√≠veis.\n",
    "\n",
    "O script a seguir executa as seguintes a√ß√µes:\n",
    "\n",
    "- Configura√ß√£o do Ambiente: Atrav√©s de vari√°veis de ambiente (os.environ), o Keras √© configurado para utilizar o backend PyTorch. Adicionalmente, s√£o aplicadas configura√ß√µes para o gerenciamento de mem√≥ria da GPU, prevenindo a aloca√ß√£o total e permitindo um crescimento din√¢mico, al√©m de especificar qual dispositivo GPU deve ser utilizado.\n",
    "\n",
    "- Importa√ß√£o do Framework Keras: S√£o importados os componentes centrais da biblioteca Keras, incluindo o modelo Sequential, as diversas layers (como Dense, Dropout e BatchNormalization) e os regularizers, que s√£o essenciais para construir a arquitetura da rede neural convolucional e implementar t√©cnicas de preven√ß√£o de overfitting.\n",
    "\n",
    "- Ferramentas de Prepara√ß√£o de Dados: Inclui a biblioteca numpy para opera√ß√µes num√©ricas e a fun√ß√£o image_dataset_from_directory do TensorFlow/Keras, uma ferramenta de alto n√≠vel para carregar e pr√©-processar imagens diretamente de uma estrutura de diret√≥rios, criando um pipeline de dados eficiente.\n",
    "\n",
    "- M√≥dulos de Avalia√ß√£o e Visualiza√ß√£o: S√£o importadas fun√ß√µes da biblioteca scikit-learn para gerar m√©tricas de avalia√ß√£o, como a matriz de confus√£o (confusion_matrix), e as bibliotecas matplotlib e seaborn para a visualiza√ß√£o gr√°fica dos resultados do treinamento e da performance do modelo.\n",
    "\n",
    "- Callbacks para Controle do Treinamento: S√£o importados os callbacks ModelCheckpoint (para salvar o melhor modelo durante o treinamento), EarlyStopping (para interromper o treinamento caso a performance estagne) e ReduceLROnPlateau (para ajustar dinamicamente a taxa de aprendizado), que permitem um controle mais robusto e automatizado do processo de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gv6GBz3mV0wB",
   "metadata": {
    "id": "gv6GBz3mV0wB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import datasets, layers, models\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2hscSzOGyD_",
   "metadata": {
    "id": "G2hscSzOGyD_"
   },
   "source": [
    "### 6.5.2 Carregamento e Pr√©-processamento dos Dados\n",
    "\n",
    "A etapa de carregamento e pr√©-processamento de dados √© fundamental para a constru√ß√£o de um modelo de aprendizado profundo eficaz. Nesta fase, os dados brutos (imagens em disco) s√£o transformados em um formato estruturado, normalizado e otimizado para o consumo pela rede neural. O processo foi dividido em quatro etapas sequenciais: defini√ß√£o de par√¢metros, constru√ß√£o do pipeline de dados, convers√£o para vetores e verifica√ß√£o final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0953ca-039e-4d8e-93fa-bdaa4fd16dd7",
   "metadata": {},
   "source": [
    "#### 1. Defini√ß√£o de Par√¢metros e Estrutura de Dados\n",
    "A primeira etapa consiste na defini√ß√£o de todos os par√¢metros que governar√£o o pipeline de dados. √â declarada uma lista expl√≠cita de class_names, que assegura uma correspond√™ncia consistente e determin√≠stica entre os nomes das esp√©cies e os r√≥tulos num√©ricos (inteiros) que ser√£o utilizados pelo modelo. S√£o tamb√©m especificados os caminhos para os diret√≥rios de treinamento e valida√ß√£o.\n",
    "\n",
    "Dois hiperpar√¢metros cr√≠ticos s√£o definidos:\n",
    "\n",
    "- IMAGE_SIZE: Estabelecido como (128, 128), este par√¢metro define as dimens√µes para as quais todas as imagens de entrada ser√£o redimensionadas. As redes neurais convolucionais exigem um tensor de entrada com tamanho fixo, e esta uniformiza√ß√£o √© um pr√©-requisito essencial.\n",
    "\n",
    "- BATCH_SIZE: Definido como 32, corresponde ao n√∫mero de imagens que ser√£o processadas em um √∫nico lote (batch) durante o carregamento e o treinamento. A utiliza√ß√£o de lotes √© uma t√©cnica padr√£o que otimiza o uso da mem√≥ria e a estabilidade do processo de otimiza√ß√£o dos gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236da8f-2137-4557-8951-2cc8748fa560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"amazona_aestiva\",\n",
    "    \"amazona_amazonica\",\n",
    "    \"anodorhynchus_hyacinthinus\",\n",
    "    \"ara_ararauna\",\n",
    "    \"ara_chloropterus\",\n",
    "    \"ara_macao\",\n",
    "    \"brotogeris_chiriri\",\n",
    "    \"diopsittaca_nobilis\",\n",
    "    \"eupsittula_aurea\",\n",
    "    \"forpus_xanthopterygius\",\n",
    "    \"orthopsittaca_manilatus\",\n",
    "    \"primolius_maracana\",\n",
    "    \"psittacara_leucophthalmus\",\n",
    "    \"touit_melanonotus\",\n",
    "]\n",
    "\n",
    "BASE_PATH = './datasets'\n",
    "train_dir = os.path.join(BASE_PATH, 'train_images')\n",
    "val_dir = os.path.join(BASE_PATH, 'val_images')\n",
    "\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e8aa6-b077-43d7-8689-408a6193c042",
   "metadata": {},
   "source": [
    "#### 2. Constru√ß√£o do Pipeline de Dados com tf.data\n",
    "\n",
    "Nesta fase, utiliza-se a fun√ß√£o de alto n√≠vel image_dataset_from_directory da biblioteca Keras para criar um pipeline de dados eficiente (tf.data.Dataset). Esta fun√ß√£o automatiza o processo de leitura das imagens a partir dos diret√≥rios, inferindo os r√≥tulos (labels='inferred') a partir da estrutura de subpastas e convertendo-os para um formato inteiro (label_mode='int').\n",
    "\n",
    "Para o conjunto de treinamento (train_ds), a op√ß√£o shuffle=True √© ativada para embaralhar os dados, uma pr√°tica crucial para evitar que o modelo aprenda com a ordem de apresenta√ß√£o das amostras e para melhorar a generaliza√ß√£o. Para o conjunto de valida√ß√£o (val_ds), o embaralhamento √© desativado (shuffle=False) para garantir que a avalia√ß√£o da performance seja consistente e reprodut√≠vel entre as √©pocas de treinamento.\n",
    "\n",
    "Subsequentemente, √© aplicada uma camada de normaliza√ß√£o (Rescaling(1./255)) a ambos os datasets. Este √© um passo de pr√©-processamento cr√≠tico que reescala os valores de pixel, originalmente no intervalo [0, 255], para o intervalo [0, 1]. A normaliza√ß√£o dos dados de entrada acelera a converg√™ncia do treinamento e melhora a estabilidade num√©rica do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba03ed0-4917-469c-9581-583b8b45a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando dados de TREINO...\")\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=class_names,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    interpolation='bilinear',\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nCarregando dados de VALIDA√á√ÉO...\")\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=class_names,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    interpolation='bilinear',\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ee932-dab0-4568-878b-4a6313ee633c",
   "metadata": {},
   "source": [
    "#### 3. Convers√£o para Vetores NumPy\n",
    "Embora o formato tf.data.Dataset seja otimizado para alimentar os modelos durante o treinamento, a convers√£o dos dados para arrays NumPy oferece maior flexibilidade para inspe√ß√£o manual e para o uso de bibliotecas de an√°lise de dados, como a scikit-learn. A fun√ß√£o dataset_to_numpy foi criada para iterar sobre o pipeline de dados, desempacotar os lotes (unbatch) e agregar todas as imagens e r√≥tulos em dois vetores NumPy distintos: um para as imagens e outro para os r√≥tulos correspondentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14305ead-6e4a-4b89-9d67-367743e5ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_batch, label_batch in dataset.unbatch().as_numpy_iterator():\n",
    "        images.append(img_batch)\n",
    "        labels.append(label_batch)\n",
    "\n",
    "    if not images:\n",
    "        print(\"Aten√ß√£o: Nenhum dado encontrado no dataset.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "print(\"\\nConvertendo dataset de treino para arrays NumPy...\")\n",
    "train_images, train_labels = dataset_to_numpy(train_ds)\n",
    "\n",
    "print(\"Convertendo dataset de valida√ß√£o para arrays NumPy...\")\n",
    "val_images, val_labels = dataset_to_numpy(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2024e-1f11-4d6e-916a-c0bf2bed23b7",
   "metadata": {},
   "source": [
    "#### 4. Verifica√ß√£o da Estrutura e Qualidade dos Dados\n",
    "\n",
    "Como etapa final do pr√©-processamento, √© apresentado um resumo tanto quantitativo quanto qualitativo dos dados preparados. Esta verifica√ß√£o combinada √© crucial para validar a integridade estrutural dos vetores e a qualidade visual das amostras antes de iniciar o treinamento.\n",
    "\n",
    "O resumo quantitativo confirma as dimens√µes (shape) e os tipos de dados (dtype) dos vetores NumPy gerados. A verifica√ß√£o da dimensionalidade ‚Äî (n√∫mero_de_amostras, altura, largura, canais_de_cor) ‚Äî assegura que os dados est√£o no formato correto esperado pela camada de entrada da rede neural.\n",
    "\n",
    "Complementarmente, uma verifica√ß√£o qualitativa √© realizada atrav√©s da exibi√ß√£o de uma amostra aleat√≥ria de cada uma das 14 classes a partir do conjunto de treinamento. Esta inspe√ß√£o visual serve como uma verifica√ß√£o de sanidade final, confirmando que as imagens foram carregadas e normalizadas corretamente e, mais importante, que a associa√ß√£o entre as imagens e seus respectivos r√≥tulos est√° correta antes de proceder para a etapa de constru√ß√£o do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc575c5-14d2-4586-828d-ab36f18d37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"RESUMO DOS VETORES GERADOS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"train_images (imagens de treino): {train_images.shape}\")\n",
    "print(f\"train_labels (r√≥tulos de treino): {train_labels.shape}\")\n",
    "print(f\"val_images (imagens de valida√ß√£o): {val_images.shape}\")\n",
    "print(f\"val_labels (r√≥tulos de valida√ß√£o): {val_labels.shape}\")\n",
    "print(f\"Tipo de dado das imagens: {train_images.dtype}\")\n",
    "print(f\"Tipo de dado dos r√≥tulos: {train_labels.dtype}\")\n",
    "\n",
    "if train_labels.size > 0:\n",
    "    exemplo_label_idx = train_labels[0]\n",
    "    nome_classe = class_names[exemplo_label_idx]\n",
    "    print(f\"\\nExemplo: O primeiro r√≥tulo ({exemplo_label_idx}) corresponde √† classe: '{nome_classe}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICA√á√ÉO VISUAL DOS DADOS (Amostra por Classe)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    indices = np.where(train_labels == i)[0]\n",
    "    \n",
    "    if indices.size > 0:\n",
    "        random_index = np.random.choice(indices)\n",
    "        image_to_show = train_images[random_index]\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.imshow(image_to_show)\n",
    "        ax.set_title(class_name)\n",
    "        ax.axis('off')\n",
    "\n",
    "for j in range(len(class_names), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2qSW3YmHPo_",
   "metadata": {
    "id": "p2qSW3YmHPo_"
   },
   "source": [
    "### 6.5.3 Defini√ß√£o de Hiperpar√¢metros e Configura√ß√µes do Modelo\n",
    "\n",
    "Antes da constru√ß√£o da arquitetura, √© crucial definir todos os hiperpar√¢metros e configura√ß√µes que governar√£o o processo de treinamento. Centralizar estes par√¢metros em uma √∫nica c√©lula de configura√ß√£o facilita a experimenta√ß√£o, a reprodutibilidade e a clareza do c√≥digo.\n",
    "\n",
    "Os par√¢metros s√£o agrupados em quatro categorias:\n",
    "\n",
    "- Dados e Modelo: Define as caracter√≠sticas fundamentais dos dados, como o formato de entrada das imagens (INPUT_SHAPE) e o n√∫mero total de classes (NUM_CLASSES). Tamb√©m especifica o nome do arquivo para salvar o modelo de melhor performance (MODEL_FILEPATH).\n",
    "\n",
    "- Arquitetura: Estabelece os valores que moldar√£o a estrutura da rede, como o tamanho dos filtros convolucionais (KERNEL_SIZE), o fator de subamostragem (POOL_SIZE), o n√∫mero de neur√¥nios na camada densa (DENSE_UNITS), e as taxas de Dropout para regulariza√ß√£o. Crucialmente, define-se o fator de regulariza√ß√£o L2 (L2_FACTOR), que penaliza pesos grandes para mitigar o superajuste (overfitting).\n",
    "\n",
    "- Treinamento: Determina os componentes do processo de otimiza√ß√£o, incluindo o otimizador (OPTIMIZER), a fun√ß√£o de perda (LOSS_FUNCTION), o tamanho dos lotes de dados (BATCH_SIZE) e o n√∫mero m√°ximo de √©pocas de treinamento (EPOCHS).\n",
    "\n",
    "- Callbacks: Configura os mecanismos de controle do treinamento, como a m√©trica a ser monitorada (MONITOR_METRIC) e os par√¢metros de paci√™ncia (patience) para as fun√ß√µes de parada antecipada e redu√ß√£o da taxa de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91349911-d5ab-461e-aa5f-116a5471e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Par√¢metros de Dados e Modelo ---\n",
    "INPUT_SHAPE = train_images.shape[1:]\n",
    "NUM_CLASSES = len(class_names)\n",
    "MODEL_FILEPATH = 'modelo_personalizado_v1.keras'\n",
    "\n",
    "# --- Hiperpar√¢metros da Arquitetura ---\n",
    "KERNEL_SIZE = (3, 3)\n",
    "POOL_SIZE = (2, 2)\n",
    "DENSE_UNITS = 512\n",
    "DROPOUT_CONV = 0.35\n",
    "DROPOUT_DENSE = 0.5\n",
    "L2_FACTOR = 0.001\n",
    "\n",
    "# --- Hiperpar√¢metros de Treinamento ---\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "\n",
    "# --- Configura√ß√µes dos Callbacks ---\n",
    "MONITOR_METRIC = 'val_accuracy'\n",
    "ES_PATIENCE = 10\n",
    "RLR_PATIENCE = 3\n",
    "RLR_FACTOR = 0.2\n",
    "RLR_MIN_LR = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cd7a2-16c7-4513-bdb8-7975c908d2fa",
   "metadata": {},
   "source": [
    "### 6.5.4 Prepara√ß√£o dos R√≥tulos para Classifica√ß√£o\n",
    "O modelo ser√° treinado para uma tarefa de classifica√ß√£o multi-classe utilizando a fun√ß√£o de ativa√ß√£o softmax na camada de sa√≠da. Esta fun√ß√£o produz uma distribui√ß√£o de probabilidade sobre as N classes. Para que a fun√ß√£o de perda categorical_crossentropy possa comparar corretamente a predi√ß√£o do modelo com o r√≥tulo verdadeiro, os r√≥tulos, que est√£o em formato de inteiros (e.g., 0, 1, 2...), devem ser convertidos para o formato one-hot encoding.\n",
    "\n",
    "Neste formato, cada r√≥tulo √© transformado em um vetor bin√°rio de tamanho NUM_CLASSES, com todos os elementos sendo 0, exceto pelo √≠ndice correspondente √† classe, que √© 1. A fun√ß√£o to_categorical da biblioteca Keras √© utilizada para realizar essa convers√£o de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c146d-ff46-4074-8f69-ec1071f72be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_one_hot = to_categorical(train_labels, NUM_CLASSES)\n",
    "val_labels_one_hot = to_categorical(val_labels, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157556fa-cdeb-4877-b9d0-f274f9c4469d",
   "metadata": {},
   "source": [
    "### 6.5.5 Constru√ß√£o da Arquitetura da Rede Neural Convolucional\n",
    "\n",
    "A arquitetura do modelo foi constru√≠da utilizando a API Sequential do Keras, seguindo um design inspirado em arquiteturas VGG, caracterizado pelo empilhamento de blocos convolucionais que progressivamente aumentam a profundidade dos filtros enquanto reduzem a dimens√£o espacial dos mapas de caracter√≠sticas.\n",
    "\n",
    "A rede √© composta por:\n",
    "\n",
    "- Quatro Blocos Convolucionais: Cada bloco √© formado por duas camadas Conv2D com ativa√ß√£o ReLU, seguidas por BatchNormalization para estabilizar e acelerar o treinamento. A profundidade dos filtros aumenta a cada bloco (32, 64, 128, 256), permitindo que a rede aprenda caracter√≠sticas de complexidade crescente. Ao final de cada bloco, uma camada MaxPool2D realiza a subamostragem (downsampling), reduzindo a dimensionalidade e criando invari√¢ncia a pequenas transla√ß√µes. Uma camada Dropout √© aplicada para regulariza√ß√£o.\n",
    "\n",
    "- Regulariza√ß√£o L2 (Weight Decay): Para combater o overfitting, a regulariza√ß√£o L2 (kernel_regularizer=regularizers.l2(L2_FACTOR)) √© aplicada a todas as camadas convolucionais e densas. Esta t√©cnica adiciona um termo de penalidade √† fun√ß√£o de perda, proporcional ao quadrado do valor dos pesos, incentivando o modelo a aprender pesos menores e, consequentemente, fun√ß√µes mais simples.\n",
    "\n",
    "- Bloco Classificador: Ap√≥s os blocos convolucionais, uma camada Flatten transforma o mapa de caracter√≠sticas 2D em um vetor 1D. Este vetor √© ent√£o processado por uma camada Dense com DENSE_UNITS neur√¥nios e ativa√ß√£o ReLU. BatchNormalization e Dropout s√£o novamente aplicados antes da camada final.\n",
    "\n",
    "- Camada de Sa√≠da: A √∫ltima camada √© uma Dense com NUM_CLASSES neur√¥nios e ativa√ß√£o softmax, que produz a distribui√ß√£o de probabilidade final para a classifica√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa272b1f-590e-40ca-b798-4b9d623c56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    # Bloco 1 (32 filtros)\n",
    "    layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, activation='relu', padding='same', input_shape=INPUT_SHAPE, kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Bloco 2 (64 filtros)\n",
    "    layers.Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Bloco 3 (128 filtros)\n",
    "    layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Bloco 4 (256 filtros)\n",
    "    layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=POOL_SIZE),\n",
    "    layers.Dropout(DROPOUT_CONV),\n",
    "\n",
    "    # Classificador\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(DENSE_UNITS, activation='relu', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(DROPOUT_DENSE),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d4b24-4824-4b94-a049-dc862931a511",
   "metadata": {},
   "source": [
    "### 6.5.6 Compila√ß√£o do Modelo e Configura√ß√£o dos Callbacks\n",
    "\n",
    "Com a arquitetura definida, o modelo deve ser compilado. A etapa de compila√ß√£o configura o processo de treinamento, especificando o otimizador, a fun√ß√£o de perda e as m√©tricas de avalia√ß√£o. Em seguida, s√£o configurados os callbacks, que s√£o mecanismos para monitorar e controlar o treinamento em tempo de execu√ß√£o.\n",
    "\n",
    "Compila√ß√£o: O modelo √© compilado com o otimizador Adam, a fun√ß√£o de perda categorical_crossentropy (adequada para a tarefa) e a m√©trica de accuracy para monitoramento.\n",
    "\n",
    "Callbacks:\n",
    "\n",
    "- ModelCheckpoint: Salva o modelo em disco (MODEL_FILEPATH) apenas quando a m√©trica monitorada (val_accuracy) melhora, garantindo que a vers√£o de melhor performance seja preservada.\n",
    "\n",
    "- EarlyStopping: Interrompe o treinamento se a val_accuracy n√£o apresentar melhora por um n√∫mero definido de √©pocas (ES_PATIENCE), evitando o desperd√≠cio de recursos computacionais e o overfitting.\n",
    "\n",
    "- ReduceLROnPlateau: Reduz a taxa de aprendizado por um fator (RLR_FACTOR) se a val_accuracy estagnar, permitindo que o modelo refine sua busca por um m√≠nimo local de forma mais precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83ba99-a5af-457d-a835-407f2e7819e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=LOSS_FUNCTION,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nResumo do Modelo:\")\n",
    "model.summary()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath=MODEL_FILEPATH, monitor=MONITOR_METRIC, save_best_only=True, mode='max', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=MONITOR_METRIC, patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=MONITOR_METRIC, factor=RLR_FACTOR, patience=RLR_PATIENCE, min_lr=RLR_MIN_LR, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d0730-fad6-49cd-8fb6-8947aeb13a54",
   "metadata": {},
   "source": [
    "### 6.5.7 Execu√ß√£o do Processo de Treinamento\n",
    "\n",
    "A etapa de treinamento √© iniciada atrav√©s da chamada ao m√©todo model.fit. Este m√©todo alimenta o modelo com os dados de treinamento (train_images, train_labels_one_hot) em lotes de tamanho BATCH_SIZE por um n√∫mero m√°ximo de EPOCHS.\n",
    "\n",
    "O argumento validation_data √© crucial, pois fornece o conjunto de dados de valida√ß√£o (val_images, val_labels_one_hot) que ser√° utilizado ao final de cada √©poca para avaliar o desempenho do modelo em dados n√£o vistos. As m√©tricas calculadas neste conjunto de valida√ß√£o s√£o utilizadas pelos callbacks para tomar decis√µes, como salvar o modelo ou interromper o treinamento. O hist√≥rico completo do treinamento, contendo as perdas e m√©tricas de treino e valida√ß√£o por √©poca, √© armazenado na vari√°vel history para posterior an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab355a-cfb7-42bc-b7c9-afb62568ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIniciando Treinamento...\")\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_images, val_labels_one_hot),\n",
    "    callbacks=[checkpoint_callback, early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0068cea-a214-4391-8b4c-1e5522b00e29",
   "metadata": {},
   "source": [
    "### 6.5.8 Avalia√ß√£o Final do Modelo de Melhor Performance\n",
    "\n",
    "Ap√≥s a conclus√£o do treinamento, a performance final do modelo n√£o √© necessariamente a da √∫ltima √©poca, mas sim a do modelo que atingiu a maior acur√°cia de valida√ß√£o, salvo pelo callback ModelCheckpoint.\n",
    "\n",
    "Portanto, a etapa final consiste em carregar este modelo de melhor performance a partir do arquivo salvo (MODEL_FILEPATH) e reavali√°-lo no conjunto de valida√ß√£o. O m√©todo evaluate retorna a perda (loss) e a acur√°cia (accuracy) finais, que representam a estimativa mais confi√°vel da capacidade de generaliza√ß√£o do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1caca-9e66-4b3d-83c3-458805409790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCarregando o melhor modelo salvo em '{MODEL_FILEPATH}'...\")\n",
    "best_model = models.load_model(MODEL_FILEPATH)\n",
    "\n",
    "print(\"\\nAvaliando o desempenho do MELHOR modelo no conjunto de valida√ß√£o:\")\n",
    "loss, accuracy = best_model.evaluate(val_images, val_labels_one_hot)\n",
    "\n",
    "print(f\"\\n-> Acur√°cia do MELHOR modelo no conjunto de valida√ß√£o: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3993a-a22c-4543-a6bd-d2e6366ae424",
   "metadata": {},
   "source": [
    "### 6.5.9 An√°lise de Desempenho e Valida√ß√£o Aprofundada do Modelo\n",
    "A avalia√ß√£o final de um modelo de classifica√ß√£o vai al√©m da acur√°cia geral, exigindo uma an√°lise aprofundada de seu comportamento durante o treinamento e de sua performance em m√©tricas mais granulares. Esta se√ß√£o detalha a valida√ß√£o do modelo de melhor performance, utilizando ferramentas visuais e relat√≥rios de classifica√ß√£o para obter uma compreens√£o completa de seus pontos fortes e fracos.\n",
    "\n",
    "A an√°lise √© conduzida em duas frentes:\n",
    "\n",
    "1. An√°lise da Din√¢mica de Treinamento: As curvas de acur√°cia e perda (loss) ao longo das √©pocas, tanto para o conjunto de treinamento quanto para o de valida√ß√£o, s√£o plotadas. Estes gr√°ficos s√£o ferramentas diagn√≥sticas essenciais. A converg√™ncia das curvas indica um aprendizado est√°vel, enquanto a diverg√™ncia entre as curvas de treinamento e valida√ß√£o pode sinalizar overfitting (superajuste), onde o modelo memoriza os dados de treino em detrimento de sua capacidade de generaliza√ß√£o.\n",
    "\n",
    "2. Avalia√ß√£o Detalhada da Performance: Para avaliar o desempenho no conjunto de valida√ß√£o, s√£o geradas previs√µes e calculadas as seguintes m√©tricas, compiladas em um relat√≥rio de classifica√ß√£o:\n",
    "\n",
    "    - Precis√£o (Precision): Mede a propor√ß√£o de predi√ß√µes positivas que foram de fato corretas. √â um indicador da exatid√£o do modelo quando ele prev√™ uma classe espec√≠fica.\n",
    "\n",
    "    - Revoca√ß√£o (Recall): Mede a propor√ß√£o de inst√¢ncias positivas reais que foram corretamente identificadas pelo modelo. √â um indicador da capacidade do modelo de encontrar todas as amostras de uma classe espec√≠fica.\n",
    "\n",
    "    - F1-Score: √â a m√©dia harm√¥nica entre precis√£o e revoca√ß√£o, fornecendo uma √∫nica m√©trica que equilibra ambos os aspectos. √â particularmente √∫til quando h√° um desequil√≠brio entre as classes.\n",
    "\n",
    "    -  Matriz de Confus√£o: Esta matriz visualiza o desempenho do modelo de forma detalhada, mostrando o n√∫mero de predi√ß√µes corretas e incorretas para cada classe. A diagonal principal representa as classifica√ß√µes corretas, enquanto os valores fora da diagonal indicam os erros, permitindo identificar quais classes s√£o frequentemente confundidas entre si.\n",
    "\n",
    "O c√≥digo a seguir implementa esta an√°lise completa, gerando os gr√°ficos de hist√≥rico,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78571a-8224-44d7-b143-02d0a3cf09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.plot(history.history['accuracy'], label='Acur√°cia de Treino')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Acur√°cia de Valida√ß√£o')\n",
    "    ax1.set_title('Acur√°cia do Modelo', fontsize=16)\n",
    "    ax1.set_xlabel('√âpoca', fontsize=12)\n",
    "    ax1.set_ylabel('Acur√°cia', fontsize=12)\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(history.history['loss'], label='Loss de Treino')\n",
    "    ax2.plot(history.history['val_loss'], label='Loss de Valida√ß√£o')\n",
    "    ax2.set_title('Loss (Perda) do Modelo', fontsize=16)\n",
    "    ax2.set_xlabel('√âpoca', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nGerando previs√µes para o conjunto de valida√ß√£o...\")\n",
    "y_pred_probs = best_model.predict(val_images)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = val_labels\n",
    "\n",
    "print(\"\\n# ========================================= #\")\n",
    "print(\"#     RELAT√ìRIO DE CLASSIFICA√á√ÉO      #\")\n",
    "print(\"# ========================================= #\\n\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "\n",
    "print(\"\\nGerando a Matriz de Confus√£o...\")\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confus√£o', fontsize=20)\n",
    "plt.ylabel('Classe Verdadeira', fontsize=15)\n",
    "plt.xlabel('Classe Prevista', fontsize=15)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QTg2n8lFGRRW",
   "metadata": {
    "id": "QTg2n8lFGRRW"
   },
   "source": [
    "## 6.6 Treinamento com Aprendizado por Transfer√™ncia (Transfer Learning)\n",
    "\n",
    "Nesta se√ß√£o, adota-se a metodologia de aprendizado por transfer√™ncia para treinar um classificador de aves. A arquitetura selecionada √© a Vision Transformer (ViT), um modelo que aplica o mecanismo de aten√ß√£o, originalmente da √°rea de Processamento de Linguagem Natural, ao dom√≠nio da vis√£o computacional. O processo consiste em utilizar um modelo ViT pr√©-treinado na base de dados ImageNet, congelar os pesos de sua base extratora de caracter√≠sticas (backbone), e treinar exclusivamente uma nova camada de classifica√ß√£o (\"cabe√ßa\") adaptada para as 14 esp√©cies de psitac√≠deos do nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pSLnMcdNTfIh",
   "metadata": {
    "id": "pSLnMcdNTfIh"
   },
   "source": [
    "### 6.6.1 Importa√ß√£o de Bibliotecas e Frameworks\n",
    "\n",
    "A etapa inaugural do processo experimental consiste na importa√ß√£o das bibliotecas e frameworks que constituem a funda√ß√£o do ambiente de desenvolvimento. A escolha recai sobre o fastai, um framework de alto n√≠vel, constru√≠do sobre PyTorch, que encapsula as melhores pr√°ticas da √°rea de aprendizado profundo. Sua utiliza√ß√£o visa abstrair complexidades operacionais, permitindo um foco maior na experimenta√ß√£o e na aplica√ß√£o de t√©cnicas avan√ßadas, como pol√≠ticas de treinamento otimizadas e pipelines de dados eficientes. Implicitamente, o fastai integra a biblioteca timm (PyTorch Image Models), um reposit√≥rio extensivo que oferece acesso a um vasto leque de arquiteturas de vis√£o computacional estado-da-arte, incluindo a fam√≠lia de modelos Vision Transformer. Complementarmente, as bibliotecas padr√£o os e pathlib s√£o empregadas para a manipula√ß√£o de caminhos e diret√≥rios, garantindo a portabilidade e a robustez do c√≥digo em diferentes sistemas operacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nlmFEkSfTbaQ",
   "metadata": {
    "id": "nlmFEkSfTbaQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b47f66-1597-4d7a-a605-846d41e45e45",
   "metadata": {},
   "source": [
    "### 6.6.2 Configura√ß√£o de Par√¢metros e Hiperpar√¢metros\n",
    "\n",
    "Nesta fase, realiza-se a defini√ß√£o centralizada de todos os par√¢metros e hiperpar√¢metros que regem o experimento, uma pr√°tica essencial para garantir a consist√™ncia e a reprodutibilidade dos resultados. S√£o estabelecidos os caminhos para os diret√≥rios contendo os dados de treinamento e valida√ß√£o, bem como o diret√≥rio de destino para o armazenamento dos artefatos do modelo treinado. Os hiperpar√¢metros, que definem a configura√ß√£o do treinamento, s√£o cuidadosamente especificados:\n",
    "\n",
    "- IMG_SIZE: A resolu√ß√£o das imagens de entrada √© fixada em 224x224 pixels. Esta dimens√£o n√£o √© arbitr√°ria; ela corresponde √† resolu√ß√£o com a qual a arquitetura ViT selecionada foi pr√©-treinada na base de dados ImageNet, sendo, portanto, um requisito para a correta utiliza√ß√£o dos pesos transferidos.\n",
    "\n",
    "- BATCH_SIZE: O tamanho do lote de dados √© definido como 64. Este hiperpar√¢metro influencia diretamente a estimativa do gradiente durante a otimiza√ß√£o, a utiliza√ß√£o da mem√≥ria da GPU e a velocidade de treinamento.\n",
    "\n",
    "- LEARNING_RATE e EPOCHS: A taxa de aprendizado m√°xima (2e-3) e o n√∫mero m√°ximo de √©pocas (100) s√£o definidos como os principais controladores do processo de otimiza√ß√£o, ditando a magnitude das atualiza√ß√µes dos pesos e a dura√ß√£o total do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63579e32-e599-4153-92c2-e3eb9c54ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('./datasets')\n",
    "SAVE_PATH = Path('./transfer_learning_models')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dir_name = 'train_images'\n",
    "val_dir_name = 'val_images'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-3\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054c9a9-9edf-4647-8d29-8991882dd9a6",
   "metadata": {},
   "source": [
    "### 6.6.3 Constru√ß√£o do Pipeline de Dados com ImageDataLoaders\n",
    "\n",
    "A constru√ß√£o de um pipeline de dados eficiente e robusto √© realizada por meio da classe ImageDataLoaders do fastai. Este objeto de alto n√≠vel automatiza a cria√ß√£o de um pipeline completo que abrange desde a leitura dos arquivos de imagem at√© a aplica√ß√£o de transforma√ß√µes complexas. O processo √© configurado da seguinte forma: as classes s√£o inferidas a partir da estrutura de subdiret√≥rios; uma transforma√ß√£o ao n√≠vel do item (item_tfms=Resize(IMG_SIZE)) √© aplicada para garantir que todas as imagens sejam redimensionadas para a resolu√ß√£o de entrada uniforme exigida pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292849e-1835-494e-8782-9d6f8bd1eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(\n",
    "    BASE_PATH,\n",
    "    train=train_dir_name,\n",
    "    valid=val_dir_name,\n",
    "    item_tfms=Resize(IMG_SIZE),\n",
    "    bs=BATCH_SIZE\n",
    ")\n",
    "\n",
    "num_classes = len(dls.vocab)\n",
    "print(f\"Classes detectadas ({num_classes}): {dls.vocab}\")\n",
    "dls.show_batch(max_n=9, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff1d77-1a34-495d-bcec-43196b0ea231",
   "metadata": {},
   "source": [
    "### 6.6.4 Instancia√ß√£o do Modelo e Implementa√ß√£o da Estrat√©gia de Congelamento\n",
    "\n",
    "UNesta etapa, o modelo Vision Transformer √© instanciado atrav√©s da fun√ß√£o vision_learner, que baixa a arquitetura vit_small_patch32_224 com seus pesos pr√©-treinados e anexa uma nova \"cabe√ßa\" de classifica√ß√£o, inicializada aleatoriamente, com um n√∫mero de sa√≠das correspondente ao n√∫mero de classes do nosso problema.\n",
    "\n",
    "Subsequentemente, a estrat√©gia central do transfer learning √© implementada atrav√©s do congelamento expl√≠cito dos pesos do backbone. O modelo no fastai √© estruturado em duas partes: o corpo extrator de caracter√≠sticas (learn.model[0]) e a cabe√ßa de classifica√ß√£o (learn.model[1]). O c√≥digo itera sobre todos os par√¢metros do backbone e define seu atributo requires_grad como False. Esta opera√ß√£o instrui o framework de otimiza√ß√£o (PyTorch) a n√£o calcular gradientes para estes par√¢metros durante a fase de retropropaga√ß√£o, efetivamente os \"congelando\". Apenas os par√¢metros da nova cabe√ßa de classifica√ß√£o permanecem trein√°veis (requires_grad = True), focando o aprendizado exclusivamente na tarefa de mapear as caracter√≠sticas de alto n√≠vel, extra√≠das pelo backbone, para as classes de aves espec√≠ficas do nosso dataset. Por fim, o otimizador √© recriado para garantir que ele esteja ciente do novo conjunto de par√¢metros trein√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71143399-b6ec-4278-a1a4-a9c2978ff648",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(\n",
    "    dls,\n",
    "    'vit_small_patch32_224',\n",
    "    metrics=[accuracy, error_rate]\n",
    ")\n",
    "\n",
    "print(\"\\nüßä Aplicando congelamento expl√≠cito ‚Äî todas as camadas do backbone ser√£o congeladas...\")\n",
    "\n",
    "for p in learn.model[0].parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in learn.model[1].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "learn.create_opt()\n",
    "\n",
    "print(\"\\nüîç Verificando camadas congeladas e trein√°veis:\")\n",
    "trainable, frozen = 0, 0\n",
    "for name, param in learn.model.named_parameters():\n",
    "    status = \"üîì trein√°vel\" if param.requires_grad else \"üßä congelado\"\n",
    "    if param.requires_grad: trainable += 1\n",
    "    else: frozen += 1\n",
    "print(f\"Resumo: üßä {frozen} camadas congeladas | üîì {trainable} camadas trein√°veis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760d1e7-fa90-407a-bdbe-8b23c15a1c33",
   "metadata": {},
   "source": [
    "### 6.6.5 Treinamento da Camada de Classifica√ß√£o\n",
    "\n",
    "Com o corpo do modelo congelado, o processo de treinamento √© iniciado, focando exclusivamente na otimiza√ß√£o dos pesos da nova cabe√ßa de classifica√ß√£o. S√£o configurados os callbacks EarlyStoppingCallback e SaveModelCallback como mecanismos de controle emp√≠rico. O primeiro previne o overfitting ao interromper o treinamento caso a performance de valida√ß√£o estagne, enquanto o segundo garante a persist√™ncia do modelo que alcan√ßou o melhor desempenho. O treinamento √© conduzido pelo m√©todo fit_one_cycle, que implementa a pol√≠tica de treinamento \"1-cycle\". Esta t√©cnica avan√ßada modula a taxa de aprendizado de forma c√≠clica ao longo das √©pocas, come√ßando baixa, aumentando at√© um m√°ximo e depois decaindo. Essa abordagem tem se mostrado eficaz para acelerar a converg√™ncia e navegar de forma mais eficiente pelo espa√ßo de perda, frequentemente resultando em modelos com melhor capacidade de generaliza√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ee308-4b16-4276-b392-7223499fe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStoppingCallback(\n",
    "        monitor='accuracy',\n",
    "        patience=10,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    "    SaveModelCallback(\n",
    "        monitor='accuracy',\n",
    "        fname='best_model_vit'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Iniciando treinamento (apenas a cabe√ßa ser√° ajustada)...\")\n",
    "learn.fit_one_cycle(\n",
    "    EPOCHS,\n",
    "    lr_max=LEARNING_RATE,\n",
    "    cbs=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6287ca7-aec5-4861-aaa1-310e58c88462",
   "metadata": {},
   "source": [
    "### 6.6.6 An√°lise Diagn√≥stica, Valida√ß√£o Final e Serializa√ß√£o do Modelo\n",
    "\n",
    "A avalia√ß√£o da performance de um modelo de classifica√ß√£o transcende a simples mensura√ß√£o da acur√°cia. Para obter uma compreens√£o completa do comportamento do modelo, √© imperativo realizar uma an√°lise mais granular de seu desempenho. Nesta se√ß√£o, o modelo de melhor performance, restaurado pelo callback, √© submetido a uma valida√ß√£o aprofundada.\n",
    "\n",
    "Para tal, √© gerado um relat√≥rio de classifica√ß√£o textual, que sumariza m√©tricas essenciais como Precis√£o, Revoca√ß√£o e F1-Score para cada uma das 14 classes. Em conjunto, estas m√©tricas oferecem uma vis√£o granular do equil√≠brio do modelo entre os diferentes tipos de erro de classifica√ß√£o (falsos positivos e falsos negativos). Adicionalmente, uma Matriz de Confus√£o √© gerada para visualizar de forma direta os padr√µes de erro, permitindo identificar quais classes s√£o sistematicamente confundidas entre si. A an√°lise √© complementada pela visualiza√ß√£o das amostras com maior erro de predi√ß√£o (top losses), uma ferramenta diagn√≥stica para inspecionar os casos mais desafiadores para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67d5e0-dc47-4a47-a438-6ea3b1d1421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Avaliando os resultados do modelo treinado...\")\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Extrai os r√≥tulos verdadeiros e as predi√ß√µes\n",
    "y_true = interp.targs.numpy()\n",
    "y_pred = interp.preds.argmax(dim=1).numpy()\n",
    "class_names = dls.vocab\n",
    "\n",
    "print(\"\\n# =================================================== #\")\n",
    "print(\"#     RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO      #\")\n",
    "print(\"# =================================================== #\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "print(\"\\nGerando a Matriz de Confus√£o...\")\n",
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
    "\n",
    "print(\"\\nExibindo amostras com maior erro de predi√ß√£o (Top Losses)...\")\n",
    "interp.plot_top_losses(9, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e180c-c389-4eba-b63c-01d60c75e97a",
   "metadata": {},
   "source": [
    "### 6.6.7 Valida√ß√£o Final e Exporta√ß√£o do Artefato do Modelo\n",
    "\n",
    "Como etapa conclusiva do processo experimental, √© realizada a valida√ß√£o final para registrar a acur√°cia global do modelo de melhor performance e, em seguida, o artefato do modelo √© serializado e exportado. O m√©todo learn.validate() √© invocado para recalcular e exibir as m√©tricas de perda (loss) e acur√°cia no conjunto de dados de valida√ß√£o, fornecendo um registro quantitativo final e definitivo do desempenho do modelo.\n",
    "\n",
    "Posteriormente, o objeto Learner treinado √© exportado para um arquivo .pkl atrav√©s do m√©todo learn.export(). Este processo de serializa√ß√£o √© cr√≠tico, pois encapsula n√£o apenas a arquitetura da rede neural e seus pesos otimizados, mas todo o pipeline de transforma√ß√µes de dados (e.g., redimensionamento, normaliza√ß√£o, aumento de dados) necess√°rio para o pr√©-processamento de novas imagens. O resultado √© um artefato de modelo autocontido, que pode ser facilmente carregado em ambientes de produ√ß√£o para realizar infer√™ncias em novos dados, garantindo que o pr√©-processamento aplicado na infer√™ncia seja id√™ntico ao utilizado durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793fdd8-b40b-4bfb-9e5f-13feb487eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculando as m√©tricas finais do melhor modelo no conjunto de valida√ß√£o...\")\n",
    "final_metrics = learn.validate()\n",
    "\n",
    "final_accuracy = final_metrics[1]\n",
    "print(f\"\\n{'='*35}\")\n",
    "print(f\"  Acur√°cia Final de Valida√ß√£o: {final_accuracy*100:.4f}%\")\n",
    "print(f\"{'='*35}\\n\")\n",
    "\n",
    "final_model_path = SAVE_PATH / \"modelo_vit_transfer_learning.pkl\"\n",
    "learn.export(final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Treinamento e valida√ß√£o conclu√≠dos.\")\n",
    "print(f\"üíæ O melhor modelo (pesos) foi salvo em: {learn.path/learn.model_dir}/best_model_vit.pth\")\n",
    "print(f\"üíæ O modelo final (artefato completo) foi exportado para: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D_K9T2ewSzjQ",
   "metadata": {
    "id": "D_K9T2ewSzjQ"
   },
   "source": [
    "## 6.7 Treinamento com a Estrat√©gia de Fine-Tuning\n",
    "\n",
    "A t√©cnica de fine-tuning (ajuste fino) √© uma metodologia de aprendizado por transfer√™ncia mais avan√ßada, que visa adaptar n√£o apenas a camada de classifica√ß√£o, mas tamb√©m as representa√ß√µes de caracter√≠sticas aprendidas pelo corpo do modelo (backbone). Diferentemente da abordagem de congelamento total, o fine-tuning opera em duas fases: primeiro, treina-se apenas a nova \"cabe√ßa\" de classifica√ß√£o com o backbone congelado; em seguida, o modelo inteiro √© \"descongelado\" e treinado de ponta a ponta, tipicamente com taxas de aprendizado diferenciais (discriminative learning rates). Esta abordagem permite que as caracter√≠sticas gen√©ricas aprendidas no dataset original (e.g., ImageNet) sejam sutilmente ajustadas para se tornarem mais espec√≠ficas ao dom√≠nio do novo problema. Para este experimento, a arquitetura selecionada √© a ResNet50, uma rede neural convolucional de refer√™ncia, conhecida por sua efic√°cia e pelo uso de conex√µes residuais para mitigar o problema do gradiente evanescente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lg0HRQSnS76o",
   "metadata": {
    "id": "lg0HRQSnS76o"
   },
   "source": [
    "### 6.7.1 Importa√ß√£o de Bibliotecas e Frameworks\n",
    "\n",
    "A etapa inaugural consiste na importa√ß√£o das bibliotecas que formam a base do experimento. O framework fastai √© novamente empregado como uma interface de alto n√≠vel sobre PyTorch, pois suas abstra√ß√µes s√£o particularmente poderosas para implementar estrat√©gias de treinamento complexas como o fine-tuning de forma concisa e eficaz. As bibliotecas os e pathlib s√£o utilizadas para a manipula√ß√£o de caminhos de arquivos de forma independente do sistema operacional, garantindo a robustez do c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVRkAm5KTwLo",
   "metadata": {
    "id": "IVRkAm5KTwLo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6762ce-c3c3-4181-abe3-e28a43a71507",
   "metadata": {},
   "source": [
    "### 6.7.2 Configura√ß√£o de Par√¢metros e Hiperpar√¢metros do Experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c6894-ab25-41a6-bd56-20f46cd3b36e",
   "metadata": {},
   "source": [
    "Nesta fase, realiza-se a defini√ß√£o centralizada de todos os par√¢metros e hiperpar√¢metros que regem o experimento. S√£o estabelecidos os caminhos para os diret√≥rios de dados. Os hiperpar√¢metros cr√≠ticos para a estrat√©gia de fine-tuning s√£o especificados:\n",
    "\n",
    "- IMG_SIZE e BATCH_SIZE: A resolu√ß√£o de entrada (224x224) √© mantida para ser compat√≠vel com as dimens√µes de pr√©-treinamento da ResNet50, e o tamanho do lote √© definido para otimizar o uso de recursos computacionais.\n",
    "\n",
    "- HEAD_LEARNING_RATE: Esta √© a taxa de aprendizado que ser√° utilizada para treinar a cabe√ßa de classifica√ß√£o durante a primeira fase do fine-tuning e como taxa de aprendizado base para as camadas finais na segunda fase.\n",
    "\n",
    "- EPOCHS: Define o n√∫mero m√°ximo de √©pocas para a segunda fase do fine-tuning, na qual o modelo inteiro √© treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a8e43-5071-4920-b0da-f083f13e5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('./datasets')\n",
    "SAVE_PATH = Path('./finetuning_models')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dir_name = 'train_images'\n",
    "val_dir_name = 'val_images'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "HEAD_LEARNING_RATE = 2e-3\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d9ebb-37ff-4f7c-a032-7ca7e4bd9851",
   "metadata": {},
   "source": [
    "### 6.7.3 Constru√ß√£o do Pipeline de Dados\n",
    "\n",
    "O carregamento e pr√©-processamento dos dados s√£o gerenciados pela classe ImageDataLoaders do fastai. Este objeto constr√≥i um pipeline de dados que automatiza a infer√™ncia de classes a partir da estrutura de diret√≥rios e aplica uma transforma√ß√£o de redimensionamento (item_tfms=Resize(IMG_SIZE)) para uniformizar as dimens√µes das imagens. √â importante notar que, nesta configura√ß√£o, n√£o foram explicitamente adicionadas transforma√ß√µes de aumento de dados, focando na capacidade do fine-tuning de adaptar os pesos do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8f67b-7c03-47ee-ad46-26302d700a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(\n",
    "    BASE_PATH,\n",
    "    train=train_dir_name,\n",
    "    valid=val_dir_name,\n",
    "    item_tfms=Resize(IMG_SIZE),\n",
    "    bs=BATCH_SIZE\n",
    ")\n",
    "\n",
    "num_classes = len(dls.vocab)\n",
    "print(f\"Classes detectadas ({num_classes}): {dls.vocab}\")\n",
    "dls.show_batch(max_n=9, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ed8ba-430a-4a89-a164-efe4d775677a",
   "metadata": {},
   "source": [
    "### 6.7.4 Instancia√ß√£o do Modelo e Configura√ß√£o dos Callbacks\n",
    "\n",
    "Um objeto Learner √© instanciado atrav√©s da fun√ß√£o vision_learner, que baixa a arquitetura resnet50 com pesos pr√©-treinados no ImageNet, anexa uma nova cabe√ßa de classifica√ß√£o adaptada ao n√∫mero de classes do nosso problema e configura as m√©tricas de accuracy e error_rate para monitoramento. A taxa de aprendizado padr√£o do otimizador √© definida como HEAD_LEARNING_RATE.\n",
    "\n",
    "Os callbacks EarlyStoppingCallback e SaveModelCallback s√£o configurados para controlar o treinamento de forma robusta. O primeiro interrompe o processo caso a performance de valida√ß√£o estagne, prevenindo o overfitting, enquanto o segundo garante a persist√™ncia do modelo que alcan√ßou o melhor desempenho ao longo de todas as √©pocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30238a-c40c-496d-8bfa-aa50abaa1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(\n",
    "    dls,\n",
    "    resnet50,\n",
    "    metrics=[accuracy, error_rate],\n",
    "    lr=HEAD_LEARNING_RATE\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStoppingCallback(\n",
    "        monitor='accuracy',\n",
    "        patience=10,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    "    SaveModelCallback(\n",
    "        monitor='accuracy',\n",
    "        fname='best_model_resnet50_finetuned'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e2e61-b8b8-4018-86f4-ea20c6ee4bab",
   "metadata": {},
   "source": [
    "### 6.7.5 Execu√ß√£o da Estrat√©gia de Fine-Tuning\n",
    "A execu√ß√£o do fine-tuning √© encapsulada pelo m√©todo learn.fine_tune(), que automatiza a estrat√©gia de duas fases:\n",
    "\n",
    "- Fase 1 (Congelada): O corpo do modelo (backbone) √© mantido congelado, e apenas a nova cabe√ßa de classifica√ß√£o √© treinada por um n√∫mero fixo de √©pocas (por padr√£o, uma √©poca). Esta etapa permite que a camada de sa√≠da se ajuste rapidamente √†s novas classes sem perturbar os pesos pr√©-treinados.\n",
    "\n",
    "- Fase 2 (Descongelada): O modelo inteiro √© descongelado, e todas as camadas s√£o treinadas por EPOCHS √©pocas. Crucialmente, o fine_tune aplica taxas de aprendizado discriminat√≥rias. A taxa de aprendizado fornecida (base_lr=BASE_LR_BODY) √© aplicada √†s camadas mais iniciais do backbone, e o fastai interpola gradualmente as taxas de aprendizado para as camadas subsequentes, at√© atingir a taxa de aprendizado original (HEAD_LEARNING_RATE) na cabe√ßa de classifica√ß√£o. Esta t√©cnica √© fundamental, pois permite que as caracter√≠sticas mais gen√©ricas (nas primeiras camadas) sejam ajustadas de forma sutil (com uma taxa de aprendizado baixa), enquanto as caracter√≠sticas mais espec√≠ficas (nas camadas finais) s√£o ajustadas de forma mais agressiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3bf85-c4d7-42d0-9995-a06c7e1ebcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando treinamento com ResNet50 e a estrat√©gia de Fine-Tuning...\")\n",
    "\n",
    "BASE_LR_BODY = 1e-4\n",
    "\n",
    "learn.fine_tune(\n",
    "    EPOCHS,\n",
    "    base_lr=BASE_LR_BODY,\n",
    "    cbs=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac09d3-fa01-4325-a3a3-877e9fb12c8d",
   "metadata": {},
   "source": [
    "### 6.7.6 An√°lise de Desempenho e Valida√ß√£o de M√©tricas\n",
    "\n",
    "Para uma avalia√ß√£o de desempenho robusta do modelo final, uma an√°lise aprofundada √© conduzida. Esta etapa vai al√©m da acur√°cia global para fornecer uma compreens√£o granular do comportamento do modelo. Para tal, √© gerado um relat√≥rio de classifica√ß√£o textual, que sumariza um conjunto de m√©tricas essenciais, incluindo Acur√°cia, Precis√£o, Recall e F1-Score para cada uma das 14 classes. Em complemento, uma Matriz de Confus√£o √© visualizada para permitir a inspe√ß√£o direta dos padr√µes de erro de classifica√ß√£o entre as esp√©cies. A an√°lise √© finalizada com a exibi√ß√£o das amostras que geraram o maior erro de predi√ß√£o (top losses), uma ferramenta diagn√≥stica para inspecionar os casos mais desafiadores para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cc0a7-8f2d-4547-ba48-84ca8128956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Avaliando os resultados do modelo treinado...\")\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Extrai os r√≥tulos verdadeiros e as predi√ß√µes\n",
    "y_true = interp.targs.numpy()\n",
    "y_pred = interp.preds.argmax(dim=1).numpy()\n",
    "class_names_report = dls.vocab\n",
    "\n",
    "print(\"\\n# =================================================== #\")\n",
    "print(\"#     RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO      #\")\n",
    "print(\"# =================================================== #\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_report))\n",
    "\n",
    "print(\"\\nGerando a Matriz de Confus√£o...\")\n",
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
    "\n",
    "print(\"\\nExibindo amostras com maior erro de predi√ß√£o (Top Losses)...\")\n",
    "interp.plot_top_losses(9, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728eacd-85c3-4b2e-97ee-f01775cf98c4",
   "metadata": {},
   "source": [
    "### 6.7.7 An√°lise de Desempenho e Valida√ß√£o de M√©tricas\n",
    "\n",
    "Como etapa conclusiva do processo experimental, √© realizada a valida√ß√£o final para registrar a acur√°cia global do modelo de melhor performance. O m√©todo learn.validate() √© invocado para recalcular e exibir as m√©tricas de perda (loss) e acur√°cia no conjunto de dados de valida√ß√£o, fornecendo um registro quantitativo final e definitivo do desempenho do modelo.\n",
    "\n",
    "Posteriormente, o objeto Learner treinado √© exportado para um arquivo .pkl. Este processo de serializa√ß√£o encapsula a arquitetura, os pesos otimizados e todo o pipeline de transforma√ß√µes de dados, criando um artefato de modelo autocontido e pronto para ser implantado em ambientes de infer√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce95002-8291-42cd-8a0f-eaae9c4a9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculando as m√©tricas finais do melhor modelo no conjunto de valida√ß√£o...\")\n",
    "final_metrics = learn.validate()\n",
    "\n",
    "final_accuracy = final_metrics[1]\n",
    "print(f\"\\n{'='*35}\")\n",
    "print(f\"  Acur√°cia Final de Valida√ß√£o: {final_accuracy*100:.4f}%\")\n",
    "print(f\"{'='*35}\\n\")\n",
    "\n",
    "final_model_path = SAVE_PATH / \"modelo_resnet50_finetuned.pkl\"\n",
    "learn.export(final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Treinamento e valida√ß√£o conclu√≠dos.\")\n",
    "print(f\"üíæ O melhor modelo (pesos) foi salvo em: {learn.path/learn.model_dir}/best_model_resnet50_finetuned.pth\")\n",
    "print(f\"üíæ O modelo final (artefato completo) foi exportado para: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CG7VJ0wXML_v",
   "metadata": {
    "id": "CG7VJ0wXML_v"
   },
   "source": [
    "# 7. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZZkqZgFCT0LC",
   "metadata": {
    "id": "ZZkqZgFCT0LC"
   },
   "source": [
    "## 7.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12pD5TTqTz3t",
   "metadata": {
    "id": "12pD5TTqTz3t"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVB44ouWTkK3",
   "metadata": {
    "id": "rVB44ouWTkK3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Para evitar um erro comum no Windows\n",
    "# temp = pathlib.PosixPath\n",
    "# pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# ===================================================================\n",
    "# 1. CONFIGURE OS CAMINHOS AQUI\n",
    "# ===================================================================\n",
    "\n",
    "# Altere esta linha para o caminho exato onde seu modelo .pkl foi salvo.\n",
    "# Exemplo: \"/home/lab11/papagaio/modelo_resnet50_finetuned_fastai.pkl\"\n",
    "caminho_do_modelo = \"/content/modelos/modelo_resnet50_finetuned_fastai.pkl\"  # <--- MUDE AQUI\n",
    "\n",
    "# Altere esta linha para o caminho da imagem que voc√™ quer classificar.\n",
    "# Exemplo: \"imagens/teste/pardal_01.jpg\"\n",
    "caminho_da_imagem = \"/home/lab11/papagaio/papagaio-main/image copy 3.png\"  # <--- MUDE AQUI\n",
    "\n",
    "# ===================================================================\n",
    "# 2. FUN√á√ÉO PARA CARREGAR O MODELO E PREVER A IMAGEM\n",
    "# ===================================================================\n",
    "\n",
    "def prever_imagem(caminho_modelo, caminho_imagem):\n",
    "    \"\"\"\n",
    "    Carrega um modelo treinado da fastai e faz a previs√£o em uma √∫nica imagem.\n",
    "    \"\"\"\n",
    "    # ---- Carregamento do Modelo ----\n",
    "    try:\n",
    "        print(f\"Carregando modelo de: {caminho_modelo}\")\n",
    "        # load_learner carrega tudo o que √© necess√°rio para a infer√™ncia\n",
    "        learn = load_learner(caminho_modelo)\n",
    "        print(\"Modelo carregado com sucesso!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: Arquivo do modelo n√£o encontrado em '{caminho_modelo}'. Verifique o caminho.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao carregar o modelo: {e}\")\n",
    "        return\n",
    "\n",
    "    # ---- Previs√£o na Imagem ----\n",
    "    try:\n",
    "        print(f\"\\nFazendo previs√£o na imagem: {caminho_imagem}\")\n",
    "\n",
    "        # O m√©todo .predict faz todo o processamento necess√°rio na imagem\n",
    "        classe_predita, _, probabilidades = learn.predict(caminho_imagem)\n",
    "\n",
    "        # Exibe a imagem com o resultado\n",
    "        img = Image.open(caminho_imagem)\n",
    "        plt.imshow(img)\n",
    "        # Pega a probabilidade da classe prevista\n",
    "        prob_max = probabilidades.max()\n",
    "        plt.title(f\"Previs√£o: {classe_predita}\\nConfian√ßa: {prob_max*100:.2f}%\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # ---- Mostra o resultado detalhado no terminal ----\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"      RESULTADO DA CLASSIFICA√á√ÉO\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"A imagem foi classificada como: '{classe_predita}'\")\n",
    "\n",
    "        # Mostra o top 5 de probabilidades para ter mais detalhes\n",
    "        print(\"\\nTop 5 probabilidades:\")\n",
    "        # Combina os nomes das classes (vocab) com suas probabilidades\n",
    "        prob_por_classe = sorted(zip(learn.dls.vocab, probabilidades), key=lambda x: x[1], reverse=True)\n",
    "        for i, (classe, prob) in enumerate(prob_por_classe[:5]):\n",
    "            print(f\"{i+1}. {classe}: {prob*100:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: Arquivo de imagem n√£o encontrado em '{caminho_imagem}'. Verifique o caminho.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a previs√£o: {e}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3. EXECU√á√ÉO PRINCIPAL\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    prever_imagem(caminho_do_modelo, caminho_da_imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0LpNqrQZTWCg",
   "metadata": {
    "id": "0LpNqrQZTWCg"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 1) CAMINHOS\n",
    "# ===================================================================\n",
    "caminho_do_modelo = \"/content/modelos/modelo_resnet50_finetuned_fastai.pkl\"\n",
    "caminho_teste     = \"/content/datasets/test_images\"\n",
    "\n",
    "# (Opcional) classes que voc√™ informou; usaremos APENAS para validar nomes/pastas\n",
    "class_names = [\n",
    "    \"amazona_aestiva\",\n",
    "    \"amazona_amazonica\",\n",
    "    \"anodorhynchus_hyacinthinus\",\n",
    "    \"ara_ararauna\",\n",
    "    \"ara_chloropterus\",\n",
    "    \"ara_macao\",\n",
    "    \"brotogeris_chiriri\",\n",
    "    \"diopsittaca_nobilis\",\n",
    "    \"eupsittula_aurea\",\n",
    "    \"forpus_xanthopterygius\",\n",
    "    \"orthopsittaca_manilatus\",\n",
    "    \"primolius_maracana\",\n",
    "    \"psittacara_leucophthalmus\",\n",
    "    \"touit_melanonotus\",\n",
    "]\n",
    "\n",
    "# ===================================================================\n",
    "# 2) FUN√á√ÉO\n",
    "# ===================================================================\n",
    "def avaliar_modelo_no_teste(caminho_modelo, caminho_teste, class_names=None):\n",
    "    # 1) Carrega o modelo\n",
    "    print(f\"Carregando modelo de: {caminho_modelo}\")\n",
    "    learn = load_learner(caminho_modelo)\n",
    "    print(\"‚úÖ Modelo carregado com sucesso!\\n\")\n",
    "\n",
    "    if not hasattr(learn.dls, \"vocab\") or learn.dls.vocab is None:\n",
    "        raise RuntimeError(\"Este Learner n√£o tem 'vocab' ‚Äî o modelo provavelmente n√£o √© de classifica√ß√£o.\")\n",
    "\n",
    "    vocab = list(map(str, learn.dls.vocab))\n",
    "    print(\"üî† Vocab do modelo:\", vocab)\n",
    "\n",
    "    # 2) Valida classes (opcional)\n",
    "    if class_names is not None:\n",
    "        print(\"üî§ Labels fornecidas:\", class_names)\n",
    "        if set(class_names) != set(vocab):\n",
    "            faltando = [c for c in vocab if c not in class_names]\n",
    "            extras   = [c for c in class_names if c not in vocab]\n",
    "            raise RuntimeError(\n",
    "                \"As classes do modelo e as fornecidas diferem.\\n\"\n",
    "                f\"Faltando no seu 'class_names': {faltando}\\n\"\n",
    "                f\"Extras no seu 'class_names': {extras}\"\n",
    "            )\n",
    "\n",
    "    # 3) Coleta arquivos do teste e r√≥tulos a partir da subpasta\n",
    "    base = Path(caminho_teste)\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Pasta de teste n√£o encontrada: {base}\")\n",
    "\n",
    "    files = get_image_files(base)\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(\"Nenhuma imagem encontrada no conjunto de teste.\")\n",
    "\n",
    "    # Filtra s√≥ arquivos cujas pastas s√£o classes v√°lidas\n",
    "    valid_classes = set(vocab)\n",
    "    files = [f for f in files if f.parent.name in valid_classes]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(\"As imagens n√£o est√£o em subpastas com nomes de classes do modelo.\")\n",
    "\n",
    "    # Contagem por classe (diagn√≥stico)\n",
    "    contagem = collections.Counter(f.parent.name for f in files)\n",
    "    print(\"üì¶ Imagens por classe no teste:\", dict(contagem))\n",
    "    print(f\"üì¶ Total de imagens no teste: {len(files)}\")\n",
    "\n",
    "    # 4) Monta r√≥tulos verdadeiros (targs) como √≠ndices do vocab\n",
    "    cls2idx = {c:i for i,c in enumerate(vocab)}\n",
    "    targs_idx = tensor([cls2idx[f.parent.name] for f in files]).long()\n",
    "\n",
    "    # 5) Cria um test_dl **sem** r√≥tulos (apenas itens). Isso evita o erro de tuplas.\n",
    "    test_dl = learn.dls.test_dl(files)  # sem with_labels\n",
    "\n",
    "    # 6) Predi√ß√µes e acur√°cia manual\n",
    "    print(\"Calculando predi√ß√µes...\")\n",
    "    preds = learn.get_preds(dl=test_dl)[0]  # s√≥ preds; targs vazio pq n√£o h√° labels no dl\n",
    "    pred_idx = preds.argmax(dim=1)\n",
    "\n",
    "    acc = (pred_idx == targs_idx).float().mean()\n",
    "    print(f\"\\nüìä Acur√°cia no conjunto de teste: {acc.item()*100:.2f}%\")\n",
    "\n",
    "    # (Opcional) mostra matriz de confus√£o rapidamente\n",
    "    try:\n",
    "        from fastai.metrics import ConfusionMatrix\n",
    "        cm = ConfusionMatrix()\n",
    "        cm.add(pred_idx, targs_idx)\n",
    "        print(\"\\nMatriz de confus√£o (primeiras linhas):\")\n",
    "        print(cm[:min(10, len(vocab)), :min(10, len(vocab))])  # evita imprimir gigante\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "# ===================================================================\n",
    "# 3) EXECU√á√ÉO\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    avaliar_modelo_no_teste(caminho_do_modelo, caminho_teste, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7KxBDsRjytN",
   "metadata": {
    "id": "A7KxBDsRjytN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sXznSQCDeSzV",
   "metadata": {
    "id": "sXznSQCDeSzV"
   },
   "source": [
    "A acur√°cia de valida√ß√£o obtida para os tr√™s modelos foi:\n",
    "\n",
    "* Rede criada: 0.8864\n",
    "* Transfer Learning: 0.998243\n",
    "* Fine-Tuning: 0.996779\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zp_0AS8TMJEX",
   "metadata": {
    "id": "Zp_0AS8TMJEX"
   },
   "source": [
    "# 8. Conclus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vydmq1hVFvg_",
   "metadata": {
    "id": "vydmq1hVFvg_"
   },
   "source": [
    "Conclui-se que, durante os tr√™s treinamentos, o com pior resultado √© a rede criada. Isso se d√° por causa do n√∫mero limitado de imagens utilizadas. O treinamento das redes utilizando o ResNet50 como base obteve um melhor resultado, porque a rede j√° havia sido treinada com mais imagens, o que gera mais precis√£o na detec√ß√£o dos elementos da imagem. O Fine-Tuning obteve o melhor resultado, pois, al√©m do treinamento do classificador para obter os resultados esperados, a rede como um todo foi ajustada para a detec√ß√£o das classes desejadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lu43YmsGAn5",
   "metadata": {
    "id": "4lu43YmsGAn5"
   },
   "source": [
    "# 9. Refer√™ncias\n",
    "\n",
    "CORNELL LAB OF ORNITHOLOGY. Merlin Bird ID. Ithaca, NY, 2025. Dispon√≠vel em: https://merlin.allaboutbirds.org/. Acesso em: 14 out. 2025.\n",
    "\n",
    "FU, J.; ZHENG, H.; MEI, T. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In: PROCEEDINGS OF THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, 2017. Anais... [S.l.]: IEEE, 2017. p. 4438-4446.\n",
    "\n",
    "KAGGLE. BirdCLEF 2024 - Birdcall Identification. San Francisco, CA, 2024. Dispon√≠vel em: https://www.kaggle.com/competitions/birdclef-2024. Acesso em: 14 out. 2025.\n",
    "\n",
    "TANG, Jiayi. Comparative analysis of CNN architectures for bird species classification. ITM Web of Conferences, [S. l.], v. 78, p. 02019, 2025. DOI: 10.1051/itmconf/20257802019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
